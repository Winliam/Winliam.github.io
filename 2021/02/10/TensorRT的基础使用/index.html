

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/time.jpg">
  <link rel="icon" href="/img/time.jpg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="王者之路，就在脚下">
  <meta name="author" content="William Guo">
  <meta name="keywords" content="">
  
  <title>TensorRT C++ API使用方法 by 电竞大师小明</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"ApWFLRBvVptYicRhN7DQqrOa-gzGzoHsz","app_key":"ihFV8rR5s2zuqoKPXqW9sydt","server_url":"https://apwflrbv.lc-cn-n1-shared.com"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>电竞大师小明</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/article.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.7)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="TensorRT C++ API使用方法">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-02-10 09:57" pubdate>
        2021年2月10日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      1.3k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      18
       分钟
    </span>
  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-page-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-page-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">TensorRT C++ API使用方法</h1>
            
            <div class="markdown-body">
              <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>把深度学习模型用pytorch等框架搭建、训练后，要把模型保存下来，部署到生产环境中服务于实际业务。</p>
<p>生产环境中运行的深度学习模型有两个特点，一当然是要快，二是只做单帧数据的前向计算（推理）。为了适应这两个特点，引入了TensorRT这个工具。</p>
<p>简单地，可以将它看作一个类似pytorch的深度学习框架。但是这个框架不提供模型训练相关的脚手架，只提供模型构建、前向计算和模型格式转换的工具。并且得益于NV一些未开源的黑科技，用TensorRT构建出来的模型能够更快地进行前向计算。</p>
<p><strong>关键概念</strong>：<br><strong>ONNX parser</strong>: Takes a converted PyTorch trained model into the ONNX format as input and populates a network object in TensorRT.<br><strong>Builder</strong>: Takes a network in TensorRT and generates an engine that is optimized for the target platform.<br><strong>Engine</strong>: Takes input data, performs inferences, and emits inference output.<br><strong>Logger</strong>: Associated with the builder and engine to capture errors, warnings, and other information during the build and inference phases.</p>
<h2 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h2><h3 id="0-头文件与namespace"><a href="#0-头文件与namespace" class="headerlink" title="0. 头文件与namespace"></a>0. 头文件与namespace</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> “NvInfer.h”</span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> nvinfer1;<br></code></pre></td></tr></table></figure>
<h3 id="1-build阶段"><a href="#1-build阶段" class="headerlink" title="1. build阶段"></a>1. build阶段</h3><h4 id="1-0-创建Logger"><a href="#1-0-创建Logger" class="headerlink" title="1.0 创建Logger"></a>1.0 创建Logger</h4><p>创建builder之前，需要先根据需要继承自己的Logger类。其中主要的任务是override父类中的log方法，定义哪个级别的信息会被log，如何log。<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">class</span> <span class="hljs-symbol">RTLogger</span> : <span class="hljs-symbol">public</span> <span class="hljs-symbol">nvinfer1::<span class="hljs-symbol">ILogger</span></span> &#123;<br>  <span class="hljs-built_in">void</span> log(Severity severity, <span class="hljs-keyword">const</span> char *msg) <span class="hljs-keyword">override</span> &#123;<br>    <span class="hljs-keyword">if</span> (severity != Severity::kINFO)    &#123;<br>      AINFO &lt;&lt; msg;<br>    &#125;<br>  &#125;<br>&#125; rt_gLogger;<br></code></pre></td></tr></table></figure></p>
<h4 id="1-1-创建builder"><a href="#1-1-创建builder" class="headerlink" title="1.1 创建builder"></a>1.1 创建builder</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">nvinfer1::IBuilder *builder_= nvinfer1::<span class="hljs-built_in">createInferBuilder</span>(rt_gLogger);<br></code></pre></td></tr></table></figure>
<h4 id="1-2-创建network"><a href="#1-2-创建network" class="headerlink" title="1.2 创建network"></a>1.2 创建network</h4><p>创建network实例的过程其实跟其他框架大同小异，只不过因为受众少，包装地没那么精致而已。</p>
<h5 id="1-2-1-创建空的network实例"><a href="#1-2-1-创建空的network实例" class="headerlink" title="1.2.1 创建空的network实例"></a>1.2.1 创建空的network实例</h5><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">nvinfer1::INetworkDefinition *network_ = builder_-&gt;<span class="hljs-built_in">createNetwork</span>();<br></code></pre></td></tr></table></figure>
<h5 id="1-2-2-填充network实例"><a href="#1-2-2-填充network实例" class="headerlink" title="1.2.2 填充network实例"></a>1.2.2 填充network实例</h5><p>上一步创建的network实例是一个空的、不包含网络层的对象，需要在此基础上添加网络层。由于TensorRT作为框架，是不做BP只做前向的，所以在构建网络时要同步给入权重。<br>此时有两种选择来做这件事:</p>
<ol>
<li>拿一个现成的模型文件(包含权重)进来，用TensorRT的格式转换工具转换一下；以ONNX的模型文件为例(ONNX模型是包含权重的)：<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">#<span class="hljs-keyword">include</span> “<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">NvOnnxParser</span>.</span></span>h”<br><br>using namespace nvonnxparser;<br><br>IParser*  parser = create<span class="hljs-constructor">Parser(<span class="hljs-operator">*</span><span class="hljs-params">network</span>, <span class="hljs-params">logger</span>)</span>;<br>parser-&gt;parse<span class="hljs-constructor">FromFile(<span class="hljs-params">modelFile</span>, ILogger::Severity::<span class="hljs-params">kWARNING</span>)</span>;<br><span class="hljs-keyword">for</span> (int32_t i = <span class="hljs-number">0</span>; i &lt; parser.get<span class="hljs-constructor">NbErrors()</span>; ++i)&#123;<br>std::cout &lt;&lt; parser-&gt;get<span class="hljs-constructor">Error(<span class="hljs-params">i</span>)</span>-&gt;desc<span class="hljs-literal">()</span> &lt;&lt; std::endl;<br>&#125;<br></code></pre></td></tr></table></figure></li>
<li>拿一个描述模型结构的配置文件，用TensorRT的模型搭建工具依次添加各种各样的网络层，从无到有地搭建出来。同时，在添加网络时，要提供对应的网络权重。<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">auto data = network_-&gt;add<span class="hljs-constructor">Input(<span class="hljs-params">dims_pair</span>.<span class="hljs-params">first</span>.<span class="hljs-params">c_str</span>()</span>, nvinfer1::DataType::kFLOAT, dims_pair.second);<br>auto convLayer = network-&gt;add<span class="hljs-constructor">Convolution(<span class="hljs-operator">*</span><span class="hljs-params">inTensor</span>, <span class="hljs-params">nbOutputs</span>, <span class="hljs-params">nvinfer1</span>::DimsHW&#123;<span class="hljs-params">kernelH</span>, <span class="hljs-params">kernelW</span>&#125;, <span class="hljs-params">wt</span>, <span class="hljs-params">bias_weight</span>)</span>;<br>auto softmaxLayer = net-&gt;add<span class="hljs-constructor">SoftMax(<span class="hljs-operator">*</span><span class="hljs-params">inputs</span>[0])</span>;<br></code></pre></td></tr></table></figure>
至此，填充完网络层和权重参数后，便完成了网络的搭建。</li>
</ol>
<h4 id="1-3-创建engine"><a href="#1-3-创建engine" class="headerlink" title="1.3 创建engine"></a>1.3 创建engine</h4><p>完成网络构建之后，其实只是把原有网络模型，以tensorRT的格式重新构建了一下，还没有进行优化加速。所以，现在要把优化前的network转成优化后的engine。</p>
<p>要做到这件事需要两种信息，一是优化什么，二是怎么优化。前者已经有了，就是network。后者需要构造一个config接口，用这个接口来配置优化项。</p>
<p>并且，在10.0之后的版本，创建engine之前还要先创建一个序列化模型，可用于保存到磁盘上。真正使用时，先反序列化模型，然后再构造出engine实例。<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">IBuilderConfig* config = builder-&gt;create<span class="hljs-constructor">BuilderConfig()</span>;<br>config-&gt;set<span class="hljs-constructor">MaxWorkspaceSize(1U &lt;&lt; 20)</span>;<br><br>IHostMemory*  serializedModel = builder-&gt;build<span class="hljs-constructor">SerializedNetwork(<span class="hljs-operator">*</span><span class="hljs-params">network</span>, <span class="hljs-operator">*</span><span class="hljs-params">config</span>)</span>;<br><br>IRuntime* runtime = create<span class="hljs-constructor">InferRuntime(<span class="hljs-params">logger</span>)</span>;<br>ICudaEngine* engine = runtime-&gt;deserialize<span class="hljs-constructor">CudaEngine(<span class="hljs-params">modelData</span>, <span class="hljs-params">modelSize</span>)</span>;<br></code></pre></td></tr></table></figure></p>
<h3 id="2-inference阶段"><a href="#2-inference阶段" class="headerlink" title="2. inference阶段"></a>2. inference阶段</h3><p>The engine holds the optimized model, but to perform inference we will need to manage additional state for intermediate activations. This is done via the <code>ExecutionContext</code> interface:<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xl">IE<span class="hljs-function"><span class="hljs-title">xecutionContext</span> *context = engine-&gt;</span>createExecutionContext();<br></code></pre></td></tr></table></figure></p>
<p>然后，在GPU上为模型的输入和输出各申请一块buffer，<br><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs abnf">int32_t inputIndex = engine-&gt;getBindingIndex(INPUT_NAME)<span class="hljs-comment">;</span><br>int32_t outputIndex = engine-&gt;getBindingIndex(OUTPUT_NAME)<span class="hljs-comment">;</span><br><br>void* buffers[<span class="hljs-number">2</span>]<span class="hljs-comment">;</span><br>buffers[inputIndex] = inputBuffer<span class="hljs-comment">;</span><br>buffers[outputIndex] = outputBuffer<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure><br>最后，执行推理（一般是GPU与CPU异步，即CPU上的控制流在调用GPU核函数之后并不阻塞）<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">context-&gt;enqueue<span class="hljs-constructor">V2(<span class="hljs-params">buffers</span>, <span class="hljs-params">stream</span>, <span class="hljs-params">nullptr</span>)</span>;<br></code></pre></td></tr></table></figure></p>
<p>由于是异步调用，在推理执行完毕之后一般还要<code>cudaDeviceSynchronize()</code>一下，等待推理过程执行完毕，然后将output结果从GPU上拷贝到CPU上。</p>
<p>至此，整个推理过程完毕。</p>
<h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ol>
<li>不同的tensorRT版本API细节有所不同，这里以TensorRT 8.2.0 Early Access (EA)为例</li>
<li>A CUDA context is automatically created the first time TensorRT makes a call to CUDA, if none exists prior to that point. It is generally preferable to create and configure the CUDA context yourself before the first call to TensoRT.</li>
<li>Interface classes in the TensorRT™ C++ API begin with the prefix I</li>
<li>config interface has many properties that you can set in order to control how TensorRT optimizes the network. One important property is the maximum workspace size. Layer implementations often require a temporary workspace, and this parameter limits the maximum size that any layer in the network can use. If insufficient workspace is provided, it is possible that TensorRT will not be able to find an implementation for a layer.</li>
<li>Serialized engines are not portable across platforms or TensorRT versions. Engines are specific to the exact GPU model they were built on (in addition to the platform and the TensorRT version).</li>
<li>The serialized engine contains the necessary copies of the weights, the parser, network definition, builder configuration and builder are no longer necessary and may be safely deleted.</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#perform_inference_c">官方文档8.2.0 Early Access (EA)</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/19193468/why-do-we-need-cudadevicesynchronize-in-kernels-with-device-printf">https://stackoverflow.com/questions/19193468/why-do-we-need-cudadevicesynchronize-in-kernels-with-device-printf</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Bruce_0712/article/details/73477126">https://blog.csdn.net/Bruce_0712/article/details/73477126</a></li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%B7%A5%E4%BD%9C%E6%8A%80%E8%83%BD/">工作技能</a>
                    
                      <a class="hover-with-bg" href="/categories/%E5%B7%A5%E4%BD%9C%E6%8A%80%E8%83%BD/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/TensorRT/">TensorRT</a>
                    
                      <a class="hover-with-bg" href="/tags/ONNX/">ONNX</a>
                    
                      <a class="hover-with-bg" href="/tags/inference/">inference</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/03/01/Faster-RCNN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Faster RCNN源码分析（零）—— 整体框架</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/12/22/pytorch%E7%AC%94%E8%AE%B0/">
                        <span class="hidden-mobile">pytorch笔记</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="twikoo"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/twikoo@1.4.1/dist/twikoo.all.min.js', function() {
        var options = Object.assign(
          {"envId":"twikoo-5gimyzvb3631ef62","region":"ap-shanghai","path":"window.location.pathname"},
          {
            el: '#twikoo',
            path: 'window.location.pathname'
          }
        )
        twikoo.init(options)
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <small><span>——————————我是有底线的——————————</span></small> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>












  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":200,"height":400},"mobile":{"show":false,"scale":0.5},"log":false});</script></body>
</html>
