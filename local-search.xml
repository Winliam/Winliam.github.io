<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>神经网络的数学原理-无公式版</title>
    <link href="/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-%E6%97%A0%E5%85%AC%E5%BC%8F%E7%89%88/"/>
    <url>/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-%E6%97%A0%E5%85%AC%E5%BC%8F%E7%89%88/</url>
    
    <content type="html"><![CDATA[<h2 id="神经网络模型的结构"><a href="#神经网络模型的结构" class="headerlink" title="神经网络模型的结构"></a>神经网络模型的结构</h2><p><strong>神经网络</strong>的基础单元是<strong>神经元</strong>，多个<strong>神经元</strong>纵向堆叠形成神经<strong>网络层</strong>，神经<strong>网络层</strong>横向堆叠形成<strong>神经网络</strong>。</p><div align=center><img title="" src="/img/article/神经网络的数学原理/神经网络.png" width="50%" height="50%" align=center></div><div align=center><img title="" src="/img/article/神经网络的数学原理/神经元.png" width="60%" height="60%" align=center></div><h2 id="神经元的数学原理"><a href="#神经元的数学原理" class="headerlink" title="神经元的数学原理"></a>神经元的数学原理</h2><p>对于一个神经元，进行的数学计算为：</p><blockquote><p>接受一个向量$\boldsymbol{a}^{[j-1]}$，通过与$\boldsymbol{w}^{[j]}_i$进行向量内积运算产生一个中间值$z_i^{[j]}$（标量），然后用激活函数$g_i^{[j]}()$将$z$转换为$a_i^{[j]}$。</p></blockquote><p>其中:</p><blockquote><p>上标用来定位该神经元位于哪一层，一般输入层后的第一层为1；<br>下标用来定位该神经元位于第几个，一般最上方的序号为0；</p></blockquote><h2 id="矩阵维度确认的数学原理"><a href="#矩阵维度确认的数学原理" class="headerlink" title="矩阵维度确认的数学原理"></a>矩阵维度确认的数学原理</h2><p>首先区分开这4个概念：<strong>模型的参数</strong>，<strong>层的参数</strong>，<strong>神经元的参数</strong>，<strong>数据及数据的中间值</strong>。然后，仔细理解上面两段话，神经网络中最为tricky的维度问题便迎刃而解：</p><ol><li>对于$\boldsymbol{x}$和$\boldsymbol{y}$，其维度看样本就知道，已经定义好了;</li><li>对于某一个神经元的权重参数$\boldsymbol{w}^{[j]}_i$，由于要跟输入的向量$\boldsymbol{a}^{[j-1]}$进行内积，所以两者的维度必然是相同的，而后者作为一个列向量，其行数等于上一层的神经元数量（因为每个神经元输出一个标量）。然后由于本层的每一个神经元都有一个权重参数$\boldsymbol{w}^{[j]}$，那么由${\boldsymbol{w}^{[j]}}^T$纵向堆叠形成的${\boldsymbol{W}^{[j]}}^T$的行数就是$\boldsymbol{w}^{[j]}$的个数，亦即本层的神经元数量，其列数前面已经说了，就是$\boldsymbol{w}^{[j]}$的行数，亦即上一层的神经元数量。<br></li><li>对于某一个神经元的偏移量参数${b}^{[j]}$，自然是一个标量。那么本层的偏移量参数$\boldsymbol{b}^{[j]}$的行数就是${b}^{[j]}$的数量，亦即本层的神经元数量。</li><li>对于每一层的中间值$\boldsymbol{z}^{[j]}$，输出值$\boldsymbol{a}^{[j]}$，其维度确定方式与$\boldsymbol{b}^{[j]}$一样。</li><li>另外对于激活函数，一般同一层都一样，所以$\boldsymbol{g}^{[j]}()$退化为${g}^{[j]}()$。</li></ol><h2 id="矢量化的数学原理"><a href="#矢量化的数学原理" class="headerlink" title="矢量化的数学原理"></a>矢量化的数学原理</h2><p>矢量化的本质是将样本在时间轴上被神经网络模型处理的序列转化为空间上的序列：<br></p><blockquote><p>$X= (\boldsymbol{x}^1,\boldsymbol{x}^2, …,  \boldsymbol{x}^)$</p></blockquote><p>说人话就是，原来每次送入模型一个列向量，计算得到一个列向量。现在每次送入m个列向量，计算的到m个列向量。当然了，各层的中间值$\boldsymbol{z}^{[j]}$和输出值$\boldsymbol{a}^{[j]}$也都将因此横向扩充一个维度。</p><h2 id="正向计算的数学原理"><a href="#正向计算的数学原理" class="headerlink" title="正向计算的数学原理"></a>正向计算的数学原理</h2><p>正向计算，用于得到所需的预测结果：</p><ol><li>输入一个列向量</li><li>进行一系列矩阵计算</li><li>得到一个列向量</li></ol><h2 id="反向传播的数学原理"><a href="#反向传播的数学原理" class="headerlink" title="反向传播的数学原理"></a>反向传播的数学原理</h2><p>反向传播，用于找到合适的模型参数。其中“合适”的标准是，整个训练集形成的代价$J$足够小。“找”的方法是，先随机初始化各层的$\boldsymbol{W}$和$\boldsymbol{b}$，然后不断更新之，直到$\boldsymbol{W}$和$\boldsymbol{b}$合适。</p><ol><li>单个样本$\boldsymbol{x}$ 正向计算，得到各层的$\boldsymbol{z}$和$\boldsymbol{a}$备用。</li><li>计算输出层的参数更新量：<ol><li>求得损失$L$对$\boldsymbol{z}^{[l]}$的偏导$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}$，记作$d\boldsymbol{z}^{[l]}$。上标$l$表示最后一层，即输出层。若输出层激活函数为$softmax$且损失函数为交叉熵，则有$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]}) - \boldsymbol{y}$。</li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{z}^{[l]}$得到$d{\boldsymbol{W}^{[l]}}^T$，$d\boldsymbol{b}^{[l]}$，$d\boldsymbol{a}^{[l-1]}$</li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{a}^{[l-1]}$得到$d\boldsymbol{z}^{[l-1]}$。</li></ol></li><li>仿照输出层的计算方式，反向传播，依次得到各层的参数更新量。</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DNN</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络的数学原理</title>
    <link href="/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
    <url>/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="神经网络模型的结构"><a href="#神经网络模型的结构" class="headerlink" title="神经网络模型的结构"></a>神经网络模型的结构</h2><p><strong>神经网络</strong>的基础单元是<strong>神经元</strong>，多个<strong>神经元</strong>纵向堆叠形成神经<strong>网络层</strong>，神经<strong>网络层</strong>横向堆叠形成<strong>神经网络</strong>。</p><div align=center><img title="" src="/img/article/神经网络的数学原理/神经网络.png" width="50%" height="50%" align=center></div><br><div align=center><img title="" src="/img/article/神经网络的数学原理/神经元.png" width="60%" height="60%" align=center></div><h2 id="神经元的数学原理"><a href="#神经元的数学原理" class="headerlink" title="神经元的数学原理"></a>神经元的数学原理</h2><p>对于一个神经元，进行的数学计算为：</p><blockquote><p>接受一个向量$\boldsymbol{a}^{[j-1]}$，通过与$\boldsymbol{w}^{[j]}_i$进行向量内积运算产生一个中间值$z_i^{[j]}$（标量），然后用激活函数$g_i^{[j]}()$将$z$转换为$a_i^{[j]}$。</p></blockquote><p>其中:</p><blockquote><p>上标用来定位该神经元位于哪一层，一般输入层后的第一层为1；<br>下标用来定位该神经元位于第几个，一般最上方的序号为0；</p></blockquote><h2 id="矩阵维度确认的数学原理"><a href="#矩阵维度确认的数学原理" class="headerlink" title="矩阵维度确认的数学原理"></a>矩阵维度确认的数学原理</h2><p>首先区分开这4个概念：<strong>模型的参数</strong>，<strong>层的参数</strong>，<strong>神经元的参数</strong>，<strong>数据及数据的中间值</strong>。然后，仔细理解上面两段话，神经网络中最为tricky的维度问题便迎刃而解：</p><ol><li>对于$\boldsymbol{x}$和$\boldsymbol{y}$，其维度看样本就知道，已经定义好了;</li><li>对于某一个神经元的权重参数$\boldsymbol{w}^{[j]}_i$，由于要跟输入的向量$\boldsymbol{a}^{[j-1]}$进行内积，所以两者的维度必然是相同的，而后者作为一个列向量，其行数等于上一层的神经元数量（因为每个神经元输出一个标量）。然后由于本层的每一个神经元都有一个权重参数$\boldsymbol{w}^{[j]}$，那么由${\boldsymbol{w}^{[j]}}^T$纵向堆叠形成的${\boldsymbol{W}^{[j]}}^T$的行数就是$\boldsymbol{w}^{[j]}$的个数，亦即本层的神经元数量，其列数前面已经说了，就是$\boldsymbol{w}^{[j]}$的行数，亦即上一层的神经元数量。<br></li><li>对于某一个神经元的偏移量参数${b}^{[j]}$，自然是一个标量。那么本层的偏移量参数$\boldsymbol{b}^{[j]}$的行数就是${b}^{[j]}$的数量，亦即本层的神经元数量。</li><li>对于每一层的中间值$\boldsymbol{z}^{[j]}$，输出值$\boldsymbol{a}^{[j]}$，其维度确定方式与$\boldsymbol{b}^{[j]}$一样。</li><li>另外对于激活函数，一般同一层都一样，所以$\boldsymbol{g}^{[j]}()$退化为${g}^{[j]}()$。</li></ol><h2 id="矢量化的数学原理"><a href="#矢量化的数学原理" class="headerlink" title="矢量化的数学原理"></a>矢量化的数学原理</h2><p>矢量化的本质是将样本在时间轴上被神经网络模型处理的序列转化为空间上的序列：<br></p><blockquote><p>$X= (\boldsymbol{x}^1,\boldsymbol{x}^2, …,  \boldsymbol{x}^)$</p></blockquote><p>说人话就是，原来每次送入模型一个列向量，计算得到一个列向量。现在每次送入m个列向量，计算的到m个列向量。当然了，各层的中间值$\boldsymbol{z}^{[j]}$和输出值$\boldsymbol{a}^{[j]}$也都将因此横向扩充一个维度。</p><h2 id="正向计算的数学原理"><a href="#正向计算的数学原理" class="headerlink" title="正向计算的数学原理"></a>正向计算的数学原理</h2><p>正向计算，用于得到所需的预测结果：</p><ol><li>输入一个列向量：<br><blockquote><p> $\boldsymbol{x} = (x_1, x_2, … , x_n)^T$</p></blockquote></li><li><p>进行一系列矩阵计算：<br></p><blockquote><p> $\boldsymbol{a}^{[0]} = \boldsymbol{x}$<br> $\boldsymbol{z}^{[1]} = {\boldsymbol{W}^{[1]}}^T\boldsymbol{a}^{[0]}+\boldsymbol{b}^{[1]}$<br> $\boldsymbol{a}^{[2]} = g^{[1]}(\boldsymbol{z}^{[1]})$<br> …<br> $\boldsymbol{z}^{[j]} = {\boldsymbol{W}^{[j]}}^T\boldsymbol{a}^{[j-1]}+\boldsymbol{b}^{[j]}$<br> $\boldsymbol{a}^{[j]} = g^{[j]}(\boldsymbol{z}^{[j]})$<br> …<br> $\boldsymbol{z}^{[l]} = {\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}$<br> $\boldsymbol{a}^{[l]} = g^{[l]}(\boldsymbol{z}^{[l]})$<br> $\boldsymbol{\hat{y}} = \boldsymbol{a}^{[l]}$</p></blockquote><p>特别地，对于多分类模型的输出层（最后一层），一般有：</p><blockquote><p>$g^{[l]}(\boldsymbol{x}) = softmax(\boldsymbol{x}) = \frac{exp(\boldsymbol{x})}{\boldsymbol{1}^Texp(\boldsymbol{x})}$</p></blockquote><p>其中 $\boldsymbol{1}$ 为全1列向量，维度可从context推得。</p></li><li>得到一个列向量：<br><blockquote><p> $\boldsymbol{\hat{y}} = (\hat{y}_1,\hat{y}_2, … , \hat{y}_n)^T$</p></blockquote></li></ol><h2 id="反向传播的数学原理"><a href="#反向传播的数学原理" class="headerlink" title="反向传播的数学原理"></a>反向传播的数学原理</h2><p>反向传播，用于找到合适的模型参数。其中“合适”的标准是，整个训练集形成的代价$J$足够小。“找”的方法是，先随机初始化各层的$\boldsymbol{W}$和$\boldsymbol{b}$，然后不断更新之，直到$\boldsymbol{W}$和$\boldsymbol{b}$合适。</p><ol><li>单个样本$\boldsymbol{x}$ 正向计算，得到各层的$\boldsymbol{z}$和$\boldsymbol{a}$备用。</li><li>计算输出层的参数更新量：<ol><li>求得损失$L$对$\boldsymbol{z}^{[l]}$的偏导$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}$，记作$d\boldsymbol{z}^{[l]}$。上标$l$表示最后一层，即输出层。若输出层激活函数为$softmax$且损失函数为交叉熵，则有$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]}) - \boldsymbol{y}$，对应的求导过程如下：<blockquote><p>$L = -\boldsymbol{y}^Tlog\hat{\boldsymbol{y}}$<br>$\downarrow$<br>$dL = -d\boldsymbol{y}^Tlog\hat{\boldsymbol{y}}-\boldsymbol{y}^Td(log\hat{\boldsymbol{y}})$<br>$= -\boldsymbol{y}^T d(log\hat{\boldsymbol{y}})$<br>$= -\boldsymbol{y}^Td(log(softmax(\boldsymbol{z}^{[l]})))$<br>$= -\boldsymbol{y}^Td(log(\frac{exp(\boldsymbol{\boldsymbol{z}^{[l]}})}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}))$<br>$= -\boldsymbol{y}^Td(log(exp(\boldsymbol{\boldsymbol{z}^{[l]}}))+\boldsymbol{1}log(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})))$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + d(log(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})))$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + \frac{d(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}}))}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + \frac{\boldsymbol{1}^T(exp(\boldsymbol{\boldsymbol{z}^{[l]}}) \odot d\boldsymbol{z}^{[l]} )}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + {\frac{exp(\boldsymbol{\boldsymbol{z}^{[l]}})}{\boldsymbol{1}^Texp(\boldsymbol{z}^{[l]})}}^Td\boldsymbol{z}^{[l]}$<br>$= (softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T)^Td\boldsymbol{z}^{[l]}$<br>$\downarrow$<br>$tr(dL) = dL = tr((softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T)^Td\boldsymbol{z}^{[l]}) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}} = softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T$<br>$\downarrow$<br>$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T$</p></blockquote></li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{z}^{[l]}$得到$d{\boldsymbol{W}^{[l]}}^T$，$d\boldsymbol{b}^{[l]}$，$d\boldsymbol{a}^{[l-1]}$<ul><li>$d{\boldsymbol{W}^{[l]}}^T$的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td{\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]})$<br>$= tr(\boldsymbol{a}^{[l-1]}{\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td{\boldsymbol{W}^{[l]}}^T)$<br>$= tr(({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}{\boldsymbol{a}^{[l-1]}}^T)^Td{\boldsymbol{W}^{[l]}}^T)$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{W}^{[l]}}^T} = {\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}{\boldsymbol{a}^{[l-1]}}^T$<br>$\downarrow$<br>$d{\boldsymbol{W}^{[l]}}^T = d\boldsymbol{z}^{[l]}{\boldsymbol{a}^{[l-1]}}^T$</p></blockquote></li><li>$d\boldsymbol{b}^{[l]}$的求导过程<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{b}^{[l]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{b}^{[l]}}} = {\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}$<br>$\downarrow$<br>$d\boldsymbol{b}^{[l]} = d\boldsymbol{z}^{[l]}$</p></blockquote></li><li>$d\boldsymbol{a}^{[l-1]}$的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^T{\boldsymbol{W}^{[l]}}^Td\boldsymbol{a}^{[l-1]})$<br>$= tr((\boldsymbol{W}^{[l]}\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}})^Td\boldsymbol{a}^{[l-1]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} = {\boldsymbol{W}^{[l]}\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}$<br>$\downarrow$<br>$d\boldsymbol{a}^{[l-1]} = \boldsymbol{W}^{[l]}d\boldsymbol{z}^{[l]}$</p></blockquote></li></ul></li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{a}^{[l-1]}$得到$d\boldsymbol{z}^{[l-1]}$，对应的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^Td\boldsymbol{a}^{[l-1]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^Td(g(\boldsymbol{z}^{[l-1]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^T(g’(\boldsymbol{z}^{[l-1]}) \odot d\boldsymbol{z}^{[l-1]}))$<br>$= tr((\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} \odot g’(\boldsymbol{z}^{[l-1]}))^T d\boldsymbol{z}^{[l-1]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l-1]}}} = \frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} \odot g’(\boldsymbol{z}^{[l-1]})$<br>$\downarrow$<br>$d\boldsymbol{z}^{[l-1]} = d\boldsymbol{a}^{[l-1]}\odot g’(\boldsymbol{z}^{[l-1]})$</p></blockquote></li></ol></li><li>仿照输出层的计算方式，反向传播，依次得到各层的参数更新量。<blockquote><p>$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]}) - \boldsymbol{y}$<br>$d{\boldsymbol{W}^{[l]}}^T = d\boldsymbol{z}^{[l]}{\boldsymbol{a}^{[l-1]}}^T$ AND $d\boldsymbol{b}^{[l]} = d\boldsymbol{z}^{[l]}$<br>…<br>$d\boldsymbol{z}^{[j]} = (\boldsymbol{W}^{[j+1]}d\boldsymbol{z}^{[j+1]}) \odot g’(\boldsymbol{z}^{[j]})$<br>$d{\boldsymbol{W}^{[j]}}^T = d\boldsymbol{z}^{[j]}{\boldsymbol{a}^{[j-1]}}^T$ AND $d\boldsymbol{b}^{[j]} = d\boldsymbol{z}^{[j]}$<br>$d\boldsymbol{z}^{[j-1]} = (\boldsymbol{W}^{[j]}d\boldsymbol{z}^{[j]}) \odot g’(\boldsymbol{z}^{[j-1]})$<br>…<br>$d{\boldsymbol{W}^{[1]}}^T = d\boldsymbol{z}^{[1]}{\boldsymbol{a}^{[0]}}^T$ AND $d\boldsymbol{b}^{[1]} = d\boldsymbol{z}^{[1]}$<br>$d\boldsymbol{z}^{[0]} = (\boldsymbol{W}^{[1]}d\boldsymbol{z}^{[1]}) \odot g’(\boldsymbol{z}^{[0]})$</p></blockquote></li></ol><h2 id="梯度下降的数学原理"><a href="#梯度下降的数学原理" class="headerlink" title="梯度下降的数学原理"></a>梯度下降的数学原理</h2><h2 id="归一化的数学原理"><a href="#归一化的数学原理" class="headerlink" title="归一化的数学原理"></a>归一化的数学原理</h2><h2 id="batch的数学原理"><a href="#batch的数学原理" class="headerlink" title="batch的数学原理"></a>batch的数学原理</h2>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DNN</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CSAPP并发编程总结</title>
    <link href="/2021/07/09/CSAPP%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <url>/2021/07/09/CSAPP%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="并发编程基本概念"><a href="#并发编程基本概念" class="headerlink" title="并发编程基本概念"></a>并发编程基本概念</h2><ul><li><strong>并发</strong>：多个逻辑控制流的生命周期有重叠，即称为<strong>并发现象</strong>(concurrency)</li><li><strong>并行</strong>：发生在多核/多计算机上的并发现象（在一个时刻上存在多个逻辑控制流），称为<strong>并行现象</strong>（parallel），是并发现象的真子集；</li></ul><h2 id="并发程序的三种构造方式："><a href="#并发程序的三种构造方式：" class="headerlink" title="并发程序的三种构造方式："></a>并发程序的三种构造方式：</h2><ul><li><strong>进程</strong>：每个逻辑控制流实现为一个进程<ul><li>特点：独立的虚拟地址空间</li><li>优点：独立则不易混淆</li><li>缺点：<ul><li>独立则难以共享数据</li><li>进程context切换和IPC开销高，所以往往比较慢（ 进程间通信机制）</li></ul></li></ul></li><li><strong>I/O多路复用</strong>：状态机化，逻辑控制流的切换实现为状态机的状态切换。具体原理看<a href="https://www.zhihu.com/question/32163005/answer/55772739">这个</a><ul><li>优点：共享数据容易，并且没有进程context切换的开销</li><li>缺点：编码复杂，不能充分利用多核处理器</li></ul></li><li><strong>线程</strong>：重点，下面展开讲。</li></ul><h2 id="线程基本概念："><a href="#线程基本概念：" class="headerlink" title="线程基本概念："></a>线程基本概念：</h2><ul><li><strong>主线程</strong>：进程中第一个运行的线程</li><li><strong>对等线程</strong>：进程中后来运行的线程</li><li><strong>与进程的区别</strong>：<ul><li>上下文内容少，切换更快，开销更少，具体包括：线程ID、栈和栈指针、PC、条件码、register value</li><li>一个进程的所有线程（对等线程池）彼此之间没有层次结构，都是对等的；</li><li>对等线程之间共享进程的虚拟地址空间，可以等待另外一个对等线程终止或主动杀死它</li></ul></li><li><strong>共享变量</strong>：一个变量的一个实例被不止一个线程引用，那么这个变量称为共享变量</li><li><strong>线程安全的函数</strong>：被多个并发线程反复调用时能够一直产生正确结果的函数称为线程安全函数</li><li><strong>可重入函数</strong>：线程安全函数的一个真子集，指不会引入任何共享数据的函数<ul><li><strong>显式可重入</strong>：传参均为值传递（且非指针值传递），而且所有数据引用的都是本地自动栈变量</li><li><strong>隐式可重入</strong>：在显式的基础上取消“值传递（且非指针值传递）”的限制，允许指针值传递和引用传递，但是传递的变量都是非共享变量时，该函数是隐式可重入的</li></ul></li><li><strong>竞争</strong>：程序的正确性依赖于某条/某些特定的轨迹线，或者说不是全部的轨迹线都能让程序正确执行，哪怕是那些绕过了互斥锁禁止区的全部轨迹线也不行。（具体例子见CSAPP P719）</li></ul><h2 id="Posix线程模型："><a href="#Posix线程模型：" class="headerlink" title="Posix线程模型："></a>Posix线程模型：</h2><p>管理Linux线程的C语言接口包<pthread.h>，包含大约60个函数。</p><ul><li><strong>线程例程概念</strong>：接受和返回一个void指针的函数类型，其内容为线程真正要做的事。若输入输出的参数较多，应封装为一个结构体。</li><li><strong>常用的pthread函数</strong>：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/*创建线程*/</span><br>int pthread_creat(&amp;tid, NULL, thread, NULL)<br><span class="hljs-regexp">//</span>(返回线程ID，设置线程属性(高阶内容)，线程例程函数名，线程例程函数的传参)，成功返回<span class="hljs-number">0</span>，否则非<span class="hljs-number">0</span><br><br><span class="hljs-regexp">/*终止线程*/</span><br><span class="hljs-regexp">//</span>方式<span class="hljs-number">1</span>：某个对等线程的例程函数执行完毕，该线程会隐式终止<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">2</span>：某个对等线程调用pthread_exit函数，线程会显式终止，而且如果是主线程调用，它会等待所有其他对等线程终止后再终止（进程也被终止了）<br>void pthread_exit(void *thread_return) <span class="hljs-regexp">//</span>函数不返回（因为逻辑控制流都结束了啊），会将一些信息写到thread_return中<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">3</span>：某个对等线程调用系统<span class="hljs-keyword">exit</span>函数，终止其所属进程及该进程所有的线程<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">4</span>：某个对等线程调用pthread_cancel函数，终止另一个对等线程<br>int pthread_cancel(pthread_t tid)  <span class="hljs-regexp">//</span>终止线程ID为tid的对等线程，成功返回<span class="hljs-number">0</span>，否则非<span class="hljs-number">0</span><br><br><span class="hljs-regexp">/*回收已终止线程的资源*/</span><br>int pthread_join(pthread_t tid, void**thread_return) <span class="hljs-regexp">//</span>调用该函数的对等线程阻塞，等待线程ID为tid的对等线程结束，然后回收其资源后返回<br><br><span class="hljs-regexp">/*分离线程*/</span><br><span class="hljs-regexp">//</span>一个线程的状态要么是detached要么是joinable，处于后者时意味着可以被其他线程杀死和回收资源，前者不可（自行终止，系统回收资源）<br>int pthread_detach(pthread_t tid)  <span class="hljs-regexp">//</span>调用该函数的对等线程将线程ID为tid的线程分离<br><br><span class="hljs-regexp">/*获取自身ID*/</span><br>pthread_t pthread_self();<br></code></pre></td></tr></table></figure></li></ul><h2 id="线程的内存模型（两个关键问题）"><a href="#线程的内存模型（两个关键问题）" class="headerlink" title="线程的内存模型（两个关键问题）"></a>线程的内存模型（两个关键问题）</h2><ul><li><strong>线程的内存模型是怎样的？</strong>——不是整齐清楚的。。。</li><li><strong>变量的实例如何映射到线程的内存模型中？</strong><ul><li>全局变量+局部的静态变量：一个进程中只有一个实例，任何线程均可引用；</li><li>局部的自动变量：多个实例，由各个线程栈自行管理。</li></ul></li></ul><h2 id="线程共享变量的冲突问题"><a href="#线程共享变量的冲突问题" class="headerlink" title="线程共享变量的冲突问题"></a>线程共享变量的冲突问题</h2><p>关键字：进度图-&gt;信号量-&gt;PV操作-&gt;互斥锁-&gt;互斥锁加/解锁-&gt;死锁</p><ul><li><strong>进度图</strong>：注意把P/V操作放到线段上，状态放到端点上，这样端点的状态即可解释为执行P/V操作前或者P/V操作后</li><li><strong>信号量</strong>s：一个非负整数全局变量</li><li><strong>PV操作</strong>（原子操作）：<ul><li>P(s)操作：检查s是否为0；<ul><li>是，则调用该函数的线程在此处阻塞；</li><li>否，则将s减1后继续向下执行；</li></ul></li><li>V(s)操作：先将s加1，然后检查有么有因为P(s)阻塞的线程，如有则将完成其P操作，然后置为就绪状态（等待调度），若没有那就没有。。若有不止一个，就随机选择一个，反正只能一个（因为要完成P操作啊）</li></ul></li><li><strong>互斥锁</strong>：二元的信号量</li><li><strong>互斥锁加/解锁</strong>：针对二元信号量的P/V操作</li><li><strong>死锁</strong>：<br>禁止区外存在这样一些状态点，既不能向右，也不能向上，因为向上会进入线程A的禁止区，向右会进入线程B的禁止区。<br><br>通过以下原则来防止死锁：<br>给定所有互斥操作的一个全序( 全序概念)，如果每个线程都是以该全序获得互斥锁并以相反的顺序（不是说全序的逆序，而是线程A和线程B释放的顺序相反）释放，那么这个程序就不会出现死锁。（但是该原则现在看下来只适用于两个线程，更多的线程就要用到更复杂的银行家算法了）<br><br>例如：<br><ul><li>线程1： P(s) -&gt; P(t) -&gt; V(t) -&gt; V(s);</li><li>线程2： P(s) -&gt; P(t) -&gt; V(s) -&gt; V(t);</li></ul></li></ul><h2 id="并行程序的性能量化（暂时略过）"><a href="#并行程序的性能量化（暂时略过）" class="headerlink" title="并行程序的性能量化（暂时略过）"></a>并行程序的性能量化（暂时略过）</h2><h2 id="信号量用于共享资源调度（暂时略过）"><a href="#信号量用于共享资源调度（暂时略过）" class="headerlink" title="信号量用于共享资源调度（暂时略过）"></a>信号量用于共享资源调度（暂时略过）</h2>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>操作系统</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>个人博客搭建</title>
    <link href="/2021/07/08/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <url>/2021/07/08/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于强迫症患者，非要自定义域名的话，先提前注册好域名，推荐阿里云，审核快。不然下面的流程会阻塞。。。</p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node.js"></a>安装node.js</h2><ol><li>去官网 <a href="https://nodejs.org">https://nodejs.org</a> 查看最新版本号</li><li>根据最新版本号添加源，然后安装<blockquote><p>curl -sL <a href="https://deb.nodesource.com/setup_10.x">https://deb.nodesource.com/setup_10.x</a> | sudo -E bash<br>sudo apt-get install -y nodejs<br></p></blockquote></li><li>查看版本，确认安装成功<blockquote><p>node -v<br>npm -v</p></blockquote></li></ol><h2 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h2><p>上交公钥，然后设置用户名+邮箱</p><blockquote><p>git config —global user.name “William_2580”<br>git config —global user.email “1611134972@qq.com”</p></blockquote><h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><blockquote><p>npm install -g hexo-cli</p></blockquote><h2 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h2><blockquote><p>mkdir hexoblog<br>hexo init hexoblog<br>hexo server<br>然后浏览器访问<a href="http://localhost:4000">http://localhost:4000</a>, 即可看到部署在本地的个人网站</p></blockquote><h2 id="云端部署"><a href="#云端部署" class="headerlink" title="云端部署"></a>云端部署</h2><ol><li>github创建repo，名称必须为：Winliam.github.io</li><li>安装hexo部署工具：<blockquote><p>npm install hexo-deployer-git —save</p></blockquote></li><li>修改hexoblog下的_config.yml中的deploy段：<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">type: git<br>repo: git@github<span class="hljs-selector-class">.com</span>:Winliam/Winliam<span class="hljs-selector-class">.github</span><span class="hljs-selector-class">.io</span><span class="hljs-selector-class">.git</span><br>branch: main<br></code></pre></td></tr></table></figure></li><li>部署：<blockquote><p>hexo d //public文件夹下的内容被推送到刚才创建的repo中</p></blockquote></li><li>然后进入repo的setting-&gt;pages页面选择正确的分支后，即可通过Winliam.github.io访问blog了</li></ol><h2 id="自定义域名"><a href="#自定义域名" class="headerlink" title="自定义域名"></a>自定义域名</h2><ol><li>阿里云域名注册购买</li><li>买到后阿里云中继续设置域名解析，完成后等待10min等DNS生效(使得www.guohongming.xyz和guohongming.xyz都可以被访问到)</li><li>在本地public文件夹添加CNAME文件(内容为guohongming.xyz)后重新部署一次</li><li>在云端repo的setting-&gt;pages页面将guohongming.xyz添加到Custom domain并保存，不用管warning</li></ol><h2 id="主题美化"><a href="#主题美化" class="headerlink" title="主题美化"></a>主题美化</h2><ul><li><p><strong>安装fluid主题</strong></p><ol><li>在blog文件夹下<blockquote><p>npm install —save hexo-theme-fluid</p></blockquote></li><li>将GitHub上的<a href="https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml">_config.yml</a>文件内容复制到本地的_config.fluid.yml中 </li><li>修改本地_config.yml的内容，以应用主题</li></ol><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">theme:</span> fluid  <span class="hljs-meta"># 指定主题</span><br><span class="hljs-symbol">language:</span> <span class="hljs-built_in">zh</span>-CN  <span class="hljs-meta"># 指定语言，会影响主题显示的语言，按需修改</span><br></code></pre></td></tr></table></figure><ol><li>此时新建文章后便会自动生成fluid的文章，包括自动生成目录等</li></ol></li><li><strong>添加评论插件</strong></li><li><strong>添加看板娘</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
      <category>电脑设置</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>markdown-ize vim</title>
    <link href="/2021/07/08/markdown-ize-vim/"/>
    <url>/2021/07/08/markdown-ize-vim/</url>
    
    <content type="html"><![CDATA[<h1 id="安装neovim"><a href="#安装neovim" class="headerlink" title="安装neovim"></a>安装neovim</h1><blockquote><p>sudo apt-get install neovim<br>neovim<br>:checkhealth</p></blockquote><h1 id="安装vim插件管理器vim-plug"><a href="#安装vim插件管理器vim-plug" class="headerlink" title="安装vim插件管理器vim-plug"></a>安装vim插件管理器vim-plug</h1><blockquote><p>sh -c ‘curl -fLo “${XDG_DATA_HOME:-$HOME/.local/share}”/nvim/site/autoload/plug.vim —create-dirs<br><a href="https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim">https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim</a>‘</p></blockquote><p>链接不上的话，需要：<br></p><blockquote><p>sudo nvim /etc/hosts<br></p></blockquote><p>添加一行：<br></p><blockquote><p>199.232.96.133 raw.githubusercontent.com<br></p></blockquote><p>其中的ip地址来自于<em><a href="https://githubusercontent.com.ipaddress.com/raw.githubusercontent.com">https://githubusercontent.com.ipaddress.com/raw.githubusercontent.com</a></em></p><h1 id="创建neovim配置文件init-vim"><a href="#创建neovim配置文件init-vim" class="headerlink" title="创建neovim配置文件init.vim"></a>创建neovim配置文件init.vim</h1><blockquote><p>mkdir .config/nvim<br>touch .config/nvim/init.vim</p></blockquote><h1 id="修改init-vim以添加插件"><a href="#修改init-vim以添加插件" class="headerlink" title="修改init.vim以添加插件"></a>修改init.vim以添加插件</h1><p>目录，markdown，preview一共三个暂时（修改的内容直接看文件）,<br>中间: </p><ul><li>遇到了自动折叠问题：修改配置文件解决  </li><li>又遇到了无法预览的问题，看github<a href="https://github.com/iamcco/markdown-preview.nvim/issues/120">作者回复</a>解决</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/07/07/hello-world/"/>
    <url>/2021/07/07/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
