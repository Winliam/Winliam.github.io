<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/07/29/python%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%E7%9A%84%E6%89%93%E5%8C%85%E4%B8%8E%E6%8B%86%E8%A7%A3/"/>
    <url>/2021/07/29/python%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%E7%9A%84%E6%89%93%E5%8C%85%E4%B8%8E%E6%8B%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<hr><p>title: python函数参数的打包与拆解<br>categories:</p><ul><li><ul><li>工作技能</li><li>python笔记<br>date: 2021-07-29 18:14:46<br>tags:</li></ul></li><li>Python —-<br><strong>标志</strong>：单/双星号出现在入参或形参之前<br><strong>原则</strong>：<ol><li>入参前加星号代表拆解，形参前加星号代表打包</li><li>list/tuple只有一种拆解方式，dictionary有两种</li></ol></li></ul><h2 id="打包情形1：单星号出现在形参前"><a href="#打包情形1：单星号出现在形参前" class="headerlink" title="打包情形1：单星号出现在形参前"></a>打包情形1：单星号出现在形参前</h2><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">def</span> pack(a, *b):<br>        print <span class="hljs-keyword">type</span>(a), a<br>        print <span class="hljs-keyword">type</span>(b), b<br><br><span class="hljs-title">pack</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;int&#x27;&gt; 1<br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;tuple&#x27;&gt; (2, 3, 4, 5)<br></code></pre></td></tr></table></figure><p>单星号打包，按照匹配顺序认领多个非关键字入参，打包称一个tuple。</p><h2 id="打包情形2：双星号出现在形参前"><a href="#打包情形2：双星号出现在形参前" class="headerlink" title="打包情形2：双星号出现在形参前"></a>打包情形2：双星号出现在形参前</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def pack(a, *<span class="hljs-number">*b</span>):<br>        <span class="hljs-builtin-name">print</span> type(a), a<br>        <span class="hljs-builtin-name">print</span> type(b), b<br> <br>pack(1, <span class="hljs-attribute">a1</span>=2, <span class="hljs-attribute">a2</span>=3)<br><br>&gt;&gt; &lt;type <span class="hljs-string">&#x27;int&#x27;</span>&gt; 1<br>&gt;&gt; &lt;type <span class="hljs-string">&#x27;dict&#x27;</span>&gt; &#123;<span class="hljs-string">&#x27;a1&#x27;</span>: 2, <span class="hljs-string">&#x27;a2&#x27;</span>: 3&#125;<br></code></pre></td></tr></table></figure><p>双星号打包，按照匹配顺序认领多个关键字入参，打包成一个dictionary</p><h2 id="拆解情形1：单星号出现在入参前"><a href="#拆解情形1：单星号出现在入参前" class="headerlink" title="拆解情形1：单星号出现在入参前"></a>拆解情形1：单星号出现在入参前</h2><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">def</span> pack(a, b, *c):<br>        print <span class="hljs-keyword">type</span>(a), a<br>        print <span class="hljs-keyword">type</span>(b), b<br>        print <span class="hljs-keyword">type</span>(c), c<br> <br><span class="hljs-title">score</span> = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>]<br><span class="hljs-title">pack</span>(*score)<br><br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;float&#x27;&gt; 1.0<br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;float&#x27;&gt; 2.0<br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;tuple&#x27;&gt; (3.0, 4.0)<br></code></pre></td></tr></table></figure><p>单星号拆解，将list/tuple/dictionary的内容当做独立非关键字的入参列表，其中dictionary只保留key</p><h2 id="拆解情形2：双星号出现子入参前"><a href="#拆解情形2：双星号出现子入参前" class="headerlink" title="拆解情形2：双星号出现子入参前"></a>拆解情形2：双星号出现子入参前</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pack</span>(<span class="hljs-params">*a, **b</span>):</span><br>        <span class="hljs-built_in">print</span> <span class="hljs-built_in">type</span>(a), a<br>        <span class="hljs-built_in">print</span> <span class="hljs-built_in">type</span>(b), b<br> <br>age = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>student = &#123;<span class="hljs-string">&#x27;score&#x27;</span> : <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;id&#x27;</span> : <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;name&#x27;</span> : <span class="hljs-string">&#x27;xiaoxiao&#x27;</span>&#125;<br>pack(*age, **student)<br><br>&gt;&gt; &lt;<span class="hljs-built_in">type</span> <span class="hljs-string">&#x27;tuple&#x27;</span>&gt; (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>&gt;&gt; &lt;<span class="hljs-built_in">type</span> <span class="hljs-string">&#x27;dict&#x27;</span>&gt; &#123;<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;xiaoxiao&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>双星号拆解，只针对dictionary，将其内容拆解为关键字入参列表。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>python列表的切片</title>
    <link href="/2021/07/29/python%E5%88%97%E8%A1%A8%E7%9A%84%E5%88%87%E7%89%87/"/>
    <url>/2021/07/29/python%E5%88%97%E8%A1%A8%E7%9A%84%E5%88%87%E7%89%87/</url>
    
    <content type="html"><![CDATA[<p>普通索引<code>a[i]</code>返回序列对象的一个元素，切片<code>a[::1]</code>则返回一些元素。</p><h2 id="基本索引"><a href="#基本索引" class="headerlink" title="基本索引"></a>基本索引</h2><div class="table-container"><table><thead><tr><th style="text-align:center">a中元素</th><th style="text-align:center">2</th><th style="text-align:center">4</th><th style="text-align:center">6</th><th style="text-align:center">8</th></tr></thead><tbody><tr><td style="text-align:center">非负索引</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">负数索引</td><td style="text-align:center">-4</td><td style="text-align:center">-3</td><td style="text-align:center">-2</td><td style="text-align:center">-1</td></tr></tbody></table></div><blockquote><p>非负索引从0开始，负数索引从-1开始<br>基本索引不可越界</p></blockquote><h2 id="简单切片"><a href="#简单切片" class="headerlink" title="简单切片"></a>简单切片</h2><p><code>a[start:stop]</code>返回基本索引范围在[start,stop)内的元素</p><ol><li>start/stop越界：此时有多少算多少，不报错</li><li>start &gt; stop：返回空即可</li><li>start/stop缺省：start默认无穷小，stop默认无穷大</li></ol><h2 id="扩展切片"><a href="#扩展切片" class="headerlink" title="扩展切片"></a>扩展切片</h2><h4 id="step为正数，从前往后选取"><a href="#step为正数，从前往后选取" class="headerlink" title="step为正数，从前往后选取"></a>step为正数，从前往后选取</h4><p><code>a[start:stop:step]</code>在[start,stop)区间内，依次选取索引为start, start + step, start + 2step…..直到索引值不在[start, stop)区间中</p><ol><li>越界：同样有多少算多少，不报错</li><li>start &gt; stop：返回空</li><li>缺省：同上</li></ol><h4 id="step为负数，从后向前选取"><a href="#step为负数，从后向前选取" class="headerlink" title="step为负数，从后向前选取"></a>step为负数，从后向前选取</h4><p>此时，start是stop，stop是start，而且还是左开右闭，比如<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">a</span>[<span class="hljs-number">2</span>:<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] -&gt;<span class="hljs-meta"> [6, 4]</span><br><br><span class="hljs-attribute">a</span>[:<span class="hljs-number">1</span>:-<span class="hljs-number">2</span>] -&gt;<span class="hljs-meta"> [8]</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>python笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>切片</tag>
      
      <tag>花式索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nvidia-smi突然跪了！</title>
    <link href="/2021/07/26/nvidia-smi%E7%AA%81%E7%84%B6%E8%B7%AA%E4%BA%86%EF%BC%81/"/>
    <url>/2021/07/26/nvidia-smi%E7%AA%81%E7%84%B6%E8%B7%AA%E4%BA%86%EF%BC%81/</url>
    
    <content type="html"><![CDATA[<h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>看着代码唱着歌，CUDA就突然啥啥啥初始化失败了。</p><p>nvidia-smi一看，又是一个新报错Failed to initialize NVML: Driver/library version mismatch，闹心。</p><p>整半天，大概意思是作为内核模块的nvidia驱动程序版本和libnvidia-compute-460这个库的版本不一致。</p><p>两者一般都应该是安装驱动的时候指定的版本（for me is 460.84），但是后者竟然通过ubuntu的后台更新程序自动更新了（460.91），所以造成了这个问题。</p><p>最后，更新了驱动程序，关闭了自动更新，reboot，告一段落。</p><h2 id="用到的命令："><a href="#用到的命令：" class="headerlink" title="用到的命令："></a>用到的命令：</h2><blockquote><p>lsmod | grep nvidia*<br>显示和nvidia有关的内核模块有哪些</p><p>modinfo nvidiaxxx | grep version<br>显示这些内核模块的版本</p><p>dpkg —list | grep nvidia<br>显示libnvidia-compute这个库的版本</p><p>dmesg | tail -4<br>nvidia-smi的报错详情，可以看到版本差异<br>dmesg | grep nvidia<br>nvidia-smi的执行过程详情</p><p>cat /var/log/apt/history.log | grep -a -C 10 nvidia<br>libnvidia-compute的更新记录，可以看到是unattended-upgrade造成的更新</p><p>cat /etc/apt/apt.conf.d/20auto-upgrades<br>自动更新的配置文件，修改即可关闭自动更新<a href="https://unix.stackexchange.com/questions/342663/how-is-unattended-upgrades-started-and-how-can-i-modify-its-schedule">detail</a></p><p>sudo apt install nvidia-driver-460<br>更新驱动程序</p></blockquote><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://stackoverflow.com/questions/62250491/nvml-driver-library-mismatch-after-libnvidia-compute-update">https://stackoverflow.com/questions/62250491/nvml-driver-library-mismatch-after-libnvidia-compute-update</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>生活记录</category>
      
      <category>电脑设置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Nvidia</tag>
      
      <tag>驱动失效</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch模型搭建</title>
    <link href="/2021/07/22/pytorch%E5%9F%BA%E7%A1%80/"/>
    <url>/2021/07/22/pytorch%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h3 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch.nn as nn<br><span class="hljs-attribute">import</span> torch.nn.functional as F<br><br><span class="hljs-attribute">class</span> LeNet(nn.Module):<br><span class="hljs-comment"># 设定有哪些网络层，每层的尺寸</span><br>    <span class="hljs-attribute">def</span> __init__(self):<br>        <span class="hljs-attribute">super</span>(LeNet, self).__init__()<br>        <span class="hljs-attribute">self</span>.conv<span class="hljs-number">1</span> = nn.Conv<span class="hljs-number">2</span>d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">self</span>.pool<span class="hljs-number">1</span> = nn.MaxPool<span class="hljs-number">2</span>d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-attribute">self</span>.conv<span class="hljs-number">2</span> = nn.Conv<span class="hljs-number">2</span>d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">self</span>.pool<span class="hljs-number">2</span> = nn.MaxPool<span class="hljs-number">2</span>d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-attribute">self</span>.fc<span class="hljs-number">1</span> = nn.Linear(<span class="hljs-number">32</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-attribute">self</span>.fc<span class="hljs-number">2</span> = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-attribute">self</span>.fc<span class="hljs-number">3</span> = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 设定层与层之间的怎样连接</span><br>    <span class="hljs-attribute">def</span> forward(self, x):<br>        <span class="hljs-attribute">x</span> = F.relu(self.conv<span class="hljs-number">1</span>(x))    # input(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>) output(<span class="hljs-number">16</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>        <span class="hljs-attribute">x</span> = self.pool<span class="hljs-number">1</span>(x)            # output(<span class="hljs-number">16</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>)<br>        <span class="hljs-attribute">x</span> = F.relu(self.conv<span class="hljs-number">2</span>(x))    # output(<span class="hljs-number">32</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-attribute">x</span> = self.pool<span class="hljs-number">2</span>(x)            # output(<span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">x</span> = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">32</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>)       # output(<span class="hljs-number">32</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">x</span> = F.relu(self.fc<span class="hljs-number">1</span>(x))      # output(<span class="hljs-number">120</span>)<br>        <span class="hljs-attribute">x</span> = F.relu(self.fc<span class="hljs-number">2</span>(x))      # output(<span class="hljs-number">84</span>)<br>        <span class="hljs-attribute">x</span> = self.fc<span class="hljs-number">3</span>(x)              # output(<span class="hljs-number">10</span>)<br>        <span class="hljs-attribute">return</span> x<br></code></pre></td></tr></table></figure><h3 id="引入数据"><a href="#引入数据" class="headerlink" title="引入数据"></a>引入数据</h3><h4 id="数据集引入"><a href="#数据集引入" class="headerlink" title="数据集引入"></a>数据集引入</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">train_set = torchvision.datasets.CIFAR10(<span class="hljs-attribute">root</span>=<span class="hljs-string">&#x27;./data&#x27;</span>, <span class="hljs-attribute">train</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">download</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">transform</span>=transform)<br></code></pre></td></tr></table></figure><ul><li>param[1] [in] : 待引入的数据集的路径</li><li>param[2] [in] : 引入的是数据集的训练集还是测试集</li><li>param[3] [in] : 是否需要先下载</li><li>param[4] [in] : 引入前需要对数据继续哪些变换</li><li>return : 一个troch定义的class，可看做一个vector；vector元素是tuple，tuple元素是tensor，每个tensor对应一个张量化后的图片</li></ul><h4 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">train_loader = torch.utils.data.DataLoader(train_set, <span class="hljs-attribute">batch_size</span>=36, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">num_workers</span>=0)<br><br>train_data_iter = iter(train_loader)<br>img, label = train_data_iter.next()<br><br><span class="hljs-keyword">for</span> <span class="hljs-keyword">step</span>, data <span class="hljs-keyword">in</span> enumerate(train_loader, <span class="hljs-attribute">start</span>=0)<br>    img, label = data<br>    pass <span class="hljs-built_in">..</span>.<br></code></pre></td></tr></table></figure><ul><li>param[1] [in] : 待导入的数据集</li><li>param[2] [in] : batchsize</li><li>param[3] [in] : 是否打乱原序列</li><li>param[4] [in] : 启用几个线程做导入任务</li><li>return : 用于<ol><li>被enumerate返回(int, [tensor,tensor]),第一个tensor代表inputs，维度为[batch_size, c, h, w], 第二个为labels，维度为[batch_size]</li><li>被iter返回一个iterator对象，iter.next()返回[tensor, tensor]含义同上</li></ol></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch笔记</title>
    <link href="/2021/07/22/pytorch%E7%AC%94%E8%AE%B0/"/>
    <url>/2021/07/22/pytorch%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="max-sum-等操作中指定dim的含义"><a href="#max-sum-等操作中指定dim的含义" class="headerlink" title="max() sum()等操作中指定dim的含义"></a><code>max() sum()</code>等操作中指定dim的含义</h2><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">a = <span class="hljs-comment">[ <span class="hljs-comment">[ <span class="hljs-comment">[1, 2]</span>, <span class="hljs-comment">[3, 4]</span> ]</span>, <span class="hljs-comment">[ <span class="hljs-comment">[5, 6]</span>, <span class="hljs-comment">[7, 8]</span> ]</span> ]</span><br></code></pre></td></tr></table></figure><ol><li>对于这个[2, 2, 2]的三维矩阵，三维意味着[]有三层，三个2表示每一层[]内有两个元素</li><li>对dim=0求max意味着<ol><li>将<script type="math/tex">a_{000}</script>与<script type="math/tex">a_{100}</script>比较得出最大值作为<script type="math/tex">a_{00}</script></li><li>将<script type="math/tex">a_{010}</script>与<script type="math/tex">a_{110}</script>比较得出最大值作为<script type="math/tex">a_{10}</script>…</li></ol></li><li>对dim=1求max意味着<ol><li>将<script type="math/tex">a_{000}</script>与<script type="math/tex">a_{010}</script>比较得出最大值作为<script type="math/tex">a_{00}</script></li><li>将<script type="math/tex">a_{100}</script>与<script type="math/tex">a_{110}</script>比较得出最大值作为<script type="math/tex">a_{10}</script>…</li></ol></li><li>而$a_{010}$对应于，第一层[]中的第一个元素的，第二个元素的，第一个元素</li></ol><h2 id="plt-imshow-与plt-show-的区别"><a href="#plt-imshow-与plt-show-的区别" class="headerlink" title="plt.imshow()与plt.show()的区别"></a><code>plt.imshow()</code>与<code>plt.show()</code>的区别</h2><p><code>plt.imshow()</code>只是将图片与plt发生关联，但不显示，还可以继续对这张图片进行其他draw操作，最后再用<code>plt.show()</code>显示出来。</p><h2 id="dataset-dataset-loader-dataset-loader-iter的区别"><a href="#dataset-dataset-loader-dataset-loader-iter的区别" class="headerlink" title="dataset dataset_loader dataset_loader_iter的区别"></a><code>dataset</code> <code>dataset_loader</code> <code>dataset_loader_iter</code>的区别</h2><ul><li><code>train_set</code>不是Iterator，也不是Iterabel;</li><li><code>train_loader</code>不是Iterator，但是Iterable;</li><li>Iterable意味着可用于for循环被迭代, Iterator则是算法，不断用next产生需要的数据，Iterator必然Iterable</li></ul>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>python</tag>
      
      <tag>plt</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CNN的数学原理</title>
    <link href="/2021/07/15/CNN%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
    <url>/2021/07/15/CNN%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="二维卷积的数学原理"><a href="#二维卷积的数学原理" class="headerlink" title="二维卷积的数学原理"></a>二维卷积的数学原理</h2><p>不管信号处理中“卷积”和“相关”的定义如何，至少在CNN中卷积的含义是明确的。就是</p><ol><li>将卷积核(一个尺寸较小的方阵)与矩阵$\boldsymbol{A}$左上角对齐</li><li>两个矩阵逐元素相乘后求和，作为结果矩阵$\boldsymbol{B}$的左上角第一个元素</li><li>向右/下移动卷积核的位置，重复2，直到卷积核到达矩阵$\boldsymbol{A}$的右下角<div align=center><img title="CNN中卷积的数学含义" src="/img/article/juanji.gif" width="60%" height="60%" align=center></div></li></ol><p>由此带来了<strong>padding</strong>和<strong>stride</strong>的概念：</p><ul><li><strong>padding</strong>：为了解决卷积带来的尺寸缩减，在卷积之前在$\boldsymbol{A}$周围补上p圈0。</li><li><strong>stride</strong>：卷积核每次向右/下移动s个元素。</li></ul><p>综上，有以下公式计算卷积前后的尺寸变化：</p><blockquote><p>$(n,n) and (f,f) \Longrightarrow \lfloor \frac{n+2p-f}{s}+1 \rfloor$</p></blockquote><h2 id="多维度卷积的数学原理"><a href="#多维度卷积的数学原理" class="headerlink" title="多维度卷积的数学原理"></a>多维度卷积的数学原理</h2><p><strong>关键字</strong>：<strong>卷积</strong>-&gt;<strong>偏移</strong>-&gt;<strong>激活</strong></p><p>CNN模型的输入——RGB三通道图像可以看做一个三层的2D矩阵，对其卷积的含义是：</p><ol><li>将一个层数相同的卷积核与这个3层的2D矩阵左上角对齐，然后做逐元素的乘积求和，并不断移动到右下角为止</li><li>由于求和是跨通道的，因此卷积的结果是一个单层的2D矩阵。</li><li>然后，仿照DNN对这个单层的2D矩阵中的每一个元素进行偏移和激活，得到作为输出的单层的2D矩阵。</li><li>选取另一个同尺寸的卷积核重复1-3，得到另一个单层的2D矩阵</li><li>将输入量与本层所有卷积核的卷积结果堆叠后，得到一个新的多层2D矩阵，作为本层的输出。</li><li>经过若干个卷积层处理后，将最后一个卷积层输出的多层2D矩阵看做一个列向量，将这个列向量作为$\boldsymbol{a}^{[l-1]}$，送入输出层处理。</li><li>若是二分类，则输出层是一个DNN神经元；若是多分类，则输出层是一个以$softmax$为激活函数的DNN网络层。此处DNN的含义指，通过矩阵乘法产生$\boldsymbol{z}^{[l]}$，而不是卷积操作。</li></ol><blockquote><ul><li>显然输出的层数等于本层卷积核的数量，而输出的每一层的尺寸则由前文提及的公式计算得到。</li><li>与DNN不同这里的权重矩阵$\boldsymbol{W}$的维度，都是人为指定的。这也正式CNN存在的理由，减少参数。</li></ul></blockquote><h2 id="池化的数学原理"><a href="#池化的数学原理" class="headerlink" title="池化的数学原理"></a>池化的数学原理</h2><p>与卷积操作的过程相同，但是每个核与输入矩阵进行的不是卷积操作，而是选取区域最大值的操作（最大池化）或者计算区域平均值的操作（平均池化）。一般用于降维。最大池化比较常用，并且一般伴随着0 padding。</p><h2 id="CNN目标检测的基本原理"><a href="#CNN目标检测的基本原理" class="headerlink" title="CNN目标检测的基本原理"></a>CNN目标检测的基本原理</h2><h3 id="单图片，单目标"><a href="#单图片，单目标" class="headerlink" title="单图片，单目标"></a>单图片，单目标</h3><p>先说最简单的情形，即判断一张图片中有没有猫的问题。只需要将图片送入CNN，然后将模型输出向量定义为如下形式即可（假设待检测目标有3类）</p><blockquote><p>$\hat{\boldsymbol{y}} = (p,x,y,w,h,c_1,c_2,c_3)^T$</p></blockquote><p>其中</p><ul><li>$p$为这个result向量，或者说这个检测框存在目标的概率</li><li>$x,y,w,h$为这个检测框的在输入图片中的位置</li><li>$c_1, c_2, c_3$为该检测框属于各个类别的概率</li></ul><h3 id="单图片，多目标"><a href="#单图片，多目标" class="headerlink" title="单图片，多目标"></a>单图片，多目标</h3><p>如果一张图片存在多个目标——这也是实际情况中更可能发生的情况，事情就变得复杂起来。最直观并且容易理解的思路是<strong>滑动窗口搜索法</strong>：<br></p><ol><li>定义一个尺寸的框，将输入图片的左上角与这个框对齐</li><li>将被框框住的部分作为“单图片，单目标”的情形处理，问题解决</li><li>然后移动框的位置，不断向右，向下遍历整张输入图片</li><li>完成遍历之后，换一个尺寸，重复1-3</li><li>所有尺寸都尝试过之后，输入图片中的所有目标自然被检测出来</li></ol><p>很明显，上述做法计算量非常大，症结主要有</p><ol><li>每一个框都要不断移动，遍历整张图</li><li>两个要穷举所有尺寸的框</li></ol><p>对于第1个问题，可以通过<strong>滑动窗口的卷积实现</strong>来规避。而对于第二个问题，目前有两种主流思路：</p><ol><li>感兴趣区域预提取的思路<ol><li>通过图像分割，提取感兴趣区域，然后将所有感兴趣区域依次送入CNN进行预测——RCNN</li><li>用滑动窗口的卷积实现，代替RCNN中依次进行的CNN处理——Fast RCNN</li><li>用一个CNN代替图像分割进行感兴趣区域提取——Faster RCNN</li></ol></li><li>YOLO思路<br>将输入图片分割为19x19个区域，然后结合<strong>滑动窗口卷积实现</strong>的思路将整张图片送入CNN网络，得到19x19个$(p,x,y,w,h,c_1,c_2,c_3)^T$，每一个$(p,x,y,w,h,c_1,c_2,c_3)^T$表示对应区域中的目标检测结果。特别的，存在区域认领目标的概念，意思是说，当一个目标的检测框的中心在某个区域之内时，这个区域对应的result向量才会生成有效信息。基于这个概念，产生了以下两个问题：<ol><li>一个区域内就是有多个目标的检测框的中心点怎么办？</li><li>相邻的几个区域都声称同一个目标的中心点在它那里，然后都生成了几乎相同的有效的result向量怎么办？<br>对于问题1，采用anchor box应对，问题2则采用NMS应对。</li></ol></li></ol><h2 id="滑动窗口的卷积实现原理"><a href="#滑动窗口的卷积实现原理" class="headerlink" title="滑动窗口的卷积实现原理"></a>滑动窗口的卷积实现原理</h2><p>将CNN网络模型中最后的几个全连接层看作卷积层。因为从数学角度看，卷积层和全连接层是一样的，因为这400个节点中每个节点的$\boldsymbol{w}$向量都可以看作一个5×5×16的过滤器，所以不论是把最后这几层看作全连接层还是卷积层，他们的输出都是上一层输出经过某个任意线性函数处理后的结果。所以本质上就是一个如何看的问题，计算的本质过程是不变的。这么做的好处是，可以把输入的19x19个区域与输出的19x19个result做有意义的关联了。</p><div align=center><img title="滑动窗口的卷积实现" src="/img/article/hdck.png" width="60%" height="60%" align=center></div><h2 id="anchor-box的工作原理"><a href="#anchor-box的工作原理" class="headerlink" title="anchor box的工作原理"></a>anchor box的工作原理</h2><p>将某个区域认领某个目标的对应关系再细分一级，变成某个区域的某个anchor box认领某个目标。这样，当一个区域内有多个中心点时，再额外的计算一下每个目标与每个预设的anchor box之间的交并比之后，即可将目标细分到anchor box。这时，每个区域便可以认领多个不同种类的目标。一般通过聚类训练集中的目标进行anchor box的尺寸设定。</p><h2 id="NMS的工作原理"><a href="#NMS的工作原理" class="headerlink" title="NMS的工作原理"></a>NMS的工作原理</h2><p>对于所有19x19个result向量，一类一类的来看，比如先看归属于vehicle类的box：</p><ol><li>选取这类box中scores最大的哪一个，记为box_best，并保留它</li><li>计算box_best与其余的box的IOU</li><li>如果其IOU&gt;0.5了，那么就舍弃这个box（由于可能这两个box表示同一目标，所以保留分数高的哪一个）</li><li>从最后剩余的boxes中，再找出最大scores的哪一个，如此循环往复</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DNN的数学原理</title>
    <link href="/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
    <url>/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="神经网络模型的结构"><a href="#神经网络模型的结构" class="headerlink" title="神经网络模型的结构"></a>神经网络模型的结构</h2><p><strong>神经网络</strong>的基础单元是<strong>神经元</strong>，多个<strong>神经元</strong>纵向堆叠形成神经<strong>网络层</strong>，神经<strong>网络层</strong>横向堆叠形成<strong>神经网络</strong>。</p><div align=center><img title="" src="/img/article/神经网络的数学原理/神经网络.png" width="50%" height="50%" align=center></div><br><div align=center><img title="" src="/img/article/神经网络的数学原理/神经元.png" width="60%" height="60%" align=center></div><h2 id="神经元的数学原理"><a href="#神经元的数学原理" class="headerlink" title="神经元的数学原理"></a>神经元的数学原理</h2><p>对于一个神经元，进行的数学计算为：</p><blockquote><p>接受一个向量$\boldsymbol{a}^{[j-1]}$，通过与$\boldsymbol{w}^{[j]}_i$进行向量内积运算产生一个中间值$z_i^{[j]}$（标量），然后用激活函数$g_i^{[j]}()$将$z$转换为$a_i^{[j]}$。</p></blockquote><p>其中:</p><blockquote><p>上标用来定位该神经元位于哪一层，一般输入层后的第一层为1；<br>下标用来定位该神经元位于第几个，一般最上方的序号为0；</p></blockquote><h2 id="矩阵维度确认的数学原理"><a href="#矩阵维度确认的数学原理" class="headerlink" title="矩阵维度确认的数学原理"></a>矩阵维度确认的数学原理</h2><p>首先区分开这4个概念：<strong>模型的参数</strong>，<strong>层的参数</strong>，<strong>神经元的参数</strong>，<strong>数据及数据的中间值</strong>。然后，仔细理解上面两段话，神经网络中最为tricky的维度问题便迎刃而解：</p><ol><li>对于$\boldsymbol{x}$和$\boldsymbol{y}$，其维度看样本就知道，已经定义好了;</li><li>对于某一个神经元的权重参数$\boldsymbol{w}^{[j]}_i$，由于要跟输入的向量$\boldsymbol{a}^{[j-1]}$进行内积，所以两者的维度必然是相同的，而后者作为一个列向量，其行数等于上一层的神经元数量（因为每个神经元输出一个标量）。然后由于本层的每一个神经元都有一个权重参数$\boldsymbol{w}^{[j]}$，那么由${\boldsymbol{w}^{[j]}}^T$纵向堆叠形成的${\boldsymbol{W}^{[j]}}^T$的行数就是$\boldsymbol{w}^{[j]}$的个数，亦即本层的神经元数量，其列数前面已经说了，就是$\boldsymbol{w}^{[j]}$的行数，亦即上一层的神经元数量。<br></li><li>对于某一个神经元的偏移量参数${b}^{[j]}$，自然是一个标量。那么本层的偏移量参数$\boldsymbol{b}^{[j]}$的行数就是${b}^{[j]}$的数量，亦即本层的神经元数量。</li><li>对于每一层的中间值$\boldsymbol{z}^{[j]}$，输出值$\boldsymbol{a}^{[j]}$，其维度确定方式与$\boldsymbol{b}^{[j]}$一样。</li><li>另外对于激活函数，一般同一层都一样，所以$\boldsymbol{g}^{[j]}()$退化为${g}^{[j]}()$。</li></ol><h2 id="矢量化的数学原理"><a href="#矢量化的数学原理" class="headerlink" title="矢量化的数学原理"></a>矢量化的数学原理</h2><p>矢量化的本质是将样本在时间轴上被神经网络模型处理的序列转化为空间上的序列：<br></p><blockquote><p>$X= (\boldsymbol{x}^1,\boldsymbol{x}^2, …,  \boldsymbol{x}^)$</p></blockquote><p>说人话就是，原来每次送入模型一个列向量，计算得到一个列向量。现在每次送入m个列向量，计算的到m个列向量。当然了，各层的中间值$\boldsymbol{z}^{[j]}$和输出值$\boldsymbol{a}^{[j]}$也都将因此横向扩充一个维度。</p><h2 id="Batch的数学原理"><a href="#Batch的数学原理" class="headerlink" title="Batch的数学原理"></a>Batch的数学原理</h2><p>一个batch指每次送入模型的一批样本。比如，现有2000个样本，将其划分为4个batch，那么每个batch包含500个样本，即</p><blockquote><p>batch_size=500;<br>batch_num = 4;</p></blockquote><p>一个epoch指整个训练集被利用了一次。即，2000个样本中的每一个都被代入模型进行了一次前向计算和反向传播。</p><p>一个iteration指权重参数更新一次。一个epoch中可能有多个iteration，也可能只有一个iteration，这取决于batch_num的值。</p><p>留一个疑问，在一个epoch中，一个batch会循环多次使用吗？还是只用1次就结束了？例如，在一个epoch中用batch_1迭代10次，然后batch_2迭代10次，….，最后batch_4迭代10次，一个epoch完成。是这样吗？<em>——<a href="https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks">目前来看不是这样，而是只用1次。 2021年7月15日</a></em></p><h2 id="正向计算的数学原理"><a href="#正向计算的数学原理" class="headerlink" title="正向计算的数学原理"></a>正向计算的数学原理</h2><p>正向计算，用于得到所需的预测结果：</p><ol><li>输入一个列向量：<br><blockquote><p> $\boldsymbol{x} = (x_1, x_2, … , x_n)^T$</p></blockquote></li><li><p>进行一系列矩阵计算：<br></p><blockquote><p> $\boldsymbol{a}^{[0]} = \boldsymbol{x}$<br> $\boldsymbol{z}^{[1]} = {\boldsymbol{W}^{[1]}}^T\boldsymbol{a}^{[0]}+\boldsymbol{b}^{[1]}$<br> $\boldsymbol{a}^{[2]} = g^{[1]}(\boldsymbol{z}^{[1]})$<br> …<br> $\boldsymbol{z}^{[j]} = {\boldsymbol{W}^{[j]}}^T\boldsymbol{a}^{[j-1]}+\boldsymbol{b}^{[j]}$<br> $\boldsymbol{a}^{[j]} = g^{[j]}(\boldsymbol{z}^{[j]})$<br> …<br> $\boldsymbol{z}^{[l]} = {\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}$<br> $\boldsymbol{a}^{[l]} = g^{[l]}(\boldsymbol{z}^{[l]})$<br> $\boldsymbol{\hat{y}} = \boldsymbol{a}^{[l]}$</p></blockquote><p>特别地，对于多分类模型的输出层（最后一层），一般有：</p><blockquote><p>$g^{[l]}(\boldsymbol{x}) = softmax(\boldsymbol{x}) = \frac{exp(\boldsymbol{x})}{\boldsymbol{1}^Texp(\boldsymbol{x})}$</p></blockquote><p>其中 $\boldsymbol{1}$ 为全1列向量，维度可从context推得。</p></li><li>得到一个列向量：<br><blockquote><p> $\boldsymbol{\hat{y}} = (\hat{y}_1,\hat{y}_2, … , \hat{y}_n)^T$</p></blockquote></li></ol><h2 id="反向传播的数学原理"><a href="#反向传播的数学原理" class="headerlink" title="反向传播的数学原理"></a>反向传播的数学原理</h2><p>反向传播，用于得到各网络层参数的更新量。主要有以下3个步骤：</p><ol><li>单个样本$\boldsymbol{x}$ 正向计算，得到各层的$\boldsymbol{z}$和$\boldsymbol{a}$备用。</li><li>计算输出层的参数更新量：<ol><li>求得损失$L$对$\boldsymbol{z}^{[l]}$的偏导$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}$，记作$d\boldsymbol{z}^{[l]}$。上标$l$表示最后一层，即输出层。若输出层激活函数为$softmax$且损失函数为交叉熵，则有$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]}) - \boldsymbol{y}$，对应的求导过程如下：<blockquote><p>$L = -\boldsymbol{y}^Tlog\hat{\boldsymbol{y}}$<br>$\downarrow$<br>$dL = -d\boldsymbol{y}^Tlog\hat{\boldsymbol{y}}-\boldsymbol{y}^Td(log\hat{\boldsymbol{y}})$<br>$= -\boldsymbol{y}^T d(log\hat{\boldsymbol{y}})$<br>$= -\boldsymbol{y}^Td(log(softmax(\boldsymbol{z}^{[l]})))$<br>$= -\boldsymbol{y}^Td(log(\frac{exp(\boldsymbol{\boldsymbol{z}^{[l]}})}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}))$<br>$= -\boldsymbol{y}^Td(log(exp(\boldsymbol{\boldsymbol{z}^{[l]}}))+\boldsymbol{1}log(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})))$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + d(log(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})))$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + \frac{d(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}}))}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + \frac{\boldsymbol{1}^T(exp(\boldsymbol{\boldsymbol{z}^{[l]}}) \odot d\boldsymbol{z}^{[l]} )}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + {\frac{exp(\boldsymbol{\boldsymbol{z}^{[l]}})}{\boldsymbol{1}^Texp(\boldsymbol{z}^{[l]})}}^Td\boldsymbol{z}^{[l]}$<br>$= (softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T)^Td\boldsymbol{z}^{[l]}$<br>$\downarrow$<br>$tr(dL) = dL = tr((softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T)^Td\boldsymbol{z}^{[l]}) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}} = softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T$<br>$\downarrow$<br>$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T$</p></blockquote></li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{z}^{[l]}$得到$d{\boldsymbol{W}^{[l]}}^T$，$d\boldsymbol{b}^{[l]}$，$d\boldsymbol{a}^{[l-1]}$<ul><li>$d{\boldsymbol{W}^{[l]}}^T$的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td{\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]})$<br>$= tr(\boldsymbol{a}^{[l-1]}{\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td{\boldsymbol{W}^{[l]}}^T)$<br>$= tr(({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}{\boldsymbol{a}^{[l-1]}}^T)^Td{\boldsymbol{W}^{[l]}}^T)$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{W}^{[l]}}^T} = {\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}{\boldsymbol{a}^{[l-1]}}^T$<br>$\downarrow$<br>$d{\boldsymbol{W}^{[l]}}^T = d\boldsymbol{z}^{[l]}{\boldsymbol{a}^{[l-1]}}^T$</p></blockquote></li><li>$d\boldsymbol{b}^{[l]}$的求导过程<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{b}^{[l]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{b}^{[l]}}} = {\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}$<br>$\downarrow$<br>$d\boldsymbol{b}^{[l]} = d\boldsymbol{z}^{[l]}$</p></blockquote></li><li>$d\boldsymbol{a}^{[l-1]}$的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^T{\boldsymbol{W}^{[l]}}^Td\boldsymbol{a}^{[l-1]})$<br>$= tr((\boldsymbol{W}^{[l]}\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}})^Td\boldsymbol{a}^{[l-1]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} = {\boldsymbol{W}^{[l]}\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}$<br>$\downarrow$<br>$d\boldsymbol{a}^{[l-1]} = \boldsymbol{W}^{[l]}d\boldsymbol{z}^{[l]}$</p></blockquote></li></ul></li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{a}^{[l-1]}$得到$d\boldsymbol{z}^{[l-1]}$，对应的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^Td\boldsymbol{a}^{[l-1]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^Td(g(\boldsymbol{z}^{[l-1]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^T(g’(\boldsymbol{z}^{[l-1]}) \odot d\boldsymbol{z}^{[l-1]}))$<br>$= tr((\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} \odot g’(\boldsymbol{z}^{[l-1]}))^T d\boldsymbol{z}^{[l-1]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l-1]}}} = \frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} \odot g’(\boldsymbol{z}^{[l-1]})$<br>$\downarrow$<br>$d\boldsymbol{z}^{[l-1]} = d\boldsymbol{a}^{[l-1]}\odot g’(\boldsymbol{z}^{[l-1]})$</p></blockquote></li></ol></li><li>仿照输出层的计算方式，反向传播，依次得到各层的参数更新量。<blockquote><p>$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]}) - \boldsymbol{y}$<br>$d{\boldsymbol{W}^{[l]}}^T = d\boldsymbol{z}^{[l]}{\boldsymbol{a}^{[l-1]}}^T$ AND $d\boldsymbol{b}^{[l]} = d\boldsymbol{z}^{[l]}$<br>…<br>$d\boldsymbol{z}^{[j]} = (\boldsymbol{W}^{[j+1]}d\boldsymbol{z}^{[j+1]}) \odot g’(\boldsymbol{z}^{[j]})$<br>$d{\boldsymbol{W}^{[j]}}^T = d\boldsymbol{z}^{[j]}{\boldsymbol{a}^{[j-1]}}^T$ AND $d\boldsymbol{b}^{[j]} = d\boldsymbol{z}^{[j]}$<br>$d\boldsymbol{z}^{[j-1]} = (\boldsymbol{W}^{[j]}d\boldsymbol{z}^{[j]}) \odot g’(\boldsymbol{z}^{[j-1]})$<br>…<br>$d{\boldsymbol{W}^{[1]}}^T = d\boldsymbol{z}^{[1]}{\boldsymbol{a}^{[0]}}^T$ AND $d\boldsymbol{b}^{[1]} = d\boldsymbol{z}^{[1]}$<br>$d\boldsymbol{z}^{[0]} = (\boldsymbol{W}^{[1]}d\boldsymbol{z}^{[1]}) \odot g’(\boldsymbol{z}^{[0]})$</p></blockquote></li></ol><h2 id="梯度下降的数学原理"><a href="#梯度下降的数学原理" class="headerlink" title="梯度下降的数学原理"></a>梯度下降的数学原理</h2><p>梯度下降的思路：</p><ol><li>将整个模型视为一个以模型参数为自变量，以L为因变量，以样本数据为参数的函数</li><li>然后将模型参数寻优的问题转化为求这个函数最值和驻点的问题</li><li>然后基于这样一个原理<strong>自变量的数值沿梯度方向靠近时因变量的数值将随之向最值靠近</strong>，便可得到梯度下降公式：<blockquote><p>$\boldsymbol{W} -= \alpha d\boldsymbol{W}$<br>$\boldsymbol{b} -= \alpha d\boldsymbol{b}$</p></blockquote></li></ol><p>然后根据不同的实现，又有以下的梯度下降法变种：</p><ol><li>基于batch_size的不同<ul><li>batch_size = sample_size，称Batch Gradient Descent；</li><li>batch_size = 1，称Stocastic Gradient Descent；</li><li>batch_size介于两者之间，称Mini-Batch Gradient Descent;</li></ul></li><li>每次不是简单的减去更新量，而是减去更新量的移动平均值，即可得到GD with Momentum；其中移动平均的含义是取前n次更新量的平均值作为本次的更新量，$n=\frac{1}{1-\beta}$，$\beta$一般取0.9；<blockquote><p>$v^{d\boldsymbol{W}} = \beta v^{d\boldsymbol{W}} + (1-\beta)d\boldsymbol{W}$<br><br>$v^{d\boldsymbol{b}} = \beta v^{d\boldsymbol{b}} + (1-\beta)d\boldsymbol{b}$<br><br>$\boldsymbol{W} -= \alpha v^{d\boldsymbol{W}}$<br><br>$\boldsymbol{b} -= \alpha v^{d\boldsymbol{W}}$<br></p></blockquote></li><li>在上面的基础上，如果将用更新量的平方进行移动平均，然后再对移动平均值开方得到本次的更新量，即可得到RMSprop；<blockquote><p>$S^{d\boldsymbol{W}} = \beta S^{d\boldsymbol{W}} + (1-\beta)({d\boldsymbol{W}})^2$<br><br>$S^{d\boldsymbol{b}} = \beta S^{d\boldsymbol{b}} + (1-\beta)({d\boldsymbol{b}})^2$<br><br>$\boldsymbol{W} -= \alpha \frac{d\boldsymbol{W}}{\sqrt{S^{d\boldsymbol{W}}}}$<br><br>$\boldsymbol{b} -= \alpha \frac{d\boldsymbol{b}}{\sqrt{S^{d\boldsymbol{b}}}}$<br></p></blockquote></li><li>将2和3合并起来使用，即可得到Adam；<blockquote><p>$v^{d\boldsymbol{W}} = \beta_1 v^{d\boldsymbol{W}} + (1-\beta_1)d\boldsymbol{W}$<br><br>$v^{d\boldsymbol{b}} = \beta_1 v^{d\boldsymbol{b}} + (1-\beta_1)d\boldsymbol{b}$<br><br>$S^{d\boldsymbol{W}} = \beta_2 S^{d\boldsymbol{W}} + (1-\beta_2)({d\boldsymbol{W}})^2$<br><br>$S^{d\boldsymbol{b}} = \beta_2 S^{d\boldsymbol{b}} + (1-\beta_2)({d\boldsymbol{b}})^2$<br><br>$\boldsymbol{W} -= \alpha \frac{v^{d\boldsymbol{W}}}{\sqrt{S^{d\boldsymbol{W}}}+\epsilon}$<br><br>$\boldsymbol{b} -= \alpha \frac{v^{d\boldsymbol{b}}}{\sqrt{S^{d\boldsymbol{b}}}+\epsilon}$<br></p></blockquote></li></ol><h2 id="归一化的数学原理"><a href="#归一化的数学原理" class="headerlink" title="归一化的数学原理"></a>归一化的数学原理</h2><p>在向量化和Mini-Batch的前提下，每一层的中间值$Z^{[j]}$先进行跨样本的normalization之后再进行激活，就是所谓的归一化。</p><blockquote><p>矩阵$Z^{[j]}$按行求算数平均，得到均值列向量$\bar{z}$；<br>矩阵$Z^{[j]}$与均值列向量$\bar{z}$进行标准差运算，得到标准差矩阵$\boldsymbol{\Sigma}$；<br>矩阵$Z^{[j]}$的每一列都减去均值列向量$\bar{z}$后，再逐元素除以标准差矩阵$\boldsymbol{\Sigma}$即可得到归一化后的新中间值矩阵。</p></blockquote><p>另外归一化还存在一个小问题，就是他的前提是向量化和Mini-Batch，就是说跨样本求均值和方差的基础是有多个样本。但是在完成模型训练之后进行预测时，肯定都是每次喂到模型中一个样本，那么此时如何求均值和方差呢？毕竟最好怎么训练出来的怎么用嘛。</p><p>一般的做法是，在这个训练过程中用移动平均数实时追踪均值和方差，或者用整个训练集的均值和方差也行，问题不大，而且主流的DL框架一般都封装好了。</p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DNN</tag>
      
      <tag>神经网络</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CSAPP并发编程总结</title>
    <link href="/2021/07/09/CSAPP%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <url>/2021/07/09/CSAPP%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="并发编程基本概念"><a href="#并发编程基本概念" class="headerlink" title="并发编程基本概念"></a>并发编程基本概念</h2><ul><li><strong>并发</strong>：多个逻辑控制流的生命周期有重叠，即称为<strong>并发现象</strong>(concurrency)</li><li><strong>并行</strong>：发生在多核/多计算机上的并发现象（在一个时刻上存在多个逻辑控制流），称为<strong>并行现象</strong>（parallel），是并发现象的真子集；</li></ul><h2 id="并发程序的三种构造方式："><a href="#并发程序的三种构造方式：" class="headerlink" title="并发程序的三种构造方式："></a>并发程序的三种构造方式：</h2><ul><li><strong>进程</strong>：每个逻辑控制流实现为一个进程<ul><li>特点：独立的虚拟地址空间</li><li>优点：独立则不易混淆</li><li>缺点：<ul><li>独立则难以共享数据</li><li>进程context切换和IPC开销高，所以往往比较慢（ 进程间通信机制）</li></ul></li></ul></li><li><strong>I/O多路复用</strong>：状态机化，逻辑控制流的切换实现为状态机的状态切换。具体原理看<a href="https://www.zhihu.com/question/32163005/answer/55772739">这个</a><ul><li>优点：共享数据容易，并且没有进程context切换的开销</li><li>缺点：编码复杂，不能充分利用多核处理器</li></ul></li><li><strong>线程</strong>：重点，下面展开讲。</li></ul><h2 id="线程基本概念："><a href="#线程基本概念：" class="headerlink" title="线程基本概念："></a>线程基本概念：</h2><ul><li><strong>主线程</strong>：进程中第一个运行的线程</li><li><strong>对等线程</strong>：进程中后来运行的线程</li><li><strong>与进程的区别</strong>：<ul><li>上下文内容少，切换更快，开销更少，具体包括：线程ID、栈和栈指针、PC、条件码、register value</li><li>一个进程的所有线程（对等线程池）彼此之间没有层次结构，都是对等的；</li><li>对等线程之间共享进程的虚拟地址空间，可以等待另外一个对等线程终止或主动杀死它</li></ul></li><li><strong>共享变量</strong>：一个变量的一个实例被不止一个线程引用，那么这个变量称为共享变量</li><li><strong>线程安全的函数</strong>：被多个并发线程反复调用时能够一直产生正确结果的函数称为线程安全函数</li><li><strong>可重入函数</strong>：线程安全函数的一个真子集，指不会引入任何共享数据的函数<ul><li><strong>显式可重入</strong>：传参均为值传递（且非指针值传递），而且所有数据引用的都是本地自动栈变量</li><li><strong>隐式可重入</strong>：在显式的基础上取消“值传递（且非指针值传递）”的限制，允许指针值传递和引用传递，但是传递的变量都是非共享变量时，该函数是隐式可重入的</li></ul></li><li><strong>竞争</strong>：程序的正确性依赖于某条/某些特定的轨迹线，或者说不是全部的轨迹线都能让程序正确执行，哪怕是那些绕过了互斥锁禁止区的全部轨迹线也不行。（具体例子见CSAPP P719）</li></ul><h2 id="Posix线程模型："><a href="#Posix线程模型：" class="headerlink" title="Posix线程模型："></a>Posix线程模型：</h2><p>管理Linux线程的C语言接口包<pthread.h>，包含大约60个函数。</p><ul><li><strong>线程例程概念</strong>：接受和返回一个void指针的函数类型，其内容为线程真正要做的事。若输入输出的参数较多，应封装为一个结构体。</li><li><strong>常用的pthread函数</strong>：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/*创建线程*/</span><br>int pthread_creat(&amp;tid, NULL, thread, NULL)<br><span class="hljs-regexp">//</span>(返回线程ID，设置线程属性(高阶内容)，线程例程函数名，线程例程函数的传参)，成功返回<span class="hljs-number">0</span>，否则非<span class="hljs-number">0</span><br><br><span class="hljs-regexp">/*终止线程*/</span><br><span class="hljs-regexp">//</span>方式<span class="hljs-number">1</span>：某个对等线程的例程函数执行完毕，该线程会隐式终止<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">2</span>：某个对等线程调用pthread_exit函数，线程会显式终止，而且如果是主线程调用，它会等待所有其他对等线程终止后再终止（进程也被终止了）<br>void pthread_exit(void *thread_return) <span class="hljs-regexp">//</span>函数不返回（因为逻辑控制流都结束了啊），会将一些信息写到thread_return中<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">3</span>：某个对等线程调用系统<span class="hljs-keyword">exit</span>函数，终止其所属进程及该进程所有的线程<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">4</span>：某个对等线程调用pthread_cancel函数，终止另一个对等线程<br>int pthread_cancel(pthread_t tid)  <span class="hljs-regexp">//</span>终止线程ID为tid的对等线程，成功返回<span class="hljs-number">0</span>，否则非<span class="hljs-number">0</span><br><br><span class="hljs-regexp">/*回收已终止线程的资源*/</span><br>int pthread_join(pthread_t tid, void**thread_return) <span class="hljs-regexp">//</span>调用该函数的对等线程阻塞，等待线程ID为tid的对等线程结束，然后回收其资源后返回<br><br><span class="hljs-regexp">/*分离线程*/</span><br><span class="hljs-regexp">//</span>一个线程的状态要么是detached要么是joinable，处于后者时意味着可以被其他线程杀死和回收资源，前者不可（自行终止，系统回收资源）<br>int pthread_detach(pthread_t tid)  <span class="hljs-regexp">//</span>调用该函数的对等线程将线程ID为tid的线程分离<br><br><span class="hljs-regexp">/*获取自身ID*/</span><br>pthread_t pthread_self();<br></code></pre></td></tr></table></figure></li></ul><h2 id="线程的内存模型（两个关键问题）"><a href="#线程的内存模型（两个关键问题）" class="headerlink" title="线程的内存模型（两个关键问题）"></a>线程的内存模型（两个关键问题）</h2><ul><li><strong>线程的内存模型是怎样的？</strong>——不是整齐清楚的。。。</li><li><strong>变量的实例如何映射到线程的内存模型中？</strong><ul><li>全局变量+局部的静态变量：一个进程中只有一个实例，任何线程均可引用；</li><li>局部的自动变量：多个实例，由各个线程栈自行管理。</li></ul></li></ul><h2 id="线程共享变量的冲突问题"><a href="#线程共享变量的冲突问题" class="headerlink" title="线程共享变量的冲突问题"></a>线程共享变量的冲突问题</h2><p>关键字：进度图-&gt;信号量-&gt;PV操作-&gt;互斥锁-&gt;互斥锁加/解锁-&gt;死锁</p><ul><li><strong>进度图</strong>：注意把P/V操作放到线段上，状态放到端点上，这样端点的状态即可解释为执行P/V操作前或者P/V操作后</li><li><strong>信号量</strong>s：一个非负整数全局变量</li><li><strong>PV操作</strong>（原子操作）：<ul><li>P(s)操作：检查s是否为0；<ul><li>是，则调用该函数的线程在此处阻塞；</li><li>否，则将s减1后继续向下执行；</li></ul></li><li>V(s)操作：先将s加1，然后检查有么有因为P(s)阻塞的线程，如有则将完成其P操作，然后置为就绪状态（等待调度），若没有那就没有。。若有不止一个，就随机选择一个，反正只能一个（因为要完成P操作啊）</li></ul></li><li><strong>互斥锁</strong>：二元的信号量</li><li><strong>互斥锁加/解锁</strong>：针对二元信号量的P/V操作</li><li><strong>死锁</strong>：<br>禁止区外存在这样一些状态点，既不能向右，也不能向上，因为向上会进入线程A的禁止区，向右会进入线程B的禁止区。<br><br>通过以下原则来防止死锁：<br>给定所有互斥操作的一个全序( 全序概念)，如果每个线程都是以该全序获得互斥锁并以相反的顺序（不是说全序的逆序，而是线程A和线程B释放的顺序相反）释放，那么这个程序就不会出现死锁。（但是该原则现在看下来只适用于两个线程，更多的线程就要用到更复杂的银行家算法了）<br><br>例如：<br><ul><li>线程1： P(s) -&gt; P(t) -&gt; V(t) -&gt; V(s);</li><li>线程2： P(s) -&gt; P(t) -&gt; V(s) -&gt; V(t);</li></ul></li></ul><h2 id="并行程序的性能量化（暂时略过）"><a href="#并行程序的性能量化（暂时略过）" class="headerlink" title="并行程序的性能量化（暂时略过）"></a>并行程序的性能量化（暂时略过）</h2><h2 id="信号量用于共享资源调度（暂时略过）"><a href="#信号量用于共享资源调度（暂时略过）" class="headerlink" title="信号量用于共享资源调度（暂时略过）"></a>信号量用于共享资源调度（暂时略过）</h2>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>并发编程</tag>
      
      <tag>线程</tag>
      
      <tag>信号量</tag>
      
      <tag>锁</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>个人博客搭建</title>
    <link href="/2021/07/08/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <url>/2021/07/08/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于强迫症患者，非要自定义域名的话，先提前注册好域名，推荐阿里云，审核快。不然下面的流程会阻塞。。。</p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node.js"></a>安装node.js</h2><ol><li>去官网 <a href="https://nodejs.org">https://nodejs.org</a> 查看最新版本号</li><li>根据最新版本号添加源，然后安装<blockquote><p>curl -sL <a href="https://deb.nodesource.com/setup_10.x">https://deb.nodesource.com/setup_10.x</a> | sudo -E bash<br>sudo apt-get install -y nodejs<br></p></blockquote></li><li>查看版本，确认安装成功<blockquote><p>node -v<br>npm -v</p></blockquote></li></ol><h2 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h2><p>上交公钥，然后设置用户名+邮箱</p><blockquote><p>git config —global user.name “William_2580”<br>git config —global user.email “1611134972@qq.com”</p></blockquote><h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><blockquote><p>npm install -g hexo-cli</p></blockquote><h2 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h2><blockquote><p>mkdir hexoblog<br>hexo init hexoblog<br>hexo server<br>然后浏览器访问<a href="http://localhost:4000">http://localhost:4000</a>, 即可看到部署在本地的个人网站</p></blockquote><h2 id="云端部署"><a href="#云端部署" class="headerlink" title="云端部署"></a>云端部署</h2><ol><li>github创建repo，名称必须为：Winliam.github.io</li><li>安装hexo部署工具：<blockquote><p>npm install hexo-deployer-git —save</p></blockquote></li><li>修改hexoblog下的_config.yml中的deploy段：<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">type: git<br>repo: git@github<span class="hljs-selector-class">.com</span>:Winliam/Winliam<span class="hljs-selector-class">.github</span><span class="hljs-selector-class">.io</span><span class="hljs-selector-class">.git</span><br>branch: main<br></code></pre></td></tr></table></figure></li><li>部署：<blockquote><p>hexo d //public文件夹下的内容被推送到刚才创建的repo中</p></blockquote></li><li>然后进入repo的setting-&gt;pages页面选择正确的分支后，即可通过Winliam.github.io访问blog了</li></ol><h2 id="自定义域名"><a href="#自定义域名" class="headerlink" title="自定义域名"></a>自定义域名</h2><ol><li>阿里云域名注册购买</li><li>买到后阿里云中继续设置域名解析，完成后等待10min等DNS生效(使得www.guohongming.xyz和guohongming.xyz都可以被访问到)</li><li>在本地public文件夹添加CNAME文件(内容为guohongming.xyz)后重新部署一次</li><li>在云端repo的setting-&gt;pages页面将guohongming.xyz添加到Custom domain并保存，不用管warning</li></ol><h2 id="主题美化"><a href="#主题美化" class="headerlink" title="主题美化"></a>主题美化</h2><p><strong>安装fluid主题</strong></p><ol><li>在blog文件夹下<blockquote><p>npm install —save hexo-theme-fluid</p></blockquote></li><li>将GitHub上的<a href="https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml">_config.yml</a>文件内容复制到本地的_config.fluid.yml中 </li><li><p>修改本地_config.yml的内容，以应用主题</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">theme:</span> fluid  <span class="hljs-meta"># 指定主题</span><br><span class="hljs-symbol">language:</span> <span class="hljs-built_in">zh</span>-CN  <span class="hljs-meta"># 指定语言，会影响主题显示的语言，按需修改</span><br></code></pre></td></tr></table></figure></li></ol><p>此时新建文章后便会自动生成fluid的文章，包括自动生成目录等</p><h2 id="评论插件"><a href="#评论插件" class="headerlink" title="评论插件"></a>评论插件</h2><ol><li>选型，综合测评，twikoo最好</li><li>注册腾讯云<a href="https://blog.zhheo.com/p/2e6bbbd0.html">参考这里</a>，建立云开发环境，建立云函数，添加域名，下载秘钥</li><li>将云开发环境的ID写入到fluid的配置yml中</li><li>根据twikoo版本修改，fluid配置yml中的CND地址</li><li>此时部署后已经可以看到插件，此时点击齿轮再将秘钥内容粘贴进去即可设置管理员密码</li><li>注册server酱，注册企业微信，按照server酱官网教程获得SENDKEY，将SENDKEY粘贴到评论插件的管理面板中，有新评论即可通过企业微信通知</li></ol><h2 id="看板娘"><a href="#看板娘" class="headerlink" title="看板娘"></a>看板娘</h2><h2 id="流量统计插件"><a href="#流量统计插件" class="headerlink" title="流量统计插件"></a>流量统计插件</h2><ol><li>注册LeanCloud，需要实名验证</li><li>创建app</li><li>将app的ID, KEY, API地址粘贴到配置文件的相应位置即可<blockquote><p>计数的对象是文章的发布日期和文章的标题的唯一性组合，修改后将导致重新计数<br>本地的计数无意义</p></blockquote></li></ol>]]></content>
    
    
    <categories>
      
      <category>生活记录</category>
      
      <category>电脑设置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hexo</tag>
      
      <tag>fluid</tag>
      
      <tag>看板娘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何用vim写markdown</title>
    <link href="/2021/07/08/%E5%A6%82%E4%BD%95%E7%94%A8vim%E5%86%99markdown/"/>
    <url>/2021/07/08/%E5%A6%82%E4%BD%95%E7%94%A8vim%E5%86%99markdown/</url>
    
    <content type="html"><![CDATA[<h1 id="安装neovim"><a href="#安装neovim" class="headerlink" title="安装neovim"></a>安装neovim</h1><blockquote><p>sudo apt-get install neovim<br>neovim<br>:checkhealth</p></blockquote><h1 id="安装vim插件管理器vim-plug"><a href="#安装vim插件管理器vim-plug" class="headerlink" title="安装vim插件管理器vim-plug"></a>安装vim插件管理器vim-plug</h1><blockquote><p>sh -c ‘curl -fLo “${XDG_DATA_HOME:-$HOME/.local/share}”/nvim/site/autoload/plug.vim —create-dirs<br><a href="https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim">https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim</a>‘</p></blockquote><p>链接不上的话，需要：<br></p><blockquote><p>sudo nvim /etc/hosts<br></p></blockquote><p>添加一行：<br></p><blockquote><p>199.232.96.133 raw.githubusercontent.com<br></p></blockquote><p>其中的ip地址来自于<em><a href="https://githubusercontent.com.ipaddress.com/raw.githubusercontent.com">https://githubusercontent.com.ipaddress.com/raw.githubusercontent.com</a></em></p><h1 id="创建neovim配置文件init-vim"><a href="#创建neovim配置文件init-vim" class="headerlink" title="创建neovim配置文件init.vim"></a>创建neovim配置文件init.vim</h1><blockquote><p>mkdir .config/nvim<br>touch .config/nvim/init.vim</p></blockquote><h1 id="修改init-vim以添加插件"><a href="#修改init-vim以添加插件" class="headerlink" title="修改init.vim以添加插件"></a>修改init.vim以添加插件</h1><p>目录，markdown，preview一共三个暂时（修改的内容直接看文件）,<br>中间: </p><ul><li>遇到了自动折叠问题：修改配置文件解决  </li><li>又遇到了无法预览的问题，看github<a href="https://github.com/iamcco/markdown-preview.nvim/issues/120">作者回复</a>解决</li></ul>]]></content>
    
    
    <categories>
      
      <category>生活记录</category>
      
      <category>电脑设置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>vim</tag>
      
      <tag>markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
