<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Faster RCNN源码分析（零）—— 整体框架</title>
    <link href="/2021/09/01/Faster-RCNN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/"/>
    <url>/2021/09/01/Faster-RCNN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/</url>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>两段式目标检测的代表作，相当复杂。</p><p>基于pytorch的源码，捋一遍Faster-RCNN到底干了啥。</p><p>这里先说一下框架，后面有时间再开几篇依次展开说各个子流程的代码实现细节。</p><div align=center><img title="" src="/img/article/fr-rcnn-frame.jpg" width="100%" height="100%" align=center></div><h2 id="1-从dataset加载图像"><a href="#1-从dataset加载图像" class="headerlink" title="1. 从dataset加载图像"></a>1. 从dataset加载图像</h2><p>首先要从把标注好的图像文件以及标注xml文件读取进来。</p><p>参考<a href="https://guohongming.xyz/2021/08/12/%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86/">这里</a>自定义的dataset类，可以知道，dataloader每次迭代返回的是，dataset类<code>__getitem__</code>方法的返回值经过自定义的collate_fn包装过的结果，也就是image tensor的tuple，targets dict的tuple。</p><p>而后在送入模型之前，还将tuple转为list，便于cat/stack之类的操作。</p><p>另外图像的BN、增广等操作也在这里进行。</p><p><strong>本阶段数据转化结果:</strong></p><ul><li><p><strong>image</strong>: jpg -&gt; [tensor(3x350x450), tensor(3x375x500)]，3x375x450指图像的原始尺寸</p></li><li><p><strong>target</strong>: xml -&gt; [dict1, dict2]，dict的结构详见<a href="https://guohongming.xyz/2021/08/12/%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86/">这里</a></p></li></ul><h2 id="2-图像预处理"><a href="#2-图像预处理" class="headerlink" title="2. 图像预处理"></a>2. 图像预处理</h2><p>原始图像的尺寸是不保证一致的，因此送入模型的图像都要先进行尺寸缩放、padding等操作。</p><p><strong>本阶段数据转化结果:</strong></p><ul><li><strong>image</strong>: [tensor(3x350x450), tensor(3x375x500)] -&gt; ImageList，ImageList是一个同时保存预处理之后的图像数据以及padding前缩放后图像尺寸的类。如<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clojure">&#123;image_sizes = [(<span class="hljs-number">800</span>,<span class="hljs-number">1028</span>), (<span class="hljs-number">800</span>, <span class="hljs-number">1066</span>)]<br>tensors = tensor(<span class="hljs-number">2</span>x3x800x1088)&#125;<br></code></pre></td></tr></table></figure></li><li><strong>target</strong>: [dict1, dict2] -&gt; [dict1, dict2]，数据类型没有变化，只是其中boxes的坐标进行了同比例缩放</li></ul><h2 id="3-Backbone"><a href="#3-Backbone" class="headerlink" title="3. Backbone"></a>3. Backbone</h2><p>使用经典分类网络的特征提取部分，作为Backbone，来得到feature map。</p><p>以配备了FPN结构的ResNet50为例，Backbone接受上一步的ImageList.tensors之后，将输出5个feature map。这5个feature map被组织为一个dict。</p><p><strong>本阶段数据转化结果:</strong></p><ul><li><strong>image</strong>: ImageList -&gt; <strong>features</strong>: dict，dict的结构如下，<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">features</span> = &#123;<br><span class="hljs-attribute">0</span> : tensor(<span class="hljs-number">2</span>x<span class="hljs-number">256</span>x<span class="hljs-number">200</span>x<span class="hljs-number">272</span>),<br><span class="hljs-attribute">1</span> : tensor(<span class="hljs-number">2</span>x<span class="hljs-number">256</span>x<span class="hljs-number">100</span>x<span class="hljs-number">136</span>),<br><span class="hljs-attribute">2</span> : tensor(<span class="hljs-number">2</span>x<span class="hljs-number">256</span>x<span class="hljs-number">50</span>x<span class="hljs-number">68</span>),<br><span class="hljs-attribute">3</span> : tensor(<span class="hljs-number">2</span>x<span class="hljs-number">256</span>x<span class="hljs-number">25</span>x<span class="hljs-number">34</span>),<br><span class="hljs-attribute">pool</span> : tensor(<span class="hljs-number">2</span>x<span class="hljs-number">256</span>x<span class="hljs-number">13</span>x<span class="hljs-number">17</span>)&#125;<br></code></pre></td></tr></table></figure></li></ul><h2 id="4-RPN"><a href="#4-RPN" class="headerlink" title="4. RPN"></a>4. RPN</h2><p>RPN可谓是Faster RCNN算法的核心，所有关键内容基本上都杂糅在其中。后续的预测结构，损失也都跟RPN类似。具体来看，依次做了这几件事：</p><h3 id="4-1-RPN-Head"><a href="#4-1-RPN-Head" class="headerlink" title="4.1 RPN Head"></a>4.1 RPN Head</h3><p>将所有feature map的value组装为一个list，送入RPN head结构，生成两个量：</p><ul><li>针对proposal的置信度参数objectness，一个跟输入对应的list</li><li>针对proposal的边界框回归参数proposal_box_reg，同样是一个跟输入对应的list</li></ul><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>features</strong>: dict -&gt; <strong>objectness</strong>: [tensor(2x3x200x272), tensor(2x3x100x136)，tensor(2x3x50x68)，tensor(2x3x25x34)，tensor(2x3x13x17)]<br>其中[B,C,H,W]中只有C维度的size发生改变，新值3的含义是要在这个feature map上的每一个HW cell上产生3个proposal，tensor的值表示对应proposal的置信度。</li><li><strong>features</strong>: dict -&gt; <strong>proposal_box_reg</strong>: [tensor(2x12x200x272), tensor(2x12x100x136)，tensor(2x12x50x68)，tensor(2x12x25x34)，tensor(2x12x13x17)]<br>其中C维度12表示每个HW cell上3个proposal的4个边界框回归参数。</li></ul><h3 id="4-2-AnchorGenerate"><a href="#4-2-AnchorGenerate" class="headerlink" title="4.2 AnchorGenerate"></a>4.2 AnchorGenerate</h3><p>为每一层feature map上的一个HW cell生成一套anchor。</p><p>anchor是基于规则生成的，而非输入图像的数据，因此需要指定一套包含几个anchor，每个anchor的面积以及HW比例。生成的这些anchor都用相对于某个中心点的相对坐标表示。</p><p>然后定位每feature map的HW cell在原图上的感受野，取感受野左上角元素为中心点，计算得到所有feature map上所有cell的所有anchor在原图上的绝对坐标，用一个list表示。</p><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>anchor参数</strong> -&gt; <strong>anchor_list</strong> : [tensor(217413x4), tensor(217413x4)]<br>其中，2个tensor对应本batch的2张图像，而且2个tensor的值完全相同，因为anchor是按照规则生成的，只和图像的尺寸有关，与图像的数据无关。另外，$217413 = (200\times272 + 100\times136 + 50\times68 + 25\times34 + 13\times17)\times3$。</li></ul><h3 id="4-3-DecodeProposal"><a href="#4-3-DecodeProposal" class="headerlink" title="4.3 DecodeProposal"></a>4.3 DecodeProposal</h3><p>用proposal_box_reg和anchor_list进行decode操作，得到proposal，如<code>tensor(2x217413x4)</code>。decode/encode公式如下：</p><script type="math/tex; mode=display">P_x=A_x + A_wd_x</script><script type="math/tex; mode=display">P_y=A_y + A_hd_y</script><script type="math/tex; mode=display">P_w=A_we^{d_w}</script><script type="math/tex; mode=display">P_h=A_he^{d_h}</script><p>式中，$(A_x, A_y, A_w, A_h)$和$(d_x, d_y, d_w, d_h)$分别为anchor_list和proposal_box_reg的元素，在这里是已知。$(P_x, P_y, P_w, P_h)$为proposal的元素，在这里是未知。</p><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>anchor_list</strong>: [tensor(217413x4), tensor(217413x4)] + <strong>proposal_box_reg</strong>: [tensor(2x12x200x272), tensor(2x12x100x136)，tensor(2x12x50x68)，tensor(2x12x25x34)，tensor(2x12x13x17)] -&gt; <strong>proposals</strong>: tensor(2x217413x4)</li></ul><h3 id="4-4-FilterProposal"><a href="#4-4-FilterProposal" class="headerlink" title="4.4 FilterProposal"></a>4.4 FilterProposal</h3><p>遍历本batch中的图片，过滤每张图片这数十万个proposal，每张图片只保留2000个/1000个（前者对应训练模式，后者对应预测模式）。筛选的规则如下</p><ol><li>将每一个feature map得到的proposal视为一个集合，取每个集合中objectness值最大的前2000个/1000个，不足则都算上。</li><li>将proposal中的越界坐标调整到图像边界上，然后去除H或W小于某个阈值的proposal。</li><li>去除对应的score小于阈值的proposal，其中score由objectness经sigmoid之后得到。</li><li>将所有feature map得到proposal视为一个集合，对这个集合做nms，去除与highscore box的iou大于0.7的lowscore box。其中各个feature map之间的proposal不滤除，就是说不会因为f1中的一个highscore box和f2中的一个lowscore box的iou过大，而去除这个lowscore box。</li><li>在nms之后的结果中，取score最大的前2000个/1000个，作为最终的proposal。</li></ol><p>最终得到proposal和score的list。</p><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>proposals</strong>: tensor(2x217413x4) -&gt; [tensor(2000x4), tensor(2000x4)]</li><li><strong>objectness</strong>: [tensor(2x3x200x272), tensor(2x3x100x136)，tensor(2x3x50x68)，tensor(2x3x25x34)，tensor(2x3x13x17)] -&gt; <strong>socores</strong>: [tensor(2000x4), tensor(2000x4)]</li></ul><h3 id="4-5-ComputeLoss"><a href="#4-5-ComputeLoss" class="headerlink" title="4.5 ComputeLoss"></a>4.5 ComputeLoss</h3><p>如果是预测模式，那么到上一步RPN模块的工作就结束了。但如果是训练模式，还要继续进行RPN loss的计算。</p><blockquote><p>备注一下，pytorch中计算RPN损失不是基于Filter之后的那2000个proposal，而是在原有的217413个proposal中进行随机抽样256个box，并且尽量保证正负样本的比例为1:1，正样本不够的话就有多少都算上。</p></blockquote><p>我们知道，计算损失的目的是为了量化模型<strong>预测结果</strong>和<strong>真实信息</strong>的偏离程度。对于RPN而言，模型输出的<strong>预测结果</strong>是这些proposal，与之对比的<strong>真实信息</strong>是图片的GTbox。而对比一个proposal box和一个GTbox，其实是对比他们两个相同属性——坐标&amp;标签，那么量化偏离程度就要从两个角度出发：</p><script type="math/tex; mode=display">L = \frac{1}{N}\sum_{i}^{N}{L_{cls}(p_i, p_i^*)} + \lambda \frac{1}{N}\sum_{i}^{N}p_i^*{L_{box}}</script><script type="math/tex; mode=display">L_{cls}(p_i, p_i^*) = -[p_i^*\ln{p_i}+(1-p_i^*)\ln(1-p_i)]</script><script type="math/tex; mode=display">L_{box} = \sum_{i = x,y,w,h} smooth_{L1}(d_i-d_i^*)</script><p>以上式中</p><ul><li>前后两个N在原论文中是不同的值，但是在pytorch源码中，就直接等同为抽样数量了。</li><li>$p_i$指objectness的值，$p_i^*$指proposal的真实标签。</li><li>$\lambda$用来平衡两个损失项，pytorch实现中直接取消了。</li><li>$d_i$指proposal_box_reg的值，$d_i^*$指anchor和与其匹配的GTbox解码出来的边界框回归参数。</li><li>$L_{cls}(p_i,p_i^*)$在原文中是softmax函数，但是因为只有两类，pytorch直接用二值交叉熵进行计算。</li></ul><h4 id="4-5-1-边界框坐标损失计算"><a href="#4-5-1-边界框坐标损失计算" class="headerlink" title="4.5.1 边界框坐标损失计算"></a>4.5.1 边界框坐标损失计算</h4><p>首先，为anchor_list中的所有anchor分配一个最佳匹配的GTbox（来自图像标注信息target）。匹配的规则是，对于一张图片的一个anchor，计算其与本图片所有GTbox的IOU，与其有最大IOU的那个GTbox，称之为该anchor的最佳匹配GTbox。获得了anchor和GTbox之间的匹配关系后，即可进行预测信息与GT信息之间的对比，从而得到偏离程度，亦即loss。现在有，</p><p><strong>因为</strong>：<br></p><ul><li>anchor坐标 + 真实的边界框编码参数 -&gt; GTbox坐标</li><li>anchor坐标 + 预测的边界框编码参数 -&gt; proposal坐标</li></ul><p><strong>因此</strong>：<br></p><ul><li>对比proposal和GTbox坐标的偏离程度等价于对比真实的和预测的边界框编码参数</li></ul><p>前者利用anchor和与之匹配的GTbox进行decode得到，后者就是RPN Head输出的proposal_box_reg。这样便得到了边界框坐标损失。</p><h4 id="4-5-2-前-后景分类损失计算"><a href="#4-5-2-前-后景分类损失计算" class="headerlink" title="4.5.2 前/后景分类损失计算"></a>4.5.2 前/后景分类损失计算</h4><p>前/后景分类损失反映的是一个proposal box的<strong>真实标签</strong>与<strong>预测标签</strong>的差异程度。我们已知<strong>预测标签</strong>就是每一个proposal的objectness值，那么一个proposal的<strong>真实标签</strong>是什么呢？至少我们的标注信息中是没有直接包含这一信息的。一种合适的做法是，比较同源的一对GTbox和proposal box之间的IOU。</p><blockquote><p>所谓同源是指，在5.1小节中我们建立了每一个anchor和Gtbox之间的匹配关系，而每一个anchor与RPN head输出的proposal_box_reg结合可以得到一个proposal box，这样这个anchor便将Gtbox和proposal box联系起来了。</p></blockquote><p>具体的做法如下：</p><ol><li>若它们的IOU极大，说明重合度高，应认为这个proposal是一个前景的提案，其真实标签应为“前景”；</li><li>若它们的IOU极小，说明重合度低，应认为这个proposal是一个背景的提案，其真实标签应为“背景”；</li><li>若IOU不大也不小，那么简单地将其视为无效的样本，并且不将其计入损失计算过程。</li></ol><p>获得proposal的<strong>真实标签（labels）</strong>之后，便可将labels和objectness代入交叉熵公式计算损失了。</p><h2 id="5-ROI-Align"><a href="#5-ROI-Align" class="headerlink" title="5. ROI Align"></a>5. ROI Align</h2><p>RPN之后，下一步操作所依赖的原理是：经过训练的RPN现在输出的proposal在一定程度上已经足够准确了，可以将一个proposal box认为是一张只包含一个目标的小图片，然后将目标检测的问题转化为图像分类的问题。具体来讲，得到proposal之后，这张小图片从原图上哪个位置截取已经知道了，但是截取下来的内容还不知道，所以要进行本段内容描述的操作。</p><blockquote><p>备注一下，跟RPN计算损失时一样，从这里往后的损失计算也不是所有217413个proposal都使用，而是只在Filter之后的2000个之中随机抽样512个，并且尽可能地保证正负样本比例为1:3。如果是预测模式则不抽样。</p><p>另外也不是所有feature map都使用，pooling出来的那一层数据后面也不用。</p></blockquote><p>因此，我们应该将proposal box框住的feature map区域当做下一步处理的对象（而不是原图）。这引入了3个问题：</p><ol><li>RPN得到的proposal box角点坐标都是基于原图的，如何知道一个proposal在feature map上所对应区域的角点坐标呢？</li><li>在FPN介入的情况下，feature map有多个，如何知道将一个proposal box映射到哪个feature map上呢？</li><li>proposal box有大有小，从原图到不同feature map上的映射比例也不尽相同，怎么封装成一个合适的tensor供后续处理呢？</li></ol><p>对于第一个问题，确定原图到feature map之间的比例，然后对proposal box坐标进行等比例缩放即可。</p><p>对于第二个问题，原论文基于FPN越低层的输入对应越大尺度对象的原则，设计了一个经验公式。</p><script type="math/tex; mode=display">k = \lfloor k_0 + \log_2(\frac{\sqrt{wh}}{224}) \rfloor</script><p>通过这个经验公式，利用proposal box的HW尺寸即可选择一个合适的feature map进行映射。</p><blockquote><p>其实，原论文中的这个公式是基于Fast RCNN的，也就是不采用RPN时的RCNN。因为采用RPN时，每个proposal都是由feature map经过RPN Head直接生成的，因此天然地知道proposal和feature map的对应关系。而Fast RCNN采用Select Search算法得到proposal，缺失这层关系，所以得用这个经验公式。</p><p>但是pytorch实现Faster RCNN还是用了这个经验公式进行proposal与feature map的匹配，而不是用RPN自带的信息。</p></blockquote><p>对于第三个问题，采用ROI Pooling或者说ROI Align算法处理，将所有大小各异的proposal feature box缩放成一致大小的tensor。基本原理是，将每个proposal feature box等分成7x7个区域，然后取每个区域内的最大值，这样无论之前的proposal feature box的HW尺寸如何，最后的输出都是一个7x7的大小。如果7x7处理后直接对子区域坐标取整然后maxpool就是ROI Pooling，用更复杂的插值算法得到最值则是ROI Align，当然后者效果更好。</p><div align=center><img title="" src="/img/article/roipooling.gif" width="60%" height="60%" align=center></div><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>proposal</strong>:[tensor(2000x4), tensor(2000x4)] -&gt; <strong>proposal</strong>:[tensor(512x4), tensor(512x4)] + <strong>proposal_Gtbox</strong>: [tensor(512x4), tensor(512x4)] + <strong>label</strong>:[tensor(512x1), tensor(512x1)]</li><li><strong>proposal</strong>:[tensor(512x4), tensor(512x4)] + <strong>features</strong> : dict -&gt; <strong>proposal_features</strong>: tensor(1024x256x7x7)</li></ul><blockquote><p>如果是预测模式则将以上出现的512替换为1000来理解</p></blockquote><h2 id="6-TwoMLPHead"><a href="#6-TwoMLPHead" class="headerlink" title="6. TwoMLPHead"></a>6. TwoMLPHead</h2><p>这一层比较简单，就是将ROI Align之后的proposal_features第一个维度之后的数据flatten，然后通过两个串联的全连接层。</p><blockquote><p>也就是说一个batch处理1024个框，每张图像512个，每个框用一个256x7x7个标量数据组成的列向量表示。如果是预测模式则是每张图像1000个框。</p></blockquote><p>第一层256x7x7个节点，没得选，输入在那，RELU激活；第二层人为设定为1024个节点，同样是RELU激活。</p><p>最终得到一个1024x1024的tensor。</p><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>proposal_features</strong>: tensor(1024x256x7x7) -&gt; tensor(1024x1024)</li></ul><blockquote><p>如果是预测模式，那么应该为1000x256x7x7-&gt;1000x1024</p></blockquote><h2 id="7-Predictor"><a href="#7-Predictor" class="headerlink" title="7. Predictor"></a>7. Predictor</h2><p>这一层也比较简单，就是将TwoMLPHead输出的tensor通过两个并联的全连接层。这两个全连接层上的结构以及作用与RPN Head基本相同，一个生成类别参数，一个生成边界框回归参数。类别层class_num个节点，边界框层class_num x 4个节点。</p><p>这里的预测结果要多说一句，在进入ROI align之前，每张图片的proposal经过过滤剩下512或500个，这个数字对于理解后面的内容非常重要。</p><p>首先在这里定义，一个proposal产生的一系列最终结果为一个detection，那么每张图片会产生512个detection。然后，一个proposal框住的feature map数据经过ROI align、MLPHead和Predictor，最终生成21组边界框回归参数和21个置信度。这个21组边界框回归参数都和这一个proposal框进行decode，得到21个final_box。21个置信度经过softmax之后的到21个score。再加上天然存在的序列信息，亦可得到label。</p><p>这样，每个detection的三个属性label &amp; score &amp; box_reg就都有了。一个detection的结构大致如下图所示（以3个detection，5个类别为例）：</p><div align=center><img title="" src="/img/article/faster-rcnn-0-0.png" width="90%" height="90%" align=center></div><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>porposal_feature</strong>: tensor(1024x1024) -&gt; <strong>class_logits</strong>: tensor(1024 x class_num) + <strong>box_regs</strong>: tensor(1024 x [class_num x 4])</li></ul><blockquote><p>数据转化结果中的第一个维度在预测模式下同样应该是1000而不是1024</p></blockquote><h2 id="8A-预测结果后处理"><a href="#8A-预测结果后处理" class="headerlink" title="8A. 预测结果后处理"></a>8A. 预测结果后处理</h2><p>上一步完成后，实质意义上的模型预测就已经完成了。如果是在预测模式下工作，接下来需要将class_logits和box_regs解释到缩放前的原始图像上。</p><h3 id="8A-1-FilterDetection"><a href="#8A-1-FilterDetection" class="headerlink" title="8A.1 FilterDetection"></a>8A.1 FilterDetection</h3><p>对于每一张图像，具体工作如下：</p><ol><li>对class_logits做softmax，得到每一个box的score；</li><li>对box_reg和proposal角点坐标做decode，得到每一个final_box的角点坐标；</li><li>类似RPN的FilterProposal，做low score过滤；</li><li>类似RPN的FilterProposal，做low WH过滤；</li><li>类似RPN的FilterProposal，针对每一个类别做nms，并且取nms之后的前100个box；</li></ol><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>box_regs</strong>: tensor(1024 x [class_num x 4]) + <strong>proposal</strong>: [tensor(512x4), tensor(512x4)] -&gt; <strong>final_boxes</strong>: [100x4, 100x4]，进行decode和filter</li><li><strong>class_logits</strong>: tensor(1024 x class_num) -&gt; <strong>final_scores</strong>: [100x1, 100x1]，进行sotfmax和filter</li><li><strong>box_labels</strong>: tensor(1024 x class_num) -&gt; <strong>final_labels</strong>: [100x1, 100x1]，进行同步filter</li></ul><blockquote><p>这里的BoxLabel由规则生成，final_labels指的是预测信息，根据上图中box在一个detection的box序列中的相对位置确定。这就跟以往的经验不同了，这里并没有对一个detection所有框的score取最大值，然后将21个类别退化为1，而是将detection的每一列也就是每一个预测框作为一个单独的个体。就是说最终的100个预测框中完全有可能同时存在两个产生自同一个detection。</p><p>数据转化结果中的第一个维度在预测模式下同样应该是1000而不是1024</p></blockquote><h3 id="8A-2-PostTransform"><a href="#8A-2-PostTransform" class="headerlink" title="8A.2 PostTransform"></a>8A.2 PostTransform</h3><p>将detection三要素中的box坐标根据原图缩放前和缩放后的比例进行等比例缩放即可。</p><h2 id="8B-FasterRCNN损失计算"><a href="#8B-FasterRCNN损失计算" class="headerlink" title="8B. FasterRCNN损失计算"></a>8B. FasterRCNN损失计算</h2><p>当在训练模式下工作时，需要进行FasterRCNN损失计算。与RPN损失类似，FasterRCNN损失同样分为两部分，坐标损失和分类损失。首先捋一下两部分各自的真实信息和预测信息是什么：</p><ol><li>坐标回归参数的预测信息，Predictor输出的box_regs；</li><li>坐标回归参数的真实信息GT_regs，用一对GTbox和proposal进行encode得到；</li><li>预测框类别的预测信息，Predictor输出的class_logits；</li><li>预测框类别的真实信息GT_labels，获取方式跟之前RPN类似，即先将proposal和GTbox进行匹配，然后用匹配之后的IOU值的大小来判定proposal是否值得拥有与之匹配的GTbox的label，若值得(IOU&gt;0.5)则proposal的真实label就是GTbox的label，否则为0；</li></ol><p>捋清楚了上述信息之后，即可将对应数据代入到类似RPN的损失函数中进行计算，得到FasterRCNN的最终损失。</p><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>class_logits</strong>: tensor(1024 x classnum) + <strong>GT_labels</strong>: [tensor(512x1), tensor(512x1)] -&gt; $L_{cls}$</li><li><strong>box_regs</strong>: tensor(1024 x [classnum x 4]) + <strong>GT_regs</strong>: [tensor(512x4),tensor(512x4)] -&gt; $L_{box}$</li></ul><blockquote><p>可以看到真实信息和预测信息之间差了一个classnum的维度，在计算$L_{cls}$时不要紧，因为交叉熵计算用到的GT_label是one-hot的形式，pytorch的实现机制自己会进行合理的计算。</p><p>在计算$L_{box}$时，要增加一个classnum上的索引将其退化为1，从而与GT_regs保持一致。索引的来源是GT_label的one-hot向量中1的位置，所以也就是说将GT_reg拓展为four-hot即可。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>目标检测</category>
      
      <category>Faster RCNN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>目标检测</tag>
      
      <tag>Faster RCNN</tag>
      
      <tag>RPN</tag>
      
      <tag>ROI Align</tag>
      
      <tag>ROI Pooling</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SSD源码分析 —— 整体框架</title>
    <link href="/2021/09/01/SSD%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/"/>
    <url>/2021/09/01/SSD%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/</url>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>最近忙着搞超声波，SSD的源码分析拖得有点久了。不过鉴于SSD的模型结构比较简单，而且较为复杂的gtbox匹配、损失计算等机制在Faster RCNN业已理解，所以前后也没用多长时间。</p><p>首先，还是算法的总体流程图，可以看出比Faster RCNN那是简单太多了。</p><div align=center><img title="" src="/img/article/ssd_overview.png" width="90%" height="90%" align=center></div><p><br>第一步还是从dataset中加载图像，然后经过图像预处理，再通过以ResNet50为蓝本的feature extractor处理，结果再送到5个additional layer中，最终连同feature extractor的输出合计得到6个feature map，每个feature map再各自通过一个predictor进而得到预测结果(box_reg和objectness)。</p><p>其中box_reg为相对于anchor的边界框回归参数，objectness经过softmax之后就是对应类别的置信度。如果是训练模式，就用这两个信息去求模型损失；如果是预测模式则对所有的边界框进行筛选得到最终的预测结果。</p><p>其中anchor还是老样子，从6个feature map每一个的HW cell上面长出来2-3个，不同feature map上anchor的scale和ratio都不同，以达到金字塔的效果。</p><h2 id="1-从dataset加载图像"><a href="#1-从dataset加载图像" class="headerlink" title="1. 从dataset加载图像"></a>1. 从dataset加载图像</h2><p>首先要从把标注好的图像文件以及标注xml文件读取进来，经过一系列图像预处理操作转化为tensor格式，大致流程跟<a href="https://guohongming.xyz/2021/09/01/Faster-RCNN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/">之前</a>Faster RCNN差不多。</p><p>但是，FasterRCNN的图像预处理有两次：</p><ul><li>第一次是在创建dataset类实例的时候，就是说从dataset类<code>__getitem__</code>方法返回的图像数据先经过了一次transform。这一次主要是to_tensor和随机水平翻转（预测模式无）。</li><li>然后在送入模型之前又通过单独的GeneralizedRCNNTransform类，再transform一次。这一次的细节看<a href="https://guohongming.xyz/2021/08/15/Faster-RCNN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/">这里</a></li></ul><p>回到SSD，预处理只有一次，全部集中在dataset类中实现，而且花样比较多(加*项为训练模式独有)：</p><h3 id="1-1-SSDcrop"><a href="#1-1-SSDcrop" class="headerlink" title="1.1 SSDcrop*"></a>1.1 SSDcrop*</h3><p>顾名思义，这是SSD的特点之一。目的是为了在一张原始图片上截取一个子区域作为模型的输入，而不是整图送进去。而且很明显，不是在原图上随便截就可以，截出来的子区域要符合一定条件：</p><ol><li>HW比例不能太夸张，要在1:2以内；</li><li>跟所有GTbox的IOU都要在一个合理范围内，这个合理范围在[0.1, inf], [0.3, inf], [0.5, inf], [0.7, inf], [0.9, inf], [-inf, inf]中随机选取。</li><li>至少有一个GTbox的中心点在这个子区域内。</li></ol><p>得到一个满足全部3个条件的子区域后，进行下一步。这意味着，对于一个epoch，这张图片只有一个子区域得到了有效使用，而且下一个epoch对于这张图片很有可能又换了一个子区域。</p><p><strong>相同，又不完全相同；重复，又不完全重复。艺术啊！</strong></p><p><strong>本阶段数据转化结果:</strong></p><ul><li><strong>image</strong>: jpg(375x500) -&gt; jpg(252x438)<blockquote><p>这一步的目的目测是为了扩充训练集的样本数量，但是这样不保持原图比例的裁剪然后再缩放，相比于预测时只是缩放不裁剪，真的可以吗？而且预测时这种不考虑原图HW大小和比例，直接硬缩放为300x300，直觉上也感觉不太对。mark</p></blockquote></li></ul><h3 id="1-2-Resize"><a href="#1-2-Resize" class="headerlink" title="1.2 Resize"></a>1.2 Resize</h3><p>简单粗暴调用官方库<code>torchvision.transforms.Resize()</code>，利用双线性插值原理将裁剪后的图像/原始图像，缩放到300x300。</p><p><strong>本阶段数据转化结果:</strong></p><ul><li><strong>image</strong>: jpg(252x438) -&gt; jpg(300x300)<blockquote><p>SSD使用了一个简单而高效的机制来处理图像缩放过程中的GTbox坐标变化，就是将GTbox的坐标以图像比例坐标的方式存储，这样不管图像缩放多少，用这个比例坐标乘以新的尺寸即可得到正确的GTbox坐标。</p></blockquote></li></ul><h3 id="1-3-ColorJitter"><a href="#1-3-ColorJitter" class="headerlink" title="1.3 ColorJitter*"></a>1.3 ColorJitter*</h3><p>调用官方库<code>torchvision.transforms.ColorJitter()</code>，在HSV空间将图像数据随机抖动一下。</p><p><strong>本阶段数据转化结果:略</strong></p><h3 id="1-4-ToTensor"><a href="#1-4-ToTensor" class="headerlink" title="1.4 ToTensor"></a>1.4 ToTensor</h3><p>PIL image格式转成tensor，换言之，之前的操作都是以PIL image格式进行的。</p><p><strong>本阶段数据转化结果:</strong></p><ul><li><strong>image</strong>: jpg(300x300) -&gt; tensor(3x300x300)</li></ul><h3 id="1-5-RandomHorizontalFlip"><a href="#1-5-RandomHorizontalFlip" class="headerlink" title="1.5 RandomHorizontalFlip*"></a>1.5 RandomHorizontalFlip*</h3><p>随机水平翻转，不解释了。</p><p><strong>本阶段数据转化结果:略</strong></p><h3 id="1-6-Normalization"><a href="#1-6-Normalization" class="headerlink" title="1.6 Normalization"></a>1.6 Normalization</h3><p>数据归一化，三通道的均值和方差还是取自ImageNet，<code>mean = [0.485, 0.456, 0.406]</code>，<code>std = [0.229, 0.224, 0.225]</code>。</p><p><strong>本阶段数据转化结果:略</strong></p><h3 id="1-7-AssignGTbox2Anchor"><a href="#1-7-AssignGTbox2Anchor" class="headerlink" title="1.7 AssignGTbox2Anchor*"></a>1.7 AssignGTbox2Anchor*</h3><p>又来了，Anchor与GTbox匹配的概念并不新奇，但这个实现方式却是SSD的鲜明特点。</p><p>为什么？因为在SSD的One-Stage理念下，不存在proposal概念，或者说anchor就是最终的proposal，而这个proposal又是预先按照既定参数生成的，不是来源于图像数据本身的。也就是说，对于SSD而言从一开始就知道proposal框的坐标，因此在预处理的时候就把Anchor与GTbox的匹配给做了。</p><p>这一步实际产生的效果是，按照anchor的排列顺序产生与之匹配的GTbox的序列以及label的序列，用以替换target中来自xml文件中的box和label信息。这样在后面计算损失时用到GT信息时，可以直接decode target中的box得到GT_box_reg，然后直接与预测的box_reg进行smooth L1损失计算。</p><p><strong>本阶段数据转化结果:</strong></p><ul><li><p><strong>image</strong>: 无变化</p></li><li><p><strong>GTbox</strong>: tensor(1x4) -&gt; tensor(8732x4)</p></li></ul><h2 id="2-FeatureExtractor"><a href="#2-FeatureExtractor" class="headerlink" title="2. FeatureExtractor"></a>2. FeatureExtractor</h2><p>其实还是一个Backbone的概念，目的是为了从原始图像中提取出Feature map。这里还是以<a href="https://guohongming.xyz/2021/08/08/ResNet/">ResNet50</a>为例，取其第一个卷积层conv1以及后续的3组Residual结构作为Backbone。</p><p>其中值得注意的是，第3组Residual结构conv4_x不是照搬过来的，而是将其这一组的6个Residual结构中的第一个的步距由2调整为1。原理这一个Residual结构因为身处组内第一的位置，因此要承担调整HW尺寸不断折半的任务，但是这里调整了stride，不用折半了。</p><div align=center><img title="" src="/img/article/resnet50_ssd.png" width="80%" height="80%" align=center></div><p><br><strong>本阶段数据转化结果:</strong></p><ul><li><strong>image</strong>: tensor(3x300x300) -&gt; tensor(64x150x150) -&gt; tensor(256x75x75) -&gt; tensor(512x38x38) -&gt; tensor(1024x38x38)</li></ul><blockquote><p>如果不调整conv4_x，那么最后一个tensor应该是1024x38x38x19</p></blockquote><h2 id="3-AdditionalLayer"><a href="#3-AdditionalLayer" class="headerlink" title="3. AdditionalLayer"></a>3. AdditionalLayer</h2><p>为了在不同尺度上得到更加准确的预测结果，这里又额外构造了5个AdditionalLayer结构，在Backbone输出的基础上继续进行卷积运算。</p><p>每一个AdditionalLayer结构都是由两个串联的ConvBNRelu单元组成，不断的缩小feature map的HW尺寸，以使得越往后的HW cell对应更大的原图感受野。</p><div align=center><img title="" src="/img/article/ssd_additional.png" width="100%" height="100%" align=center></div><p><br><strong>本阶段数据转化结果: 详见上图</strong></p><h2 id="4-AnchorGenerator"><a href="#4-AnchorGenerator" class="headerlink" title="4. AnchorGenerator"></a>4. AnchorGenerator</h2><p>未在总流程图体现的一点是AnchorGenerator，因为anchor是基于规则生成的。老样子，还是为每一个feature map上的每一个HW cell生成一套anchor，并且总体上还是遵循在不同的feature map上生成不同scale的anchor。但是相比于用了FPN的Faster RCNN，SSD的AnchorGenerator的规则有一些自己的特点。</p><div align=center><img title="" src="/img/article/ssd_anchor.png" width="80%" height="80%" align=center></div><p><br>按照上表的规则生成的这些anchor都直接用在300x300图像上的比例坐标表示，而不是向FasterRCNN一样，用相对于某个中心点的相对坐标。</p><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>anchor参数</strong> -&gt; <strong>anchor</strong> : tensor(8732x4)</li></ul><h2 id="5-Predictor"><a href="#5-Predictor" class="headerlink" title="5. Predictor"></a>5. Predictor</h2><p>得到6个feature map之后，将其分别送入一个Predictor。每个Predictor包含两个卷积结构，一个用来生成objectness，一个用来生成box_reg。具体的结构也比较简单：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs lisp"># box_extractor<br>ModuleList(<br>  (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">1024</span>, <span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">1</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">24</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">2</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">24</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">3</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">24</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">4</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">5</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>)<br><br># objectness_extractor<br>ModuleList(<br>  (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">1024</span>, <span class="hljs-number">84</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">1</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">126</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">2</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">126</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">3</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">126</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">4</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">84</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">5</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">84</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>)<br></code></pre></td></tr></table></figure></p><p>可以看到，3x3的卷积，SAME PADDING，1 STRIDE，就是不改变卷积前后的HW尺寸，只调整深度：</p><ul><li>对于box_extractor，深度调整为4 x anchor_num；</li><li>对于objectness_extractor，深度调整为21 x anchor_num。</li></ul><p>例如，1024x38x38的feature map在经过box_extractor处理之后，得到一个16x38x38的tensor。16的来由是这一层feature map上的每个cell生成4个anchor，而每个anchor有4个坐标值。就是说，1个HW cell的每4个通道组成一组完整的box_reg参数。然后再将这个16x38x38的tensor reshape成5776x4，即可得到第一个feature map生成的5776组box_reg参数，同理可以得到其余的box_reg参数，最后拼成一个8732x4的box_reg。</p><p>类似地，1024x38x38的feature map在经过objectness_extractor处理之后，得到一个84x38x38的tensor，将其reshape为5774x21然后再去拼接,最终得到8732x21的objectness。</p><blockquote><p>这里出现了取消proposal的另外一个优势，因为没有了proposal，所以300x300的图像上所有数据或者说一个feature map上的所有数据，都会被拿来使用，所以不存在需要使用ROIalign的场景。</p></blockquote><p>然后，来回忆一下FasterRCNN中的detection概念。由于未做过滤和采样，因此8732个anchor全部参与后续计算，这意味这最终将产生8732个detection。而且因为box_extractor的结构跟FasterRCNN不同，SSD每个detection的结构跟FasterRCNN也略有不同，SSD的一个detection只有1组box_reg参数，而FasterRCNN是21组，也就是每个类一组。</p><p>FasterRCNN的detection结构如下：</p><div align=center><img title="" src="/img/article/faster-rcnn-0-0.png" width="90%" height="90%" align=center></div><p><br>SSD的detection结构如下：</p><div align=center><img title="" src="/img/article/ssd_preditor.png" width="90%" height="90%" align=center></div><p><br><strong>本阶段数据转化结果</strong></p><ul><li><strong>feature map1</strong>: tensor(1024x38x38) -&gt; <strong>objectness1</strong>: tensor(16x38x38) + <strong>box_reg</strong>: tensor(84x38x38) -&gt; <strong>objectness1</strong>: tensor(5776x21) +<strong>box_reg</strong>: tensor(5776x4)</li><li><strong>objectness1</strong> + … + <strong>objectness6</strong> -&gt; <strong>objectness</strong>: tensor(8732x4)</li><li><strong>box_reg1</strong> + … + <strong>box_reg6</strong> -&gt; <strong>box_reg</strong>: tensor(8732x21)</li></ul><h2 id="6A-预测结果后处理"><a href="#6A-预测结果后处理" class="headerlink" title="6A. 预测结果后处理"></a>6A. 预测结果后处理</h2><p>上一步完成后，实质意义上的模型预测就已经完成了。如果是在预测模式下工作，接下来需要将objectness和box_reg解释到缩放前的原始图像上。</p><h3 id="6A-1-FilterDetection"><a href="#6A-1-FilterDetection" class="headerlink" title="6A.1 FilterDetection"></a>6A.1 FilterDetection</h3><p>对于每一张图像，具体工作如下：</p><ol><li>对class_logits做softmax，得到每一个box的score；</li><li>对box_reg和anchor角点坐标做decode，得到每一个final_box的角点坐标；</li><li>low score过滤；</li><li>low WH过滤；</li><li>针对每一个类别做nms，并且取nms之后的前100个box；</li></ol><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>box_reg</strong>: tensor(8732x4) + <strong>anchor</strong>: tensor(8732x4) -&gt; <strong>final_boxes</strong>: tensor(100x4)</li><li><strong>objectness</strong>: tensor(8732x21) -&gt; <strong>final_scores</strong>: tensor(100x1)</li><li><strong>box_label</strong>: tensor(8732x21) -&gt; <strong>final_labels</strong>: tensor(100x1)</li></ul><blockquote><p>和FasterRCNN相同，这里的BoxLabel由规则生成，final_labels指的是预测信息，根据上图中box在一个detection的box序列中的相对位置确定。这就跟以往的经验不同了，这里并没有对一个detection所有框的score取最大值，然后将21个类别退化为1，而是将detection的每一列也就是每一个预测框作为一个单独的个体。就是说最终的100个预测框中完全有可能同时存在两个产生自同一个detection。</p></blockquote><h3 id="6A-2-PostTransform"><a href="#6A-2-PostTransform" class="headerlink" title="6A.2 PostTransform"></a>6A.2 PostTransform</h3><p>将detection三要素中的box坐标根据原图缩放前和缩放后的比例进行等比例缩放即可。</p><h2 id="6B-SSD损失计算"><a href="#6B-SSD损失计算" class="headerlink" title="6B. SSD损失计算"></a>6B. SSD损失计算</h2><p>当在训练模式下工作时，需要进行SSD损失计算。与FasterRCNN损失类似，SSD损失同样分为两部分，坐标损失和分类损失。首先捋一下两部分各自的真实信息和预测信息是什么：</p><ol><li>坐标回归参数的预测信息，Predictor输出的box_reg；</li><li>坐标回归参数的真实信息GT_regs，用anchor和与之匹配的GTbox进行encode得到；</li><li>预测框类别的预测信息，Predictor输出的objectness；</li><li>预测框类别的真实信息GT_labels，先将anchor和GTbox进行匹配，然后用匹配之后的IOU值的大小来判定proposal是否值得拥有与之匹配的GTbox的label，若值得(IOU&gt;0.5)则anchor的真实label就是GTbox的label，否则为0；</li></ol><p>捋清楚了上述信息之后，即可将对应数据代入到FasterRCNN的损失函数中进行计算，得到SSD的最终损失。</p><p>另外，具体还有一些细节需要注意：</p><ul><li>定位损失和分类损失SSD取1:1。</li><li>定位损失只统计真实标签不为0的anchor，或者说基于这个anchor产生的GT_reg和box_reg，即所谓正样本。</li><li>分类损失统计全部的正样本，然后再选取3倍的负样本。并且负样本不是像FasterRCNN那样随机选取，而是选取负样本中损失最大的那些。例如这一个batch有10个正样本，那么就应该选取交叉熵损失最大的30个负样本。而且负样本的数量不应超过8732。据原论文称可以这样做可以加速收敛。</li></ul><p><strong>本阶段数据转化结果</strong></p><ul><li><strong>box_label</strong>: tensor(8732x21) + <strong>GT_labels</strong>: tensor(8732x21) -&gt; $L_{cls}$</li><li><strong>box_reg</strong>: tensor(8732x4) + <strong>GT_regs</strong>: tensor(8732x4) -&gt; $L_{box}$</li></ul><blockquote><p>同样跟FasterRCNN一样，可以看到真实信息和预测信息之间差了一个classnum的维度，在计算$L_{cls}$时不要紧，因为交叉熵计算用到的GT_label是one-hot的形式，pytorch的实现机制自己会进行合理的计算。</p><p>在计算$L_{box}$时，要增加一个classnum上的索引将其退化为1，从而与GT_regs保持一致。索引的来源是GT_label的one-hot向量中1的位置，所以也就是说将GT_reg拓展为four-hot即可。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>目标检测</category>
      
      <category>SSD</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>目标检测</tag>
      
      <tag>SSD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Faster RCNN源码分析（一）—— 图像预处理</title>
    <link href="/2021/08/15/Faster-RCNN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/"/>
    <url>/2021/08/15/Faster-RCNN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一般数据集中的图像尺寸、通道都是不一致的，因此不管是进行训练还是预测之前，都需要进行尺寸一致化等预处理操作。处理的最终结果是每一个batch中的tensor尺寸一致，但是跨batch不一定一致。</p><p>Faster RCNN源码中，将图像预处理操作封装在一个类<code>GeneralizedRCNNTransform</code>中，下面针对这个类进行介绍。</p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-comment"># 接受4个初始化参数，用于初始化4个对应成员</span><br><span class="hljs-comment"># min_size : 指定允许的最小尺寸</span><br><span class="hljs-comment"># max_size : 指定允许的最大尺寸</span><br><span class="hljs-comment"># image_mean : 指定归一化时三个通道各自的均值（默认来自ImageNet）</span><br><span class="hljs-comment"># image_std : 指定归一化时三个通道各自的方差（默认来自ImageNet）</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, min_size, max_size, image_mean, image_std)</span></span>:<br>    <span class="hljs-keyword">self</span>.min_size = min_size      <span class="hljs-comment"># 指定图像的最小边长范围</span><br>    <span class="hljs-keyword">self</span>.max_size = max_size      <span class="hljs-comment"># 指定图像的最大边长范围</span><br>    <span class="hljs-keyword">self</span>.image_mean = image_mean  <span class="hljs-comment"># 指定图像在标准化处理中的均值</span><br>    <span class="hljs-keyword">self</span>.image_std = image_std    <span class="hljs-comment"># 指定图像在标准化处理中的方差</span><br></code></pre></td></tr></table></figure><h2 id="正向过程"><a href="#正向过程" class="headerlink" title="正向过程"></a>正向过程</h2><p>接受1个batch的图像和对应的目标信息（预测时无目标），逐个遍历。依次进行：</p><ol><li>维度检查 —— <em>因为有些数据集通道数还真不一定是3</em></li><li>归一化 —— <em>减去方差，除以均值即可</em></li><li>缩放 —— <em>因为图像与其目标信息是耦合的因此要同步做，后面细讲</em></li><li>padding —— <em>真正的尺寸一致化操作</em></li><li>二次封装 —— <em>将resize后的图像尺寸与图像封装在一起</em></li></ol><h3 id="缩放"><a href="#缩放" class="headerlink" title="缩放"></a>缩放</h3><h4 id="图像缩放"><a href="#图像缩放" class="headerlink" title="图像缩放"></a>图像缩放</h4><p>缩放的原则是长和宽等比例缩放。缩放的目的是使得原图像能够最大程度进一个既定尺寸的框中，不管横着放还是竖着放。</p><p>这两个条件意味着，缩放之后的图像的长或宽，至少有一个等于这个框的一个边，所谓最大程度嘛。</p><p>具体操作时，先用缩放前图像的短边和框的短边产生缩放比例，然后检查长边按照该比例缩放后是否fit，如果超限了那么就要用两个长边产生缩放比例然后再去缩放短边。</p><h4 id="box缩放"><a href="#box缩放" class="headerlink" title="box缩放"></a>box缩放</h4><p>因为图像的尺寸变了，box的坐标自然也要跟着变。具体操作是，对比原图像和缩放后图像的长产生长度缩放因子，然后将box的两个角点的x坐标乘以该缩放因子。y坐标同理。</p><h3 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h3><p>经过缩放之后，一个batch内所有图像的尺寸还是不一致，毕竟有横有竖，而且未填满的那一个边的尺寸也不能保证一致。因此，在这一步找一个更大的，刚好能装下这个batch所有图像的框，以左上角对齐的方式进行尺寸一致化，右下角填不满的地方补0。</p><h3 id="二次封装"><a href="#二次封装" class="headerlink" title="二次封装"></a>二次封装</h3><p>就是简单的将缩放后，padding前的图像尺寸与图像封装称一个结构体<code>ImageList</code>。结构体的两个成员分别是一个表示本batch所有图像的4维tensor和一个记录图像尺寸的<code>List[Tuple[int, int]]</code></p><h3 id="返回"><a href="#返回" class="headerlink" title="返回"></a>返回</h3><p>最终，返回一个<code>ImageListd</code>对象和目标信息target。</p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>目标检测</category>
      
      <category>Faster RCNN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>目标检测</tag>
      
      <tag>Faster RCNN</tag>
      
      <tag>图形预处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>双线性插值是啥？</title>
    <link href="/2021/08/15/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E6%98%AF%E5%95%A5%EF%BC%9F/"/>
    <url>/2021/08/15/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E6%98%AF%E5%95%A5%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今天看FASTER RCNN的pytorch实现源码，进行图像缩放时用到了双线性插值算法，记录一下自己的理解。</p><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>当将一幅图像从100x100的尺寸放大到200x200时，势必要新增30000个像素点。那么这些新增的像素点的灰度值如何得到呢？</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>双线性插值的做法是这样：</p><h3 id="1-建立坐标的对应关系"><a href="#1-建立坐标的对应关系" class="headerlink" title="1. 建立坐标的对应关系"></a>1. 建立坐标的对应关系</h3><p>将放大前的图像A和放大后的图形B的几何中心对齐，然后有</p><script type="math/tex; mode=display">\frac{x_A + 0.5}{x_B + 0.5} = \frac{w_A}{w_B}</script><script type="math/tex; mode=display">\frac{y_A + 0.5}{y_B + 0.5} = \frac{h_A}{h_B}</script><p>如果按照上面公式计算出来是整数，就直接将原图中的对应位置的灰度值复制过来。如果是小数，说明原图像中没有对应点，就将x/y分别向上/下取整得到4个坐标。</p><blockquote><p>其实也无所谓，一律按照下面方法计算即可，只不过是整数的时候取得是两个相同灰度值的均值，也就是还是他自己罢了。省的在这讨论，x整数y小数，x小数y整数这些问题了。</p></blockquote><h3 id="2-用A上4个对应像素点的灰度值计算B上1个像素点的灰度值"><a href="#2-用A上4个对应像素点的灰度值计算B上1个像素点的灰度值" class="headerlink" title="2. 用A上4个对应像素点的灰度值计算B上1个像素点的灰度值"></a>2. 用A上4个对应像素点的灰度值计算B上1个像素点的灰度值</h3><p>双线性插值说白了就是先将这4个点分成两组在一个方向上进行线性插值，得到两个中间值，然后用这两个中间值再进行一次线性插值，得到最终结果。</p><div align=center><img title="" src="/img/sxxcz.png" width="40%" height="40%" align=center></div><p>即，先有</p><script type="math/tex; mode=display">\frac{R_1-Q_{11}}{x-x_1}=\frac{Q_{21}-Q_{11}}{x_2-x_1}</script><script type="math/tex; mode=display">\frac{R_2-Q_{12}}{x-x_1}=\frac{Q_{22}-Q_{12}}{x_2-x_1}</script><p>然后，有</p><script type="math/tex; mode=display">\frac{P-R_1}{y-y_1}=\frac{R_2-R_1}{y_2-y_1}</script><p>其中，P为所求的B上$(x_B, y_B)$处的灰度值，$(x,y)$就是上面的$(x_A, y_A)$，四个Q就是A上四个对应像素点的灰度值，R为中间值。</p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像缩放</tag>
      
      <tag>双线性插值</tag>
      
      <tag>bilinear interpolation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自定义目标检测数据集</title>
    <link href="/2021/08/12/%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <url>/2021/08/12/%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目标检测的数据集跟分类数据集不同，不仅仅包含图片和图片的类别信息，还要包括bounding box等一些额外的信息。因此，使用<a href="https://guohongming.xyz/2021/08/11/pytorch%E6%A0%B8%E5%BF%83%E7%94%A8%E6%B3%95-2/">之前</a>建立目录结构 + <code>datasets.ImageFolder()</code>的方法是不行的。</p><p>总的来看，自定义目标检测数据集要围绕<strong>原始图像</strong>、<strong>标注文件</strong>、<strong>入选名单</strong>三个要素，完成<strong>图像标注</strong>和<strong>自定义dataset类</strong>两项工作。</p><h2 id="三个要素"><a href="#三个要素" class="headerlink" title="三个要素"></a>三个要素</h2><ol><li>原始图像，统统存放在一个目录下</li><li>标注文件，一般为一个xml或json，文件名与原始图像一一对应，保存对应图像的标注相关信息</li><li>入选名单，一个txt文档，记录分到train_set和val_set的名单，每行一个去掉后缀的文件名</li></ol><h2 id="两项工作"><a href="#两项工作" class="headerlink" title="两项工作"></a>两项工作</h2><h3 id="图像标注"><a href="#图像标注" class="headerlink" title="图像标注"></a>图像标注</h3><ol><li>安装lableImg<blockquote><p>pip3 install labelImg   #中间遇到报错，参考<a href="https://stackoverflow.com/questions/59711301/install-pyqt5-5-14-1-on-linux">here</a>解决。</p></blockquote></li><li>新建一个文件夹，包含img目录，annotation目录，class.txt，分别用于存放待标注图像，标注结果，类别清单。<blockquote><p>labelImg ./img ./class.txt   #即可开始标注</p></blockquote></li><li>标注完成后写一个随机抽样脚本，生成train_set和val_set入选清单两个txt文件。<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs css">import os<br>import random<br><br>def <span class="hljs-selector-tag">main</span>():<br>    random.<span class="hljs-built_in">seed</span>(<span class="hljs-number">0</span>)  # 设置随机种子，保证随机结果可复现<br><br>    files_path = <span class="hljs-string">&quot;./annotation&quot;</span><br>    assert os.path.<span class="hljs-built_in">exists</span>(files_path), <span class="hljs-string">&quot;path: &#x27;&#123;&#125;&#x27; does not exist.&quot;</span>.<span class="hljs-built_in">format</span>(files_path)<br><br>    val_rate = <span class="hljs-number">0.5</span><br><br>    files_name = <span class="hljs-built_in">sorted</span>([file.<span class="hljs-built_in">split</span>(<span class="hljs-string">&quot;.&quot;</span>)[<span class="hljs-number">0</span>] for file in os.<span class="hljs-built_in">listdir</span>(files_path)])<br>    files_num = <span class="hljs-built_in">len</span>(files_name)<br>    val_index = random.<span class="hljs-built_in">sample</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, files_num), k=<span class="hljs-built_in">int</span>(files_num*val_rate))<br>    train_files = []<br>    val_files = []<br>    for index, file_name in <span class="hljs-built_in">enumerate</span>(files_name):<br>        if index in val_index:<br>            val_files.<span class="hljs-built_in">append</span>(file_name)<br>        else:<br>            train_files.<span class="hljs-built_in">append</span>(file_name)<br><br>    try:<br>        train_f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;train.txt&quot;</span>, <span class="hljs-string">&quot;x&quot;</span>)<br>        eval_f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;val.txt&quot;</span>, <span class="hljs-string">&quot;x&quot;</span>)<br>        train_f.<span class="hljs-built_in">write</span>(<span class="hljs-string">&quot;\n&quot;</span>.<span class="hljs-built_in">join</span>(train_files))<br>        eval_f.<span class="hljs-built_in">write</span>(<span class="hljs-string">&quot;\n&quot;</span>.<span class="hljs-built_in">join</span>(val_files))<br>    except FileExistsError as e:<br>        <span class="hljs-built_in">print</span>(e)<br>        <span class="hljs-built_in">exit</span>(<span class="hljs-number">1</span>)<br><br>if __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-built_in">main</span>()<br></code></pre></td></tr></table></figure></li></ol><h3 id="自定义dataset类"><a href="#自定义dataset类" class="headerlink" title="自定义dataset类"></a>自定义dataset类</h3><p>自定义一个dataset类需要先继承<code>torch.utils.data.Dataset</code>类，然后重写<code>__init__</code>方法，<code>__getitem__</code>方法，<code>__len__</code>方法</p><h4 id="init-方法"><a href="#init-方法" class="headerlink" title="__init__方法"></a><code>__init__</code>方法</h4><p>主要有以下几项工作：</p><ol><li>根据train_set.txt或者val_set.txt的内容建立<code>self.xml_list</code>，一个存放所有xml文件路径的list，并检查这些xml文件是否确实存在</li><li>根据描述类别信息的json文件建立<code>self.class_dict</code></li><li>根据传入参数设定预处理函数<code>self.transforms</code>的行为，包括RandomFlip等augmentation操作以及to_tensor，normalize等操作，在<code>getitem</code>方法中也就是具体内容前执行。</li></ol><h4 id="getitem-方法"><a href="#getitem-方法" class="headerlink" title="__getitem__方法"></a><code>__getitem__</code>方法</h4><p><code>__getitem__</code>方法目的是定义用[idx]索引本类示例时返回什么。在这里，是返回一个tuple，内容为<code>(image_tensor, img_attr_dict)</code>。整个函数的行为就是为了得到这两个元素。</p><ol><li>解析<code>self.xml_list[idx]</code>对应的xml，得到一个dict<blockquote><p>借助lxml的etree工具</p></blockquote></li><li>根据dict中的<code>filename</code>找到对应的图像文件，并转化tensor，得到<code>image_tensor</code><blockquote><p>借助PIL库的<code>Image.open()</code>函数</p></blockquote></li><li>根据dict中的其他信息找到boxes，label等标注信息，并转为tensor，最后组装成<code>img_attr_dict</code>。<blockquote><p>多目标情形时，box作为key，对应一个list of tensor，每个tensor对应一个box，label等同理。一个典型的包含两个box的dict如下：<br>{boxes:tensor(2x4),<br>labels:tensor(2x1),<br>image_id:tensor(1x1),<br>area:tensor(2x1),<br>iscrowd:tensor(2x1),<br>}</p></blockquote></li></ol><h4 id="len-方法"><a href="#len-方法" class="headerlink" title="__len__方法"></a><code>__len__</code>方法</h4><p>返回<code>self.xml_list</code>的长度即可</p><h4 id="one-more-thing"><a href="#one-more-thing" class="headerlink" title="one more thing"></a>one more thing</h4><p>当<code>torch.utils.data.DataLoader</code>按照既定的batch_size去获取和组装图像时，不能用默认的<code>collate_fn</code>。</p><p>因为这个默认的<code>collate_fn</code>只是简单的调用<code>torch.stack()</code>将b个[c, h, w]的tensor组合成一个[b, c, h, w]的tensor。但是这里索引到的是b个<code>(image_tensor, img_attr_dict)</code>，直接stack将得到一个</p><blockquote><p>[(image_tensor_0, img_attr_dict_0)<br> ···<br> (image_tensor_b, img_attr_dict_b)]</p></blockquote><p>这不是我们想要的。我们想要的是各个图像的像素值集中在一起，图像的标注信息集中在一起：</p><blockquote><p>[(image_tensor_0,<br>  …<br>  image_tensor_b),</p><p> (img_attr_dict_0,<br>  …,<br>  img_attr_dict_b)]</p></blockquote><p>所以要<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collate_fn</span>(<span class="hljs-params">batch</span>):</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">zip</span>(*batch))<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>pytorch使用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>目标检测</tag>
      
      <tag>数据集</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基础用法-预测</title>
    <link href="/2021/08/11/pytorch%E6%A0%B8%E5%BF%83%E7%94%A8%E6%B3%95-3/"/>
    <url>/2021/08/11/pytorch%E6%A0%B8%E5%BF%83%E7%94%A8%E6%B3%95-3/</url>
    
    <content type="html"><![CDATA[<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs julia">transform = transforms.Compose(<br>        [transforms.Resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),<br>         transforms.ToTensor(),<br>         transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<br><br><span class="hljs-literal">im</span> = Image.open(&#x27;<span class="hljs-number">1.</span>jpg&#x27;)<br><span class="hljs-literal">im</span> = transform(<span class="hljs-literal">im</span>)  <span class="hljs-comment"># [C, H, W]</span><br><span class="hljs-literal">im</span> = torch.unsqueeze(<span class="hljs-literal">im</span>, dim=<span class="hljs-number">0</span>)  <span class="hljs-comment"># [N, C, H, W]</span><br></code></pre></td></tr></table></figure><h2 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h2><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 不需要to(device)吗？mark</span><br><span class="hljs-attr">net</span> = LeNet()<br></code></pre></td></tr></table></figure><h2 id="参数加载"><a href="#参数加载" class="headerlink" title="参数加载"></a>参数加载</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 优化器也有同样的操作，可用于恢复训练<br>net.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(&#x27;Lenet.<span class="hljs-params">pth</span>&#x27;)</span>)<br></code></pre></td></tr></table></figure><h2 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">with torch<span class="hljs-selector-class">.no_grad</span>():<br>    outputs = net(im)<br>    predict = torch<span class="hljs-selector-class">.max</span>(outputs, dim=<span class="hljs-number">1</span>)<span class="hljs-selector-attr">[1]</span><span class="hljs-selector-class">.data</span><span class="hljs-selector-class">.numpy</span>()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>pytorch使用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基础用法-训练</title>
    <link href="/2021/08/11/pytorch%E6%A0%B8%E5%BF%83%E7%94%A8%E6%B3%95-2/"/>
    <url>/2021/08/11/pytorch%E6%A0%B8%E5%BF%83%E7%94%A8%E6%B3%95-2/</url>
    
    <content type="html"><![CDATA[<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="引入数据集"><a href="#引入数据集" class="headerlink" title="引入数据集"></a>引入数据集</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 图片在送入网络之前要进行一定的变化才能适应网络入口的要求，比如尺寸一致化，张量化，归一化等</span><br>data_transform = &#123;<br>        <span class="hljs-string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(224),<br>                                     transforms.RandomHorizontalFlip(),<br>                                     transforms.ToTensor(),<br>                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),<br>        <span class="hljs-string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((224, 224)), <br>                                   transforms.ToTensor(),<br>                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])&#125;<br><br><span class="hljs-comment"># 用pytorch官方的数据集，本地没有就去下载，需要指明是用数据集中的train部分，还是val部分</span><br>train_set = torchvision.datasets.CIFAR10(<span class="hljs-attribute">root</span>=<span class="hljs-string">&#x27;./data&#x27;</span>, <span class="hljs-attribute">train</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">download</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">transform</span>=transform)<br>val_set = torchvision.datasets.CIFAR10(<span class="hljs-attribute">root</span>=<span class="hljs-string">&#x27;./data&#x27;</span>, <span class="hljs-attribute">train</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">download</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">transform</span>=transform)<br><br><span class="hljs-comment"># 用自定义的数据集，前提是本地先下载好，并且建立好目录结构:</span><br><span class="hljs-comment"># image_path</span><br><span class="hljs-comment"># - train</span><br><span class="hljs-comment"># - class1</span><br><span class="hljs-comment">#- 1.png</span><br><span class="hljs-comment">#- 2.png</span><br><span class="hljs-comment"># - class2</span><br><span class="hljs-comment">#- 1.png</span><br><span class="hljs-comment">#- 2.png</span><br><span class="hljs-comment"># - val</span><br><span class="hljs-comment"># - class1</span><br><span class="hljs-comment">#- 1.png</span><br><span class="hljs-comment">#- 2.png</span><br><span class="hljs-comment"># - class2</span><br><span class="hljs-comment">#- 1.png</span><br><span class="hljs-comment">#- 2.png</span><br>train_dataset = datasets.ImageFolder(<span class="hljs-attribute">root</span>=os.path.join(image_path, <span class="hljs-string">&quot;train&quot;</span>), <span class="hljs-attribute">transform</span>=data_transform[<span class="hljs-string">&quot;train&quot;</span>])<br>validate_dataset = datasets.ImageFolder(<span class="hljs-attribute">root</span>=os.path.join(image_path, <span class="hljs-string">&quot;val&quot;</span>), <span class="hljs-attribute">transform</span>=data_transform[<span class="hljs-string">&quot;val&quot;</span>])<br><br><span class="hljs-comment"># 最后得到的是一个torch定义的class，按[]索引得到的是一个tuple，tuple元素是(tensor,id)，每个tensor对应一个张量化后的图片。</span><br></code></pre></td></tr></table></figure><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 用num_workers个线程，将train_set打乱顺序，分成36个一份的batch</span><br>train_loader = torch.utils.data.DataLoader(train_set, <span class="hljs-attribute">batch_size</span>=36, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">num_workers</span>=0)<br><br><span class="hljs-comment"># 最后得到也是一个torch定义的class，被enumerate返回一个[tensor,tensor]</span><br><span class="hljs-comment"># 第一个tensor代表inputs，维度为[batch_size, c, h, w]</span><br><span class="hljs-comment"># 第二个为labels，维度为[batch_size]</span><br><span class="hljs-comment"># 也可以用iter将其加工成一个可迭代对象来访问其内容</span><br>train_data_iter = iter(train_loader)<br>img, label = train_data_iter.next()<br></code></pre></td></tr></table></figure><h2 id="工具定义"><a href="#工具定义" class="headerlink" title="工具定义"></a>工具定义</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs nix">from model <span class="hljs-built_in">import</span> LeNet<br><span class="hljs-built_in">import</span> torch.optim as optim<br><br><span class="hljs-comment"># 模型定义</span><br><span class="hljs-attr">net</span> = LeNet()<br>net.to(device)<br><br><span class="hljs-comment"># 损失函数定义</span><br><span class="hljs-attr">loss_function</span> = nn.CrossEntropyLoss()<br><br><span class="hljs-comment"># 优化器定义</span><br><span class="hljs-comment"># 注意这里的优化器不是按照SGD/BGD/MBGD这样实现的，因为这种分类方式依赖的分类标准是batch_size的值。也就是说这里的SGD其实是基本的GD，然后如果实际的batch_size是1，那么才是真正的SGD。而且如果加上momentum参数，那么就是GD with Momentum。在定义优化器是需要指定的其实是参数更新的方式，而不是每次送入几个样本。</span><br><span class="hljs-attr">params</span> = [p for p <span class="hljs-keyword">in</span> model.parameters() <span class="hljs-keyword">if</span> p.requires_grad] <span class="hljs-comment">#可以在这里之前将某些层的参数锁定，从而实现冻结某些层不训练的效果</span><br><span class="hljs-attr">optimizer</span> = optim.Adam(params, <span class="hljs-attr">lr=0.0002)</span><br><span class="hljs-attr">optimizer</span> = optim.SGD(params, <span class="hljs-attr">lr=0.0002,</span> <span class="hljs-attr">momentum=0.1)</span><br><br><br><span class="hljs-comment"># 学习率调整器定义</span><br><span class="hljs-comment"># 在整个训练过程中，学习率保持一个定值效果往往不太好。因此一般用这个调整器来规划学习率随着epoch数量增加的变化策略。一种流行的做法是三角学习率，即学习率先增后减。开始阶段过大容易直接跑飞，结束阶段过大很难收敛，中间过程太小又会导致损失下降太慢，因此先增后减。</span><br><br>def f(x):<br>     <span class="hljs-keyword">if</span> x &gt;= warmup_iters:  <span class="hljs-comment"># 当迭代数大于给定的warmup_iters时，倍率因子为1</span><br>         return <span class="hljs-number">1</span><br>     <span class="hljs-attr">alpha</span> = float(x) / warmup_iters<br>     <span class="hljs-comment"># 迭代过程中倍率因子从warmup_factor -&gt; 1</span><br>     <span class="hljs-attr">fx</span> = warmup_factor * (<span class="hljs-number">1</span> - alpha) + alpha<br>     return fx<br><span class="hljs-attr">lr_warmpuper</span> = torch.optim.lr_scheduler.LambdaLR(optimizer, <span class="hljs-attr">lr_lambda=f)</span> <span class="hljs-comment">#第i次调用step方法时会将optimizer的学习率变为initial_alpha*f(i)</span><br><span class="hljs-attr">lr_scheduler</span> = torch.optim.lr_scheduler.StepLR(optimizer, <span class="hljs-attr">step_size=3,</span> <span class="hljs-attr">gamma=0.33)</span> <span class="hljs-comment">#每调用3次step方法，optimizer的学习率乘以一次0.33</span><br></code></pre></td></tr></table></figure><h2 id="核心循环"><a href="#核心循环" class="headerlink" title="核心循环"></a>核心循环</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs nix">for epoch <span class="hljs-keyword">in</span> range(epochs):<br><span class="hljs-comment"># 训练循环</span><br>        <span class="hljs-comment"># 设置train模式，以打开BN，Dropout等训练专用的运算</span><br>        net.train()<br>        for data <span class="hljs-keyword">in</span> train_loader:<br>            images, <span class="hljs-attr">labels</span> = data<br>            <span class="hljs-comment"># 重置梯度变换量，防止过大？mark</span><br>            optimizer.zero_grad()<br>            <span class="hljs-comment"># 正向计算</span><br>            <span class="hljs-attr">outputs</span> = net(images.to(device))<br>            <span class="hljs-comment"># 计算损失</span><br>            <span class="hljs-attr">loss</span> = loss_function(outputs, labels.to(device))<br>            <span class="hljs-comment"># 反向传播</span><br>            loss.backward()<br>            <span class="hljs-comment"># 梯度更新 </span><br>    optimizer.step()<br>    <span class="hljs-keyword">if</span> <span class="hljs-attr">epoch==0:</span>  <span class="hljs-comment"># 第一轮使用warmup训练方式，每一个iteration调整一次学习率</span><br>               lr_warmuper.step()         <br>lr_scheduler.step()<br><span class="hljs-comment"># 验证循环</span><br>        <span class="hljs-comment"># 设置eval模式，以关闭BN，Dropout等训练专用的运算</span><br>        net.eval()<br>        <span class="hljs-comment"># 验证过程不需要自动存储梯度？mark</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            for val_data <span class="hljs-keyword">in</span> val_loader:<br>                val_images, <span class="hljs-attr">val_labels</span> = val_data<br>                <span class="hljs-attr">outputs</span> = net(val_images.to(device))<br>                <span class="hljs-comment"># 求softmax向量的最大值的索引作为预测结果。</span><br>                <span class="hljs-attr">predict_y</span> = torch.max(outputs, <span class="hljs-attr">dim=1)[1]</span><br><span class="hljs-comment"># 模型保存</span><br>        <span class="hljs-keyword">if</span> val_accurate &gt; best_acc:<br>            <span class="hljs-attr">best_acc</span> = val_accurate<br>            <span class="hljs-comment"># 只保存权重，官方推荐这一种</span><br>            torch.save(net.state_dict(), save_weight_path)<br>            <span class="hljs-comment"># 同时保存和网络</span><br>            torch.save(net, save_model_path)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>pytorch使用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基础用法-模型</title>
    <link href="/2021/08/11/pytorch%E6%A0%B8%E5%BF%83%E7%94%A8%E6%B3%95-1/"/>
    <url>/2021/08/11/pytorch%E6%A0%B8%E5%BF%83%E7%94%A8%E6%B3%95-1/</url>
    
    <content type="html"><![CDATA[<p>搭建一个CNN模型用作图像分类的过程，整体而言还是比较清晰的，主要分为模型、训练、预测三项工作。</p><p>第一项工作，模型搭建，套路也很清晰。新建一个继承于<code>nn.Module</code>的类，然后</p><ol><li>将网络层一一作为类的成员添加进来。这些网络层可以是<code>Conv2d</code>, <code>MaxPool2d</code>, <code>Linear</code>等<code>torch.nn</code>提供的基本模块，也可以是由这些基本模块组合成的自定义building blocks，如ResNet中的Residual单元，GoogLeNet中的Inception单元等。</li><li>定义网络层之间的连接关系，也就是<code>forward</code>函数的行为。</li></ol><p>另外，一种常见的做法是在定义网络层成员的时候，直接使用<code>nn.sequential</code>函数将多个网络层连接模块（比如一个classifier和一个features），然后在<code>forward</code>中连接这些模块即可。<br></p><p>BN, Dropout, Relu都可以作为一个网络层存在和连接。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch.nn as nn<br><span class="hljs-attribute">import</span> torch.nn.functional as F<br><br><span class="hljs-attribute">class</span> LeNet(nn.Module):<br>    <span class="hljs-comment"># 设定有哪些网络层，每层的尺寸</span><br>    <span class="hljs-attribute">def</span> __init__(self):<br>        <span class="hljs-attribute">super</span>(LeNet, self).__init__()<br>        <span class="hljs-attribute">self</span>.conv<span class="hljs-number">1</span> = nn.Conv<span class="hljs-number">2</span>d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">self</span>.pool<span class="hljs-number">1</span> = nn.MaxPool<span class="hljs-number">2</span>d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-attribute">self</span>.conv<span class="hljs-number">2</span> = nn.Conv<span class="hljs-number">2</span>d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">self</span>.pool<span class="hljs-number">2</span> = nn.MaxPool<span class="hljs-number">2</span>d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-attribute">self</span>.fc<span class="hljs-number">1</span> = nn.Linear(<span class="hljs-number">32</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-attribute">self</span>.fc<span class="hljs-number">2</span> = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-attribute">self</span>.fc<span class="hljs-number">3</span> = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-comment"># 设定层与层之间的怎样连接</span><br>    <span class="hljs-attribute">def</span> forward(self, x):<br>        <span class="hljs-attribute">x</span> = F.relu(self.conv<span class="hljs-number">1</span>(x))    # input(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>) output(<span class="hljs-number">16</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>        <span class="hljs-attribute">x</span> = self.pool<span class="hljs-number">1</span>(x)            # output(<span class="hljs-number">16</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>)<br>        <span class="hljs-attribute">x</span> = F.relu(self.conv<span class="hljs-number">2</span>(x))    # output(<span class="hljs-number">32</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-attribute">x</span> = self.pool<span class="hljs-number">2</span>(x)            # output(<span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">x</span> = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">32</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>)       # output(<span class="hljs-number">32</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">x</span> = F.relu(self.fc<span class="hljs-number">1</span>(x))      # output(<span class="hljs-number">120</span>)<br>        <span class="hljs-attribute">x</span> = F.relu(self.fc<span class="hljs-number">2</span>(x))      # output(<span class="hljs-number">84</span>)<br>        <span class="hljs-attribute">x</span> = self.fc<span class="hljs-number">3</span>(x)              # output(<span class="hljs-number">10</span>)<br>        <span class="hljs-attribute">return</span> x<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>pytorch使用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>流形(manifold)是个啥?</title>
    <link href="/2021/08/10/manifold%E6%98%AF%E4%B8%AA%E5%95%A5/"/>
    <url>/2021/08/10/manifold%E6%98%AF%E4%B8%AA%E5%95%A5/</url>
    
    <content type="html"><![CDATA[<p>今天看MobileNet v2，其中为了得到<strong><em>“在处理低维度的特征tensor时，Relu激活函数相比于Linear激活函数在激活前后会造成更多的特征丢失”</em></strong>这一推论，使用了流形（manifold）的概念。所以来看看流形到底是个啥：</p><p>数学语言的定义是：<strong><em>流形是局部具有欧几里得空间性质的空间，在数学中用于描述几何形体。物理上，经典力学的相空间和构造广义相对论的时空模型的四维伪黎曼流形都是流形的实例</em></strong>。牵扯出更多新概念，比较复杂。所以这里只看machine learning中的流形概念。如下图所示，</p><div align=center><img title="" src="/img/article/manifold.jpeg" width="60%" height="60%" align=center></div><p><br>图中的每一个点，严格来讲都是一个三维的点。但是又可以明显看出这些点在三维空间中的分布是非常有规律的，基本上都集中在一个三维曲面上。基于这种现象，我们将流形理解为，高维空间中一个可变换成低维空间的子空间，例如上图，这个曲面作为一个三维曲面其实可以变换为一个二维平面。</p><p>对于机器学习和深度学习领域的数据，一般数据因为固有的特性（比如在不同光照条件下的同一张人脸的灰度图像），导致无法“填满”整个高维空间。例如如果数据只能出现在三维空间中的一个球面上。那这个球面以外的空间永远不会有数据点。而一个表面我们完全可以只用两个参数来表示(经度、维度)。</p><p>这样在聚类求样本间的相似距离时，就有了两种方式。一种是在流形的高维空间中求，即图中直线。另一种是在流形展开后的低维空间中求，即图中的曲线。具体用途，后面谈到再说吧。mark</p><p>参考链接：<br>[1] <a href="https://www.zhihu.com/question/24015486/answer/194284643">https://www.zhihu.com/question/24015486/answer/194284643</a></p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>流形</tag>
      
      <tag>manifold</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MobileNet v2</title>
    <link href="/2021/08/08/2-MobileNet/"/>
    <url>/2021/08/08/2-MobileNet/</url>
    
    <content type="html"><![CDATA[<h2 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h2><p>2018年Google团队在v1的基础上提出的改进版。核心思想仍然是DW卷积，但是这次借鉴了ResNet的残差结构，提高了性能。</p><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><h3 id="Inverted-Residual"><a href="#Inverted-Residual" class="headerlink" title="Inverted Residual"></a>Inverted Residual</h3><p>所谓倒残差结构，其实就是在ResNet残差结构的基础上做了一些修改。首先看以下简图：</p><div align=center><img title="" src="/img/net/ir.png" width="70%" height="70%" align=center></div><p><br>对于残差结构而言，三个卷积层每一层卷积核的个数一般依次为[64, 64, 256]这样，先对具有256个通道的输入进行降维，然后3x3卷积 with SAME PADDING, 尺寸和通道都不变，最后再用1x1卷积进行升维。就三个卷积层的4个计算对象而言，通道数依次为[256, 64, 64, 256]，两头大中间小。</p><p>对于倒残差结构则刚好相反，它先升维，再用DW卷积保持深度，然后再降维输出，呈现两头小中间大的形状，因此称之为倒残差。</p><p>具体来看倒残差结构的结构和参数，如下表所示：<br><br><div align=center><img title="" src="/img/net/ir2.png" width="60%" height="60%" align=center></div><br></p><ol><li>第一层为PW卷积+BN+RELU6，其中卷积层参数为1F-1S-SP-tk#，k为单元输入的通道数；</li><li>第二层为DW卷积+BN+RELU6，参数为3F-sS-SP, 当s=1或2；</li><li>第三层为PW卷积+BN+Linear，卷积层参数为1F-1S-SP-k’#，k’为单元输出的通道数；</li></ol><p>综上，一个倒残差单元的参数有k，t，s，k’四个。4个计算对象的深度分别为[k, tk, tk, k’]。</p><h3 id="Relu6激活函数"><a href="#Relu6激活函数" class="headerlink" title="Relu6激活函数"></a>Relu6激活函数</h3><p>倒残差结构相对于残差结构的另外一个不同点，其实就是在x=6处饱和的Relu函数。</p><h3 id="Linear激活函数"><a href="#Linear激活函数" class="headerlink" title="Linear激活函数"></a>Linear激活函数</h3><p>倒残差单元输出之前进行的不是Relu激活，而是线性激活。因为，据称，在对通道数较少的tensor进行激活时，Relu会丢失更多信息。而每个倒残差单元又是两头小，因此最后的激活函数使用了简单的线性函数。另外，由于线性激活函数的表达式就是$y = x$，所以线性激活=不做激活。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>整个网络来看，先用倒残差单元串联组成单元层，单元层和普通卷积层、平均池化层、FC层再串联组成网络。</p><div align=center><img title="" src="/img/net/mbv2.png" width="50%" height="50%" align=center></div><p>看懂上表首先要明确一个单元层的4个参数t,c,n,s的含义：</p><ul><li>t, 对应倒残差单元的参数t</li><li>c, 对应本单元层中的第一个倒残差单元的k’参数，以及后续的倒残差单元的k和k’参数。就是说对于一个单元层，计算对象的深度依次为[input, input x t, input x t, c]、[c, c x t, c x t, c]、[c, c x t, c x t, c]……</li><li>n, 将几个倒残差单元串联起来形成本单元层</li><li>s，对应本单元层中第一个倒残差单元的s参数，其余单元的s参数都为1</li></ul><p>另外，还要注意以下3个细节：</p><ol><li>每个单元层的第一个倒残差单元与ResNet中的处理方式不一样，这里是放弃shortcut而不是在shortcut上插入一个1x1卷积来进行深度的一致化。</li><li>当每个单元层的中间单元，不满足输入输出的tensor形状、深度一致的条件时，也不进行shortcut。</li><li>对于t=1的单元层，取消其倒残差单元的第一个PW卷积，因为通道数量并不需要变化。</li></ol><h2 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h2><ol><li>倒残差结构的三个特点为什么work？mark</li><li>DW卷积在pytorch中的实现是通过控制<code>nn.conv2D()</code>构造函数中的<code>groups</code>参数来实现。<code>groups = 1</code>时就普通的卷积，<code>groups = input_chs</code>时就是DW卷积。</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>经典网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>经典网络</tag>
      
      <tag>Residual结构</tag>
      
      <tag>MobileNet</tag>
      
      <tag>Inverted Residual结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MobileNet v1</title>
    <link href="/2021/08/08/MobileNet/"/>
    <url>/2021/08/08/MobileNet/</url>
    
    <content type="html"><![CDATA[<h2 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h2><p>由Google团队于2017年提出，专注于移动端和嵌入式设备。采用Depthwise卷积，大幅减少参数量，虽然效果有一些折扣，但是可以接受。</p><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><h3 id="DW卷积"><a href="#DW卷积" class="headerlink" title="DW卷积"></a><strong>DW卷积</strong></h3><p>先来回忆以下常规的卷积是怎样的，如下图所示。</p><div align=center><img title="" src="/img/net/mobilenet1.png" width="70%" height="70%" align=center></div><p>而DW卷积呢，其实就是最符合直觉的那种卷积。就是每个卷积核的深度不是和上一层输出相同，而是等于1。这意味着：</p><ul><li>卷积之后不再做跨深度的求和</li><li>每一个DW卷积层的#参数必然等于输入tensor的深度，因为要保证输入tensor的每一个通道都有一个1层的卷积核去和它运算</li><li>经过DW卷积之后得到的输出，深度不变</li></ul><div align=center><img title="" src="/img/net/mobilenet2.png" width="70%" height="70%" align=center></div><h3 id="PW卷积"><a href="#PW卷积" class="headerlink" title="PW卷积"></a><strong>PW卷积</strong></h3><p>DW卷积一看就是个瘸腿儿的机制，搞来搞去没法manipulate tensor的深度啊，所以必须伴随这PW卷积使用。PW卷积就是常规1x1的卷积，它的特点是不改变tensor的尺寸，只改变tensor的深度。这么一来跟DW卷积就完美配合了——先对tensor进行DW卷积改变尺寸，然后进行PW卷积对DW的输出进行跨通道加权求和，每个PW卷积核得到一个2维的tensor，这样，通过设置不同的卷积核个数即可控制最终输出的通道数。</p><div align=center><img title="" src="/img/net/mobilenet3.png" width="70%" height="70%" align=center></div><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><div align=center><img title="" src="/img/net/mbv1.png" width="60%" height="60%" align=center></div><p>主要就是DW层和PW层的堆叠。另外，额外设有两个可调的超参数$\alpha$和$\beta$，前者是每一层的PW卷积核数量基于上表中数量的系数，后者是基于224x224的输入图像分辨率的系数。</p><h2 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h2><ol><li>实际使用发现，经常有很多DW卷积核的元素值为0，造成浪费，原因后面再补充吧。mark</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>经典网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>经典网络</tag>
      
      <tag>MobileNet</tag>
      
      <tag>DW卷积</tag>
      
      <tag>PW卷积</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ResNet</title>
    <link href="/2021/08/08/ResNet/"/>
    <url>/2021/08/08/ResNet/</url>
    
    <content type="html"><![CDATA[<h2 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h2><p>来自2015年的微软实验室，He Kaiming/Ren Shaoqing/Zhang Xiangyu等，啥啥啥都第一名…不用dropout了，引入BN，提出residual以应对退化问题，支持上千层的网络结构。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>网络由一种叫做residual的building block串行组建而成，整体结构见下表：</p><div align=center><img title="" src="/img/net/resnet.png" width="100%" height="100%" align=center></div><h3 id="常规的Residual单元"><a href="#常规的Residual单元" class="headerlink" title="常规的Residual单元"></a>常规的Residual单元</h3><p>如下图所示，常规的Residual有两种规格，前者用于层数较少的网络，后者用于层数较多的网络。</p><div align=center><img title="" src="/img/net/residual1.png" width="80%" height="80%" align=center></div><p>Residual特点如下：</p><ol><li>为了实现shortcut结构，必须保证单元的输入与输出尺寸完全一致，包括维度。这样才能做tensor的逐元素相加。</li><li>最后一个relu是在逐元素相加之后进行的。</li><li>和Inception单元一样，每个卷积层的FSP都是确定的，其中F看位置取1或3，S=1，P=SAME PADDING。因此一个Residual单元的配置参数只有每个卷积层的#。</li></ol><h3 id="特殊的Residual单元"><a href="#特殊的Residual单元" class="headerlink" title="特殊的Residual单元"></a>特殊的Residual单元</h3><p>在分界线处的Residual单元还要额外承担tensor尺寸调整的任务，因此其结构相对于常规结构要增加一些变化。</p><div align=center><img title="" src="/img/net/residual2.png" width="60%" height="60%" align=center></div><div align=center><img title="" src="/img/net/residual3.png" width="60%" height="60%" align=center></div><p>首先是在shortcut上增加一个1F2S0P?#的卷积层对单元输入进行尺寸调整。另外是主线上的第一个卷积层的S变为2，进行尺寸调整。</p><blockquote><p>上图是按照pytorch中的实现，是在第2个卷积层做的下采样，实测效果更好，所以这么用了。</p></blockquote><h2 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h2><ol><li><strong>Residual结构为什么有效？</strong><br>xxxx</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>经典网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>经典网络</tag>
      
      <tag>ResNet</tag>
      
      <tag>Residual结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>argparse的基本用法</title>
    <link href="/2021/08/06/argparse%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/"/>
    <url>/2021/08/06/argparse%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h3 id="用途及原理"><a href="#用途及原理" class="headerlink" title="用途及原理"></a>用途及原理</h3><p>为python脚本运行时所添加额外命令行参数提供解释。大致原理是为一个parser对象添加一些属性，然后这个parser对象在接受一系列参数（默认是命令行参数）之后，就拥有了一系列属性，然后程序就可以用这些属性来做事了。就是说，信息从命令行传递到了脚本内。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 创建对象</span><br>import argparse<br>parser = argparse.ArgumentParser()<br><br><span class="hljs-comment"># 添加属性</span><br>parser.add_argument(<span class="hljs-string">&#x27;integers&#x27;</span>, <span class="hljs-regexp">//</span>位置型属性，与添加顺序有关<br>    metavar=<span class="hljs-string">&#x27;N&#x27;</span>,  <span class="hljs-regexp">//</span>帮助文档中的示例显示内容<br>    type=int,  <span class="hljs-regexp">//</span>这个属性的类型<br>    nargs=<span class="hljs-string">&#x27;+&#x27;</span>, <span class="hljs-regexp">//</span>这个属性的值为自适应长度的列表<br>                    help=<span class="hljs-string">&#x27;an integer for the accumulator&#x27;</span>) <span class="hljs-regexp">//</span>与这个属性相关的帮助信息<br>parser.add_argument(<span class="hljs-string">&#x27;--sum&#x27;</span>,  <span class="hljs-regexp">//</span>flag型属性，位置无关<br>                    dest=<span class="hljs-string">&#x27;accumulate&#x27;</span>, <span class="hljs-regexp">//</span>属性的正式名称，有了这个之后sum这个名称就不能用了，没有的话sum是正式名称<br>    action=<span class="hljs-string">&#x27;store_const&#x27;</span>, <span class="hljs-regexp">//</span>与下面的const配合使用，定义--sum在命令行中出现时，sum属性的赋值行为，这里是accumulate = sum<br>                    const=sum, <br>    default=max, <span class="hljs-regexp">//</span>定义当--sum未出现时sum属性的赋值行为，这里是accumulate = max。<br>                    help=<span class="hljs-string">&#x27;sum the integers (default: find the max)&#x27;</span>)<br><br><span class="hljs-comment"># 接受参数</span><br>args = parser.parse_args()<br><br><span class="hljs-comment"># 使用参数</span><br>print(args.accumulate(args.integers)) <span class="hljs-regexp">//</span>解析后等价于print(sum([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br></code></pre></td></tr></table></figure><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p>[1] <a href="https://towardsdatascience.com/a-simple-guide-to-command-line-arguments-with-argparse-6824c30ab1c3">https://towardsdatascience.com/a-simple-guide-to-command-line-arguments-with-argparse-6824c30ab1c3</a><br>[2] <a href="https://www.huaweicloud.com/articles/208c06ca1f4aba8dfd9219c6e2c72b23.html">https://www.huaweicloud.com/articles/208c06ca1f4aba8dfd9219c6e2c72b23.html</a><br>[3] <a href="https://blog.csdn.net/liuweiyuxiang/article/details/82918911">https://blog.csdn.net/liuweiyuxiang/article/details/82918911</a><br>[4] <a href="https://docs.python.org/zh-cn/3/library/argparse.html">https://docs.python.org/zh-cn/3/library/argparse.html</a></p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>python笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>argparse</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GoogLeNet</title>
    <link href="/2021/08/03/GoogleNet/"/>
    <url>/2021/08/03/GoogleNet/</url>
    
    <content type="html"><![CDATA[<h2 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h2><p>2014年Google团队提出，当年的ImageNet分类任务第一名（第二名是VGG）。其中的Inception结构和辅助分类器的概念很有启发性和创造性。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>构建起整个网络的building blocks有5种：</p><ol><li>常规的卷积层，默认以RELU为激活函数，SAME padding</li><li>常规的池化层，包括最大池化和平均池化</li><li>Inception单元，共计3层9个，具体结构后面解释</li><li>辅助分类器2个，具体结构后面解释</li><li>最终输出的softmax FC层</li></ol><div align=center><img title="" src="/img/net/googlenet.png" width="90%" height="90%" align=center></div><p><br>如上图所示，常规卷积层、池化层、softmax FC层的结构参数已标出。各个Inception单元的参数可参照下表得出，表头的解释详见<a href="https://towardsdatascience.com/deep-learning-googlenet-explained-de8861c82765">这里</a>。辅助分类器的结构是固定的，在后面会讲。</p><blockquote><p>第一个maxpool之后，第二个maxpool之前还各有一个LRN操作，但据说用处不大，就省略了。</p></blockquote><div align=center><img title="" src="/img/net/googlenet_table.png" width="100%" height="100%" align=center></div><h3 id="Inception单元"><a href="#Inception单元" class="headerlink" title="Inception单元"></a>Inception单元</h3><p>理解GoogLeNet的网络结构首先要理解Inception结构，整个GoogLeNet主要由若干个Inception单元的组合。</p><div align=center><img title="" src="/img/net/inception.png" width="80%" height="80%" align=center></div><p>从上图可以看出，对于一个Inception单元而言，input通过4个并联的卷积运算分支产生4路输出。这4个分支的卷积核各不相同，意味着从input提取出不同尺度的特征，这使得训练出来的模型能够同时兼顾一副图像中不同尺度的特征。并且，4路输出的tensor的宽和高是相同的，因此可以将其沿着深度方向堆叠起来，形成整个Inception单元的输出。看下图更直观的说明了这个过程</p><div align=center><img title="" src="/img/net/inception_eg.png" width="80%" height="80%" align=center></div><p>具体来看这4路分支：</p><ul><li>1x1卷积，调整深度，宽和高相对于input不变。</li><li>1x1卷积 + 3x3卷积，其中会造成尺寸缩减的3x3卷积采用SAME padding，同样保持了尺寸。</li><li>1x1卷积 + 5x5卷积，其中会造成尺寸缩减的5x5卷积采用SAME padding，同样保持了尺寸。</li><li>3x3池化 + 1x1卷积，其中池化层做了SAME padding。<strong><em>——先后顺序为什么这么突出？mark</em></strong></li></ul><blockquote><p>另外值得注意的是，在对于一个Inception单元而言，4个支路的卷积核数量一般是配置参数，而卷积核的尺寸是固定的。这意味着一个Inception单元的参数应该为上述6个卷积核的个数+本单元的输入的深度。</p></blockquote><h3 id="辅助分类器"><a href="#辅助分类器" class="headerlink" title="辅助分类器"></a>辅助分类器</h3><p>另外一个特殊之处是，GoogLeNet在进行模型训练时（预测时没有）在中间层也进行了两次结果输出。</p><h4 id="辅助分类器的结构"><a href="#辅助分类器的结构" class="headerlink" title="辅助分类器的结构"></a>辅助分类器的结构</h4><p>这两个辅助分类器的结构是相同的：</p><div align=center><img title="" src="/img/net/inception_aux.png" width="20%" height="20%" align=center></div><ul><li>首先是一个5x5,s=3的平均池化</li><li>然后接一个1x1卷积层，卷积核数量128</li><li>然后展平、dropout、接第一个FC（激活函数为RELU）</li><li>然后在dropout，接第二个FC(激活函数为softmax)</li></ul><p>第一层FC有1024个神经元，第二层为输出层，神经元数量取决于class_num。另外值得注意的是，两个辅助分类器的输入tensor的形状是相同的，只有深度不同，都是[batch, d, 4, 4]。经过相同的池化层处理后，形状继续保持一致。再经过相同的1x1卷积层处理后，深度也变为一致的。因此可以看出这里的1x1卷积层是为了rectify辅助分类器的输入，使其适配性更强。</p><h4 id="对应的损失函数"><a href="#对应的损失函数" class="headerlink" title="对应的损失函数"></a>对应的损失函数</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">loss0 = loss<span class="hljs-constructor">_function(<span class="hljs-params">logits</span>, <span class="hljs-params">labels</span>.<span class="hljs-params">to</span>(<span class="hljs-params">device</span>)</span>)<br>loss1 = loss<span class="hljs-constructor">_function(<span class="hljs-params">aux_logits1</span>, <span class="hljs-params">labels</span>.<span class="hljs-params">to</span>(<span class="hljs-params">device</span>)</span>)<br>loss2 = loss<span class="hljs-constructor">_function(<span class="hljs-params">aux_logits2</span>, <span class="hljs-params">labels</span>.<span class="hljs-params">to</span>(<span class="hljs-params">device</span>)</span><br>loss = loss0 + loss1<span class="hljs-operator"> * </span><span class="hljs-number">0.3</span> + loss2<span class="hljs-operator"> * </span><span class="hljs-number">0.3</span><br></code></pre></td></tr></table></figure><p>三个分类器的输出分别计算loss，然后加权之后的和作为整个模型的loss。</p><h4 id="实现时的trick"><a href="#实现时的trick" class="headerlink" title="实现时的trick"></a>实现时的trick</h4><p>由于最终的结果只看主分类器的，那么就带了两个问题：</p><ol><li>训练过程的验证时不需要进行辅助分类器的计算，此时通过在模型的forward方法中借助<code>self.training</code>设定即可。该属性在<code>net.train()</code>时为true，<code>net.eval()</code>时为false；</li><li>实际部署时，不仅不需要辅助分类器的计算，连辅助分类器的结构都不需要。此时，首先通过设定一个额外的flag，在model class中不构建辅助分类器。然后还要解决保存训练时保存的权重文件比此时的网络结构的参数多的问题。这个通过在<code>model.load_state_dict(torch.load(PATH),strict=False)</code>增加strict参数来实现。</li></ol><h2 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h2><ul><li>Inception结构中的卷积层默认包含一个RELU。</li><li>池化层跟1x1卷积刚好相反，它不改变tensor的深度，只改变tensor的宽和高，因此两者可以组成tensor尺寸manipulation的一对基。</li><li>tensor的堆叠在pytorch中用<code>torch.cat()</code>实现。</li></ul>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>经典网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>经典网络</tag>
      
      <tag>Inception结构</tag>
      
      <tag>GoogLeNet</tag>
      
      <tag>辅助分类器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VGGnet</title>
    <link href="/2021/08/01/VGGnet/"/>
    <url>/2021/08/01/VGGnet/</url>
    
    <content type="html"><![CDATA[<h2 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h2><p>牛津VGG团队2014年提出，ImageNet分类任务第二名，定位任务第一名。结构简单，配置丰富，被广泛用作backbone。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>原作有5种配置，分别为11层，13层，16层-1，16层-3，19层。统计层数时不统计池化层和softmax，只统计有weight的网络层。</p><div align=center><img title="" src="/img/net/vgg.png" width="80%" height="80%" align=center></div><h2 id="网络特点"><a href="#网络特点" class="headerlink" title="网络特点"></a>网络特点</h2><ul><li>所有的卷积核尺寸一致，均采用3x3。</li><li><p>引入<strong>感受野（receptive field）</strong>的概念，利用小卷积核的堆叠代替大卷积核，从而减少了参数量。所谓感受野，简单说就是对于某一层的输出的矩阵中的一个元素而言，它对应直接输入层中的那一片区域，就叫做它在那一层的感受野，然后依次向前递归。<br><div align=center><img title="" src="/img/net/rf.png" width="60%" height="60%" align=center></div><br>VGG网络对于这个概念，使用了这么一个推论：<strong>相同的感受野得到的特征表达的含义应该是同质的。</strong>因此，在同一个感受野上进行一层大卷积核的卷积操作，和进行多层小卷积核的卷积操作，最终得到的特征应该是同质的。具体操作是，用两层3x3的卷积来表达一层5x5的卷积效果，用三层3x3的卷积来表达一层7x7的卷积效果。由此引入一个计算过程，即如何计算某一层L在其之前某层N上的感受野尺寸。计算公式如下：</p><blockquote><script type="math/tex; mode=display">RF_n = (RF_{n+1} - 1) * s + f</script></blockquote><p>该公式其中就是卷积尺寸计算公式的移项，可以理解为已知卷积后的尺寸为1x1，求卷积前的尺寸。并且，不考虑padding，为什么呢？因为考虑padding考虑的是全局尺寸，而感受野讨论的是局部尺寸。</p></li><li><p>参数量减少的原理</p><blockquote><script type="math/tex; mode=display">3*3*3 = 27 < 49 = 7*7</script></blockquote></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>经典网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>经典网络</tag>
      
      <tag>VGGnet</tag>
      
      <tag>感受野</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预测结果评价的量化指标</title>
    <link href="/2021/08/01/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E8%AF%84%E4%BB%B7%E7%9A%84%E9%87%8F%E5%8C%96%E6%8C%87%E6%A0%87/"/>
    <url>/2021/08/01/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E8%AF%84%E4%BB%B7%E7%9A%84%E9%87%8F%E5%8C%96%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="二分类情形下的P-N-T-F概念"><a href="#二分类情形下的P-N-T-F概念" class="headerlink" title="二分类情形下的P/N/T/F概念"></a>二分类情形下的P/N/T/F概念</h3><ol><li>Positive/Negative：样本空间的原始分类;</li><li>True/False，预测结果与样本原始属性的偏差情况，即P-&gt;P/N-&gt;N为T，P-&gt;N/N-&gt;P为F;因此，T和F需要再细分为：<ul><li>TP：P-&gt;P(正类被预测为正类，真正类)</li><li>TN：N-&gt;N(负类被预测为负类，真负类)</li><li>FP：N-&gt;P(负类被预测为正类，假正类)</li><li>FN：P-&gt;N(正类被预测为负类，假负类）</li></ul></li></ol><h3 id="多分类情形下的P-N-T-F概念"><a href="#多分类情形下的P-N-T-F概念" class="headerlink" title="多分类情形下的P/N/T/F概念"></a>多分类情形下的P/N/T/F概念</h3><p>多分类时需要针对单独每一个类别进行分析，然后再综合分析。比如，对于一个三分类情形，先分析类别A，则A类为Positive，B类和C类为Negative。然后得到A的混淆矩阵：</p><div class="table-container"><table><thead><tr><th style="text-align:center">原始\预测</th><th style="text-align:center">a</th><th style="text-align:center">非a</th></tr></thead><tbody><tr><td style="text-align:center"><strong>A</strong></td><td style="text-align:center">$TP_A$</td><td style="text-align:center">$FN_A$</td></tr><tr><td style="text-align:center"><strong>非A</strong></td><td style="text-align:center">$FP_A$</td><td style="text-align:center">$TN_A$</td></tr></tbody></table></div><h2 id="变量命名-personal"><a href="#变量命名-personal" class="headerlink" title="变量命名(personal)"></a>变量命名(personal)</h2><p>若以X为样本中X类数量，M为样本总数，x为预测结果中的X类数量，T为正确预测数量，F为错误预测数量，综上有：</p><ul><li>$M = A+B+C = a+b+c = T + F$（不解释）</li><li>$T = TP_A+TP_B+TP_C$（不解释）</li><li>$F = FP_A+FP_B+FP_C = FN_A+FN_B+FN_C$（不解释） </li><li>$A = TP_A+FN_A$（样本中的A类数量=真A类数量+假非A类数量）</li><li>$a = TP_A+FP_A$（预测中的A类数量=真A类数量+假A类数量）</li></ul><p>综合来看还是比较复杂的，实操时最好先把混淆矩阵算出来，然后再进行下面基本指标和高级指标的计算。</p><h2 id="基本指标"><a href="#基本指标" class="headerlink" title="基本指标"></a>基本指标</h2><ul><li><strong>accuracy</strong>: 模型做正确预测的能力 <script type="math/tex; mode=display">\frac{T}{M}</script></li><li><strong>recall/hit-rate/TruePositiveRate/sensitivity</strong>: 原始样本中的A类有多少被正确预测出来 <script type="math/tex; mode=display">\frac{TP_A}{A}</script></li><li><strong>precision</strong>: 找到的A类中有多少是真正的A类 <script type="math/tex; mode=display">\frac{TP_A}{a}</script></li><li><strong>fall-out/FalsePositiveRate</strong>: 原始样本中的非A类有多少被错误地预测成了A类 <script type="math/tex; mode=display">\frac{FN_A}{B+C}</script></li><li><strong>F1 score</strong>: precision和recall的调和平均数<script type="math/tex; mode=display">\frac{2}{\frac{1}{precision}+\frac{1}{recall}}</script></li></ul><p>综上可以看出，除了accuracy之外，其他4个指标都与类别强相关，这意味着对于每一类别都有这4个指标。那么如何综合不同类别的基本指标，从而得到对于一个模型而言的全局指标呢？常用以下三种方式：</p><ol><li>Macro-Average : 求各个类别指标的算数平均值作为全局指标。举个例子<script type="math/tex; mode=display">presicion = \frac{precision_A+precision_B+precision_C}{3}= \frac{1}{3}(\frac{TP_A}{a}+\frac{TP_B}{b}+\frac{TP_C}{c})</script></li><li>Micro-Average : 看下面例子就明白，此时全局precision和recall是相同的。<script type="math/tex; mode=display">precision = \frac{TP_A+TP_B+TP_C}{a+b+c} = \frac{TP_A+TP_B+TP_C}{M}</script></li><li>Weighted-Average : 求各个类别指标的加权平均值作为全局指标，权重为该类别在样本空间中的比例。<script type="math/tex; mode=display">precision = precision_A * \frac{A}{M} + precision_B * \frac{B}{M} +precision_C * \frac{C}{M}</script></li></ol><h2 id="高级指标"><a href="#高级指标" class="headerlink" title="高级指标"></a>高级指标</h2><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a><strong>ROC曲线</strong></h3><ul><li>横轴为FPR，样本中非A类多少被错误预测成了A类</li><li>纵轴为TPR，样本中A类多少被正确预测成了A类（也就是recalll）</li><li>0-1之间选择不同的分类阈值，从而得到同一分类器的不同subversion，因此ROC曲线上一个点(x,y)的含义为：对于某个特定的分类阈值，有y%的A类被成功预测成了A类，但代价是x%的非A类也被预测成了A类。</li><li>我们肯定希望x越小越好，y越大越好（所有的飞机都被找到，大雁及其他鸟类还不被当成飞机），从图形上来看就是尽量靠近左上角。</li><li><strong>ROC-AUC值</strong>，ROC曲线对x轴的积分，对于同样的x值，y值越大，面积越大，越符合我们的期望，也就是分类效果越好。</li></ul><h3 id="P-R曲线"><a href="#P-R曲线" class="headerlink" title="P-R曲线"></a><strong>P-R曲线</strong></h3><ul><li>横轴为recall（真A类占全体样本A类的比例）</li><li>纵轴为precision（真A类占全体预测A类的比例）</li><li>0-1之间选择不同的分类阈值，从而得到同一分类器的不同subversion，因此P-R曲线上一个点(x,y)的含义为：对于某个特定的分类阈值，有x%的A类被成功预测成了A类(被召回)，并且这些真A类占a的比例为y%。</li><li>我们希望x和y都越大越好，因为这意味着所有的飞机都被找到的同时，被认为是飞机的还真就都是飞机。从图形上来看就是尽量靠近右上角。</li><li><strong>PR-AUC值</strong>，即所谓的<strong>AP</strong>值，P-R曲线对x轴的积分，也是越大越好。</li><li><strong>mAP</strong>，计算出所有类别的<strong>AP</strong>值，然后计算算数平均数，即为整个模型的mAP值。</li></ul><h2 id="其他话题"><a href="#其他话题" class="headerlink" title="其他话题"></a>其他话题</h2><h3 id="mAP计算步骤"><a href="#mAP计算步骤" class="headerlink" title="mAP计算步骤"></a>mAP计算步骤</h3><ol><li>取得模型预测结果，一般为一个prediction score</li><li>选取一个threshold，将prediction score转化为PNTF</li><li>依次统计每个类别的混淆矩阵</li><li>根据每个类别的混淆矩阵计算出对应的precision和recall</li><li>更换threshold，重复2-4</li><li>所有的threshold完成后，即可得到每个类别各自的PR曲线，求AUC面积得到各个类别的AP值</li><li>求所有类别AP值的算数平均数，即可得到mAP</li></ol><h3 id="目标检测中的mAP计算"><a href="#目标检测中的mAP计算" class="headerlink" title="目标检测中的mAP计算"></a>目标检测中的mAP计算</h3><p>目标检测中不仅要对比原始标签和预测结果标签的偏离情况，还要计算Bounding Box的IOU值。在预测前后标签相同的前提下，将IOU值代替分类任务中的prediction score来进行后续计算。</p><h3 id="ROC曲线和PR曲线对比"><a href="#ROC曲线和PR曲线对比" class="headerlink" title="ROC曲线和PR曲线对比"></a><strong>ROC曲线和PR曲线对比</strong></h3><ol><li>选定一个分类阈值，那么此时的recall是相同的，那么对于达到这个recall效果:<ul><li>ROC曲线中衡量的是FPR（FP/N），也就是样本中的这么多大雁有多少被错当成了飞机</li><li>PR曲线中衡量的是precision，也就是预测中的这么多飞机有多少真的是飞机（或者说有多少其实是大雁）</li></ul></li><li>如果非A类样本增加：<ul><li>预测结果中，由于分类模型没有变，所以将B/C类错误地预测为A类的数量和B/C类正确地预测为B/C必然会同时增加，因此FPR的分子分母同时变大，总体变化不会很大</li><li>预测结果中，真A类数量不会变化，但是假A类数量必然增加，precision总体会变小</li></ul></li><li>如果A类样本增加：<ul><li>B/C类数量不会发生变化，假A类数量也不会发生变化，因此FPR不变 假A类数量不变，真A类数量必然增加，precision总体会变大</li><li>综上所述，我们说，样本的分布情况会影响PR曲线形状，但（基本）不会影响ROC曲线形状；也因此，样本分布极不均匀时，优先选择PR曲线评估分类模型的性能（ROC曲线特喵的不受样本分布影响啊）</li><li>归根结底，还是因为precision这个指标受样本分布影响太大，如果样本中A类占巨大多数，那么对A类进行预测的precision很容易显得很高，vice versa。</li></ul></li></ol><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>[1] <a href="https://zhuanlan.zhihu.com/p/147663370">https://zhuanlan.zhihu.com/p/147663370</a><br>[2] <a href="https://blog.paperspace.com/mean-average-precision/">https://blog.paperspace.com/mean-average-precision/</a></p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ROC曲线</tag>
      
      <tag>PR曲线</tag>
      
      <tag>mAP</tag>
      
      <tag>mmAP</tag>
      
      <tag>recall</tag>
      
      <tag>precision</tag>
      
      <tag>AUC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AlexNet</title>
    <link href="/2021/07/31/AlexNet/"/>
    <url>/2021/07/31/AlexNet/</url>
    
    <content type="html"><![CDATA[<h2 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h2><p>Hinton学生Alex之作，2012年的ISLVRC冠军，深度学习方法甩开传统图像算法的开端，GPU训练的开端。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><div align=center><img title="" src="/img/net/AlexNet_model.pth.png" width="100%" height="100%" align=center></div><ol><li><p><strong>原始输入</strong>：尺寸为[224, 224, 3]的三通道RGB彩色图像</p></li><li><p><strong>特征提取层</strong>：上图中的第一行，(conv, relu, max_pool)经典三元结构x2 + (conv, relu)经典二元结构x3 + max_pool</p></li><li><p><strong>分类层</strong>：将特征提取层的结果flatten之后，接上2个FC层和1个softmax层，其中这两个FC层之前都用了dropout机制。</p></li></ol><h2 id="网络特点"><a href="#网络特点" class="headerlink" title="网络特点"></a>网络特点</h2><ul><li>首用RELU代替以往的sigmoid和tanh(后面补充上各种激活函数的详情文章链接)</li><li>首用LRN局部相应归一化（有时间这里展开讲一下，目前看不是很重要。。）</li><li>首用DROPOUT机制进行正则，具体的实现原理如下：<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">d = np.<span class="hljs-built_in">random</span>.rand( <span class="hljs-keyword">a</span>.shape[<span class="hljs-number">1</span>], <span class="hljs-keyword">a</span>.shape[<span class="hljs-number">1</span>] ) &lt; keep_prob<br><span class="hljs-keyword">a</span> = <span class="hljs-keyword">a</span> * d / keep_prob<br></code></pre></td></tr></table></figure><ol><li>对于一个需要进行dropout的网络层，在获得这一层的激活值a之后，输入到下一层网络之前，用一个随机生成的dropout mask矩阵和a进行逐元素乘法。当然，两者形状一致。</li><li>dropout mask矩阵一般是用rand函数产生的一个和a同尺寸的矩阵，每个元素的值为0-1之间的随机数。若某个元素小于keep_prob则得到1，大于则得到0。这解释了keep_prob的名称的由来，有比例为keep_prob的元素未被置0。</li><li>最后在将a输入到下一层之前，还要恢复a的数学期望，即代码中除以keep_prob的操作。这样就只需要在train过程考虑dropout，而在test时直接关闭dropout即可(对应pytorch中模型eval和train模式的设定)</li></ol></li></ul><h2 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h2><p>网络结构图中为了减少参数量，加快训练过程，所有网络层的尺寸都设为了原作的一半。这不是重点。</p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>经典网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>神经网络</tag>
      
      <tag>经典网络</tag>
      
      <tag>AlexNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python列表的切片</title>
    <link href="/2021/07/29/python%E5%88%97%E8%A1%A8%E7%9A%84%E5%88%87%E7%89%87/"/>
    <url>/2021/07/29/python%E5%88%97%E8%A1%A8%E7%9A%84%E5%88%87%E7%89%87/</url>
    
    <content type="html"><![CDATA[<p>普通索引<code>a[i]</code>返回序列对象的一个元素，切片<code>a[::1]</code>则返回一些元素。</p><h2 id="基本索引"><a href="#基本索引" class="headerlink" title="基本索引"></a>基本索引</h2><div class="table-container"><table><thead><tr><th style="text-align:center">a中元素</th><th style="text-align:center">2</th><th style="text-align:center">4</th><th style="text-align:center">6</th><th style="text-align:center">8</th></tr></thead><tbody><tr><td style="text-align:center">非负索引</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">负数索引</td><td style="text-align:center">-4</td><td style="text-align:center">-3</td><td style="text-align:center">-2</td><td style="text-align:center">-1</td></tr></tbody></table></div><blockquote><p>非负索引从0开始，负数索引从-1开始<br>基本索引不可越界</p></blockquote><h2 id="简单切片"><a href="#简单切片" class="headerlink" title="简单切片"></a>简单切片</h2><p><code>a[start:stop]</code>返回基本索引范围在[start,stop)内的元素</p><ol><li>start/stop越界：此时有多少算多少，不报错</li><li>start &gt; stop：返回空即可</li><li>start/stop缺省：start默认无穷小，stop默认无穷大</li></ol><h2 id="扩展切片"><a href="#扩展切片" class="headerlink" title="扩展切片"></a>扩展切片</h2><h4 id="step为正数，从前往后选取"><a href="#step为正数，从前往后选取" class="headerlink" title="step为正数，从前往后选取"></a>step为正数，从前往后选取</h4><p><code>a[start:stop:step]</code>在[start,stop)区间内，依次选取索引为start, start + step, start + 2step…..直到索引值不在[start, stop)区间中</p><ol><li>越界：同样有多少算多少，不报错</li><li>start &gt; stop：返回空</li><li>缺省：同上</li></ol><h4 id="step为负数，从后向前选取"><a href="#step为负数，从后向前选取" class="headerlink" title="step为负数，从后向前选取"></a>step为负数，从后向前选取</h4><p>此时，start是stop，stop是start，而且还是左开右闭，比如<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">a</span>[<span class="hljs-number">2</span>:<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>] -&gt;<span class="hljs-meta"> [6, 4]</span><br><br><span class="hljs-attribute">a</span>[:<span class="hljs-number">1</span>:-<span class="hljs-number">2</span>] -&gt;<span class="hljs-meta"> [8]</span><br></code></pre></td></tr></table></figure></p><h2 id="多维数组切片"><a href="#多维数组切片" class="headerlink" title="多维数组切片"></a>多维数组切片</h2><p>numpy和tensor这样的数据类型相比于原生的list提供了多维数组切片方法，示例如下：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">pad_img<span class="hljs-selector-attr">[: img.shape[0]</span>, : <span class="hljs-selector-tag">img</span><span class="hljs-selector-class">.shape</span><span class="hljs-selector-attr">[1]</span>, : <span class="hljs-selector-tag">img</span><span class="hljs-selector-class">.shape</span><span class="hljs-selector-attr">[2]</span>]<span class="hljs-selector-class">.copy_</span>(img)<br></code></pre></td></tr></table></figure><br>两个逗号隔开三个索引表达式，分别对应这个图像/tensor/numpy数组的3个维度，各个索引表达式在各自维度上的切片规则同上，得到各个维度上的索引列表后对3个列表的元素进行全排列，即可得到最终的三维索引(x, y, z)。这个例子的含义是把img中的像素值复制到pad_img中，左上角对齐。</p><p>研究SSD的源码时又发现一个tensor的切片方式：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">bboxes = <span class="hljs-selector-attr">[[0, 1, 2]</span>, <span class="hljs-selector-attr">[3, 4, 5]</span>, <span class="hljs-selector-attr">[6, 7, 8]</span>]<br><br><span class="hljs-selector-tag">a</span> = bboxes<span class="hljs-selector-attr">[[True, False, True]</span>, <span class="hljs-number">0</span>]<br><span class="hljs-selector-tag">b</span> = bboxes<span class="hljs-selector-attr">[[1, 0, 1]</span>, <span class="hljs-number">0</span>]<br><br>&gt;&gt;&gt; <span class="hljs-selector-tag">a</span> = <span class="hljs-selector-attr">[0, 6]</span><br>&gt;&gt;&gt; <span class="hljs-selector-tag">b</span> = <span class="hljs-selector-attr">[3, 0, 3]</span><br></code></pre></td></tr></table></figure><br>基本的规则还是跟第一个例子一样，先获取各个维度的索引列表，然后进行全排列，得到最终的二维索引(x, y)。但是第一个维度上的索引列表是一个bool类型元素的列表，当取Fasle时，不参与索引。</p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>python笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>切片</tag>
      
      <tag>花式索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python函数参数的打包与拆解</title>
    <link href="/2021/07/29/python%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%E7%9A%84%E6%89%93%E5%8C%85%E4%B8%8E%E6%8B%86%E8%A7%A3/"/>
    <url>/2021/07/29/python%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%E7%9A%84%E6%89%93%E5%8C%85%E4%B8%8E%E6%8B%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p><strong>标志</strong>：单/双星号出现在入参或形参之前<br><strong>原则</strong>：</p><ol><li>入参前加星号代表拆解，形参前加星号代表打包</li><li>list/tuple只有一种拆解方式，dictionary有两种</li></ol><h2 id="打包情形1：单星号出现在形参前"><a href="#打包情形1：单星号出现在形参前" class="headerlink" title="打包情形1：单星号出现在形参前"></a>打包情形1：单星号出现在形参前</h2><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">def</span> pack(a, *b):<br>        print <span class="hljs-keyword">type</span>(a), a<br>        print <span class="hljs-keyword">type</span>(b), b<br><br><span class="hljs-title">pack</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;int&#x27;&gt; 1<br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;tuple&#x27;&gt; (2, 3, 4, 5)<br></code></pre></td></tr></table></figure><p>单星号打包，按照匹配顺序认领多个非关键字入参，打包称一个tuple。</p><h2 id="打包情形2：双星号出现在形参前"><a href="#打包情形2：双星号出现在形参前" class="headerlink" title="打包情形2：双星号出现在形参前"></a>打包情形2：双星号出现在形参前</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def pack(a, *<span class="hljs-number">*b</span>):<br>        <span class="hljs-builtin-name">print</span> type(a), a<br>        <span class="hljs-builtin-name">print</span> type(b), b<br> <br>pack(1, <span class="hljs-attribute">a1</span>=2, <span class="hljs-attribute">a2</span>=3)<br><br>&gt;&gt; &lt;type <span class="hljs-string">&#x27;int&#x27;</span>&gt; 1<br>&gt;&gt; &lt;type <span class="hljs-string">&#x27;dict&#x27;</span>&gt; &#123;<span class="hljs-string">&#x27;a1&#x27;</span>: 2, <span class="hljs-string">&#x27;a2&#x27;</span>: 3&#125;<br></code></pre></td></tr></table></figure><p>双星号打包，按照匹配顺序认领多个关键字入参，打包成一个dictionary</p><h2 id="拆解情形1：单星号出现在入参前"><a href="#拆解情形1：单星号出现在入参前" class="headerlink" title="拆解情形1：单星号出现在入参前"></a>拆解情形1：单星号出现在入参前</h2><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">def</span> pack(a, b, *c):<br>        print <span class="hljs-keyword">type</span>(a), a<br>        print <span class="hljs-keyword">type</span>(b), b<br>        print <span class="hljs-keyword">type</span>(c), c<br> <br><span class="hljs-title">score</span> = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>]<br><span class="hljs-title">pack</span>(*score)<br><br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;float&#x27;&gt; 1.0<br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;float&#x27;&gt; 2.0<br>&gt;&gt; &lt;<span class="hljs-keyword">type</span> &#x27;tuple&#x27;&gt; (3.0, 4.0)<br></code></pre></td></tr></table></figure><p>单星号拆解，将list/tuple/dictionary的内容当做独立非关键字的入参列表，其中dictionary只保留key</p><h2 id="拆解情形2：双星号出现在入参前"><a href="#拆解情形2：双星号出现在入参前" class="headerlink" title="拆解情形2：双星号出现在入参前"></a>拆解情形2：双星号出现在入参前</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pack</span>(<span class="hljs-params">*a, **b</span>):</span><br>        <span class="hljs-built_in">print</span> <span class="hljs-built_in">type</span>(a), a<br>        <span class="hljs-built_in">print</span> <span class="hljs-built_in">type</span>(b), b<br> <br>age = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>student = &#123;<span class="hljs-string">&#x27;score&#x27;</span> : <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;id&#x27;</span> : <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;name&#x27;</span> : <span class="hljs-string">&#x27;xiaoxiao&#x27;</span>&#125;<br>pack(*age, **student)<br><br>&gt;&gt; &lt;<span class="hljs-built_in">type</span> <span class="hljs-string">&#x27;tuple&#x27;</span>&gt; (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>&gt;&gt; &lt;<span class="hljs-built_in">type</span> <span class="hljs-string">&#x27;dict&#x27;</span>&gt; &#123;<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;xiaoxiao&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>双星号拆解，只针对dictionary，将其内容拆解为关键字入参列表。</p>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>python笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nvidia-smi突然跪了！</title>
    <link href="/2021/07/26/nvidia-smi%E7%AA%81%E7%84%B6%E8%B7%AA%E4%BA%86%EF%BC%81/"/>
    <url>/2021/07/26/nvidia-smi%E7%AA%81%E7%84%B6%E8%B7%AA%E4%BA%86%EF%BC%81/</url>
    
    <content type="html"><![CDATA[<h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>看着代码唱着歌，CUDA就突然啥啥啥初始化失败了。</p><p>nvidia-smi一看，又是一个新报错Failed to initialize NVML: Driver/library version mismatch，闹心。</p><p>整半天，大概意思是作为内核模块的nvidia驱动程序版本和libnvidia-compute-460这个库的版本不一致。</p><p>两者一般都应该是安装驱动的时候指定的版本（for me is 460.84），但是后者竟然通过ubuntu的后台更新程序自动更新了（460.91），所以造成了这个问题。</p><p>最后，更新了驱动程序，关闭了自动更新，reboot，告一段落。</p><h2 id="用到的命令："><a href="#用到的命令：" class="headerlink" title="用到的命令："></a>用到的命令：</h2><blockquote><p>lsmod | grep nvidia*<br>显示和nvidia有关的内核模块有哪些</p><p>modinfo nvidiaxxx | grep version<br>显示这些内核模块的版本</p><p>dpkg —list | grep nvidia<br>显示libnvidia-compute这个库的版本</p><p>dmesg | tail -4<br>nvidia-smi的报错详情，可以看到版本差异<br>dmesg | grep nvidia<br>nvidia-smi的执行过程详情</p><p>cat /var/log/apt/history.log | grep -a -C 10 nvidia<br>libnvidia-compute的更新记录，可以看到是unattended-upgrade造成的更新</p><p>cat /etc/apt/apt.conf.d/20auto-upgrades<br>自动更新的配置文件，修改即可关闭自动更新<a href="https://unix.stackexchange.com/questions/342663/how-is-unattended-upgrades-started-and-how-can-i-modify-its-schedule">detail</a></p><p>sudo apt install nvidia-driver-460<br>更新驱动程序</p></blockquote><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://stackoverflow.com/questions/62250491/nvml-driver-library-mismatch-after-libnvidia-compute-update">https://stackoverflow.com/questions/62250491/nvml-driver-library-mismatch-after-libnvidia-compute-update</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>生活记录</category>
      
      <category>电脑设置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Nvidia</tag>
      
      <tag>驱动失效</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python笔记</title>
    <link href="/2021/07/22/python%E7%AC%94%E8%AE%B0/"/>
    <url>/2021/07/22/python%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="python的zip函数"><a href="#python的zip函数" class="headerlink" title="python的zip函数"></a>python的zip函数</h3><p>输入若干个Iterable对象，将这些Iterable对象对应位置的元素打包为一个tuple，然后将所有tuple作为一个zip对象返回。</p><p>当传入的Iterable对象长度不一致时取最短的。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">a</span> = <span class="hljs-selector-attr">[[1,2]</span>,<span class="hljs-selector-attr">[3,4]</span>,<span class="hljs-selector-attr">[5,6]</span>]<br><span class="hljs-selector-tag">b</span> = zip(*a)<br>c = tuple(b)<br><br>&gt;&gt;&gt; <span class="hljs-selector-tag">a</span> = ((<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>))<br></code></pre></td></tr></table></figure></p><h3 id="python-string的split方法"><a href="#python-string的split方法" class="headerlink" title="python string的split方法"></a>python string的split方法</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">a</span> = <span class="hljs-string">&quot;, my, name, is, ming&quot;</span><br><span class="hljs-selector-tag">b</span> = <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.split</span>(<span class="hljs-string">&#x27;,&#x27;</span>)<br><br>&gt;&gt;&gt; <span class="hljs-selector-tag">b</span> = <span class="hljs-selector-attr">[<span class="hljs-string">&#x27;&#x27;</span>,<span class="hljs-string">&#x27; my&#x27;</span>,<span class="hljs-string">&#x27; name&#x27;</span>,<span class="hljs-string">&#x27; is&#x27;</span>,<span class="hljs-string">&#x27; ming&#x27;</span> ]</span><br></code></pre></td></tr></table></figure><p>即，将原string按照字符串分割，不包含分隔符，返回一个列表。</p><h3 id="pyhton-函数装饰器"><a href="#pyhton-函数装饰器" class="headerlink" title="pyhton 函数装饰器"></a>pyhton 函数装饰器</h3><p>对被修饰的函数增加一层封装，被修饰的函数仍然正常被调用，被执行，但是增加了一些“私货”，比如打log。这么做的好处是在不修改原函数代码以及调用原函数的代码的前提下改变其行为。<br><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> logging<br><br>def use_logging(func):<br>def <span class="hljs-keyword">wrapper</span>(*args, **kwargs):<br>logging.warn(&quot;%s is running&quot; % func.__name__)<br><span class="hljs-keyword">return</span> func(*args)<br><span class="hljs-keyword">return</span> <span class="hljs-keyword">wrapper</span><br><br>@use_logging<br>def foo():<br>print(&quot;i am foo&quot;)<br><br>@use_logging<br>def bar():<br>print(&quot;i am bar&quot;)<br><br>foo()<br>bar()<br><br>&gt;&gt;&gt; i am foo<br>&gt;&gt;&gt; i am bar<br>&gt;&gt;&gt; <span class="hljs-built_in">WARNING</span>:root:foo <span class="hljs-keyword">is</span> running<br>&gt;&gt;&gt; <span class="hljs-built_in">WARNING</span>:root:bar <span class="hljs-keyword">is</span> running<br></code></pre></td></tr></table></figure></p><h3 id="python-赋值-amp-浅拷贝-amp-深拷贝的区别"><a href="#python-赋值-amp-浅拷贝-amp-深拷贝的区别" class="headerlink" title="python 赋值&amp;浅拷贝&amp;深拷贝的区别"></a>python 赋值&amp;浅拷贝&amp;深拷贝的区别</h3><ol><li>对于不可变对象，三者同质，都只增加原对象的引用计数</li><li>对于可变对象<ul><li>赋值，仍然是引用计数</li><li>浅拷贝，新瓶装旧酒，瓶是新的，酒还是原对象元素的计数引用</li><li>深拷贝，实质意义上的拷贝，啥都重新搞一份</li></ul></li></ol><h3 id="python-init-py文件的作用"><a href="#python-init-py文件的作用" class="headerlink" title="python init.py文件的作用"></a>python <strong>init</strong>.py文件的作用</h3><p>一个包含了<strong>init</strong>.py目录可以被识别为一个包，而目录内的.py文件是这个包的模块。一个包也是一个模块，反之则不一定。</p><p>在import一个包时，实际上是在执行这个包的<strong>init</strong>.py中的代码。看一个例子，</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">.<br>├── <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span><br>└── mypackage<br>    ├── __init__<span class="hljs-selector-class">.py</span><br>    ├── subpackage_1<br>    │   ├── test11<span class="hljs-selector-class">.py</span><br>    │   └── test12<span class="hljs-selector-class">.py</span><br>    └── subpackage_2<br>        ├── __init__<span class="hljs-selector-class">.py</span><br>        ├── test21<span class="hljs-selector-class">.py</span><br>        └── test22.py<br></code></pre></td></tr></table></figure><p>在main.py中<code>import mypackage</code>时，python后台程序会按照以下顺序搜索<code>mypackage</code></p><ol><li>当前的工作目录；</li><li>PYTHONPATH（环境变量）中的每一个目录；</li><li>Python 默认的安装目录。</li></ol><p>本例中，在当前工作目录下即可找到<code>mypackage</code>，然后会执行其下<strong>init</strong>.py中的代码。</p><p>对于mypackage的<strong>init</strong>.py，值得注意的是：<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> subpackage_1# 错误，因为在main的当前目录找不到subpackage_1<br><span class="hljs-keyword">import</span> subpackage_2# 错误<br><span class="hljs-keyword">import</span> mypackage.subpackage_1# 正确，先找到mypackage再找到subpackage_1<br><span class="hljs-keyword">import</span> mypackage.subpackage_2# 正确<br><span class="hljs-keyword">from</span> . <span class="hljs-keyword">import</span> subpackage_1# 正确，在<span class="hljs-keyword">from</span>指定的目录可以找到subpackage_1<br><span class="hljs-keyword">from</span> . <span class="hljs-keyword">import</span> subpackage_2# 正确<br><span class="hljs-keyword">from</span> mypackage <span class="hljs-keyword">import</span> subpackage_1 # 正确，同上<br><span class="hljs-keyword">from</span> mypackage <span class="hljs-keyword">import</span> subpackage_2<br><br><span class="hljs-keyword">from</span> mypackage <span class="hljs-keyword">import</span> test11# 错误<br><span class="hljs-keyword">from</span> mypackage.subpackage_1 <span class="hljs-keyword">import</span> test11# 正确<br><span class="hljs-keyword">from</span> .subpackage_1 <span class="hljs-keyword">import</span> test11# 正确，语义同上<br><span class="hljs-keyword">from</span> mypackage <span class="hljs-keyword">import</span> test21<br><span class="hljs-keyword">from</span> mypackage.subpackage_2 <span class="hljs-keyword">import</span> test21<br><span class="hljs-keyword">from</span> .subpackage_2 <span class="hljs-keyword">import</span> test21<br></code></pre></td></tr></table></figure></p><h3 id="itertools库"><a href="#itertools库" class="headerlink" title="itertools库"></a><code>itertools</code>库</h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs fortran">v1 = [(i, j) for i, j <span class="hljs-keyword">in</span> itertools.<span class="hljs-built_in">product</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>), <span class="hljs-built_in">repeat</span>=<span class="hljs-number">2</span>)]<br>v2 = [(i, j) for i, j <span class="hljs-keyword">in</span> itertools.<span class="hljs-built_in">product</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>), <span class="hljs-built_in">repeat</span>=<span class="hljs-number">3</span>)]<br><br>&gt;&gt;&gt; v1 = [(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)]<br>&gt;&gt;&gt; v2 = [(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)]<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>python笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>split</tag>
      
      <tag>函数装饰器</tag>
      
      <tag>深拷贝</tag>
      
      <tag>浅拷贝</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch笔记</title>
    <link href="/2021/07/22/pytorch%E7%AC%94%E8%AE%B0/"/>
    <url>/2021/07/22/pytorch%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="max-sum-等操作中指定dim的含义"><a href="#max-sum-等操作中指定dim的含义" class="headerlink" title="max() sum()等操作中指定dim的含义"></a><code>max() sum()</code>等操作中指定dim的含义</h3><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">a = <span class="hljs-comment">[ <span class="hljs-comment">[ <span class="hljs-comment">[1, 2]</span>, <span class="hljs-comment">[3, 4]</span> ]</span>, <span class="hljs-comment">[ <span class="hljs-comment">[5, 6]</span>, <span class="hljs-comment">[7, 8]</span> ]</span> ]</span><br></code></pre></td></tr></table></figure><ol><li>对于这个[2, 2, 2]的三维矩阵，三维意味着[]有三层，三个2表示每一层[]内有两个元素</li><li>对dim=0求max意味着<ol><li>将<script type="math/tex">a_{000}</script>与<script type="math/tex">a_{100}</script>比较得出最大值作为<script type="math/tex">a_{00}</script></li><li>将<script type="math/tex">a_{010}</script>与<script type="math/tex">a_{110}</script>比较得出最大值作为<script type="math/tex">a_{10}</script>…</li></ol></li><li>对dim=1求max意味着<ol><li>将<script type="math/tex">a_{000}</script>与<script type="math/tex">a_{010}</script>比较得出最大值作为<script type="math/tex">a_{00}</script></li><li>将<script type="math/tex">a_{100}</script>与<script type="math/tex">a_{110}</script>比较得出最大值作为<script type="math/tex">a_{10}</script>…</li></ol></li><li>而$a_{010}$对应于，第一层[]中的第一个元素的，第二个元素的，第一个元素</li></ol><h3 id="torch-flatten-中指定dim的含义"><a href="#torch-flatten-中指定dim的含义" class="headerlink" title="torch.flatten()中指定dim的含义"></a><code>torch.flatten()</code>中指定dim的含义</h3><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs less"># 默认情况，消除<span class="hljs-selector-tag">x</span>第<span class="hljs-selector-tag">1</span>层<span class="hljs-selector-attr">[]</span>内的所有<span class="hljs-selector-attr">[]</span><br><span class="hljs-selector-tag">torch</span><span class="hljs-selector-class">.flatten</span>(x)<br><br># 指定<span class="hljs-selector-tag">start_dim</span>,从<span class="hljs-selector-tag">x</span>的第<span class="hljs-selector-tag">s</span>+<span class="hljs-selector-tag">1</span>层<span class="hljs-selector-attr">[]</span>开始，消除其中的所有<span class="hljs-selector-attr">[]</span><br><span class="hljs-selector-tag">torch</span><span class="hljs-selector-class">.flatten</span>(x, <span class="hljs-number">1</span>)<br><br># 指定<span class="hljs-selector-tag">start_dim</span>和<span class="hljs-selector-tag">end_dim</span>，将第<span class="hljs-selector-tag">e</span>+<span class="hljs-selector-tag">1</span>层<span class="hljs-selector-attr">[]</span>包围的元素作为一个基础元素，然后从第<span class="hljs-selector-tag">s</span>+<span class="hljs-selector-tag">1</span>层开始消除所有基础元素外的<span class="hljs-selector-attr">[]</span><br><span class="hljs-selector-tag">torch</span><span class="hljs-selector-class">.flatten</span>(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><br># 综上，其实就是<span class="hljs-selector-tag">start_dim</span>默认为<span class="hljs-selector-tag">0</span>，<span class="hljs-selector-tag">end_dim</span>默认为<span class="hljs-selector-tag">-1</span><br></code></pre></td></tr></table></figure><h3 id="torch-cat-x-y-d-中指定dim的含义"><a href="#torch-cat-x-y-d-中指定dim的含义" class="headerlink" title="torch.cat((x,y),d)中指定dim的含义"></a><code>torch.cat((x,y),d)</code>中指定dim的含义</h3><ol><li>第一种解释，d=0就是按行堆叠x和y，堆叠后有了更多的行。然后是按列，按深度，以此类推。容易理解，适用于3维以下。</li><li>第二种解释，就是扩展x第d+1层[]中的内容，拿什么来扩展呢，拿y对应位置的元素，比较难理解，但适用性更广。</li></ol><h3 id="nn-MaxPool2d-3-stride-2-ceil-mode-True-中ceil-mode的含义"><a href="#nn-MaxPool2d-3-stride-2-ceil-mode-True-中ceil-mode的含义" class="headerlink" title="nn.MaxPool2d(3, stride=2, ceil_mode=True)中ceil_mode的含义"></a><code>nn.MaxPool2d(3, stride=2, ceil_mode=True)</code>中ceil_mode的含义</h3><p>其实就是尺寸计算公式中是向上取整还是向下取整，默认向下取整，开了ceil_mode之后就向上取整。</p><h3 id="nn-AvgPool2d-kernel-size-5-stride-3-与nn-AdaptiveAvgPool2d-1-1-的区别"><a href="#nn-AvgPool2d-kernel-size-5-stride-3-与nn-AdaptiveAvgPool2d-1-1-的区别" class="headerlink" title="nn.AvgPool2d(kernel_size=5, stride=3)与nn.AdaptiveAvgPool2d((1, 1))的区别"></a><code>nn.AvgPool2d(kernel_size=5, stride=3)</code>与<code>nn.AdaptiveAvgPool2d((1, 1))</code>的区别</h3><p>前者用指定的kernel_size和stride定义池化操作，后者则通过指定输出tensor的尺寸来定义池化操作，stride和kernel_size会自动算出来。</p><blockquote><p>In average-pooling or max-pooling, you essentially set the stride and kernel-size by your own, setting them as hyper-parameters. You will have to re-configure them if you happen to change your input size.<br>In Adaptive Pooling on the other hand, we specify the output size instead. And the stride and kernel-size are automatically selected to adapt to the needs. The following equations are used to calculate the value in the source code.<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-attr">Stride</span> = (input_size//output_size)  <br>Kernel <span class="hljs-attr">size</span> = input_size - (output_size-<span class="hljs-number">1</span>)*stride  <br><span class="hljs-attr">Padding</span> = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></p></blockquote><h3 id="plt-imshow-与plt-show-的区别"><a href="#plt-imshow-与plt-show-的区别" class="headerlink" title="plt.imshow()与plt.show()的区别"></a><code>plt.imshow()</code>与<code>plt.show()</code>的区别</h3><p><code>plt.imshow()</code>只是将图片与plt发生关联，但不显示，还可以继续对这张图片进行其他draw操作，最后再用<code>plt.show()</code>显示出来。</p><h3 id="dataset-dataset-loader-dataset-loader-iter的区别"><a href="#dataset-dataset-loader-dataset-loader-iter的区别" class="headerlink" title="dataset dataset_loader dataset_loader_iter的区别"></a><code>dataset</code> <code>dataset_loader</code> <code>dataset_loader_iter</code>的区别</h3><ul><li><code>train_set</code>不是Iterator，也不是Iterabel;</li><li><code>train_loader</code>不是Iterator，但是Iterable;</li><li>Iterable意味着可用于for循环被迭代, Iterator则是算法，不断用next产生需要的数据，Iterator必然Iterable</li></ul><h3 id="model的常用方法"><a href="#model的常用方法" class="headerlink" title="model的常用方法"></a>model的常用方法</h3><p>一般一个model都是由一串<code>nn.Module</code>的派生类组合而成的class，而且其本身一般也是一个<code>nn.Module</code>的派生类。因为各个相关对象都来自一个公共基类，因此常用的成员函数也大致相同：</p><ul><li><code>model.modules()</code> &amp; <code>model.named_modules()</code></li><li><code>model.children()</code> &amp; <code>model.named_children()</code></li><li><code>model.parameters()</code> &amp; <code>model.named_parameters()</code></li><li><code>model.buffers()</code> &amp; <code>model.named_buffers()</code></li></ul><p>这些方法都是返回一个generator，迭代它可以得到相应的元素。</p><p>其中，module和children返回的是一个<code>nn.Module</code>的派生类对象，不同点是前者递归式地返回model所有后代(而不只是叶子节点哦)，后者返回其直接后代。</p><p>parameters返回的是一个Parameter对象，暂时可以理解为是一个tensor。</p><p>buffers返回的是一个tensor，保存过程数据running_mean、running_var等，具体含义暂时未知。一般只有作为叶子节点的module才有parameter和buffers。</p><p>另外就是带name的版本，相比与不带name的版本，迭代generator得到的是一个包含两个元素的tuple，第一个元素是name，第二个元素跟不带name的版本一样。</p><p>各个方法返回的元素的命名都是基于module的名称，以InvertedResidual模块为例，具体的命名逻辑如下图所示：</p><div align=center><img title="" src="/img/article/pytorch_module_name.png" width="90%" height="90%" align=center></div>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
      <category>pytorch使用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>plt</tag>
      
      <tag>AaptivePool</tag>
      
      <tag>flatten</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CNN的数学原理</title>
    <link href="/2021/07/15/CNN%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
    <url>/2021/07/15/CNN%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="二维卷积的数学原理"><a href="#二维卷积的数学原理" class="headerlink" title="二维卷积的数学原理"></a>二维卷积的数学原理</h2><p>不管信号处理中“卷积”和“相关”的定义如何，至少在CNN中卷积的含义是明确的。就是</p><ol><li>将卷积核(一个尺寸较小的方阵)与矩阵$\boldsymbol{A}$左上角对齐</li><li>两个矩阵逐元素相乘后求和，作为结果矩阵$\boldsymbol{B}$的左上角第一个元素</li><li>向右/下移动卷积核的位置，重复2，直到卷积核到达矩阵$\boldsymbol{A}$的右下角<div align=center><img title="CNN中卷积的数学含义" src="/img/article/juanji.gif" width="60%" height="60%" align=center></div></li></ol><p>由此带来了<strong>padding</strong>和<strong>stride</strong>的概念：</p><ul><li><strong>padding</strong>：为了解决卷积带来的尺寸缩减，在卷积之前在$\boldsymbol{A}$周围补上p圈0。</li><li><strong>stride</strong>：卷积核每次向右/下移动s个元素。</li></ul><p>综上，有以下公式计算卷积前后的尺寸变化：</p><blockquote><p>$(n,n) and (f,f) \Longrightarrow \lfloor \frac{n+2p-f}{s}+1 \rfloor$</p></blockquote><h2 id="多维度卷积的数学原理"><a href="#多维度卷积的数学原理" class="headerlink" title="多维度卷积的数学原理"></a>多维度卷积的数学原理</h2><p><strong>关键字</strong>：<strong>卷积</strong>-&gt;<strong>偏移</strong>-&gt;<strong>激活</strong></p><p>CNN模型的输入——RGB三通道图像可以看做一个三层的2D矩阵，对其卷积的含义是：</p><ol><li>将一个层数相同的卷积核与这个3层的2D矩阵左上角对齐，然后做逐元素的乘积求和，并不断移动到右下角为止</li><li>由于求和是跨通道的，因此卷积的结果是一个单层的2D矩阵。</li><li>然后，仿照DNN对这个单层的2D矩阵中的每一个元素进行偏移和激活，得到作为输出的单层的2D矩阵。</li><li>选取另一个同尺寸的卷积核重复1-3，得到另一个单层的2D矩阵</li><li>将输入量与本层所有卷积核的卷积结果堆叠后，得到一个新的多层2D矩阵，作为本层的输出。</li><li>经过若干个卷积层处理后，将最后一个卷积层输出的多层2D矩阵看做一个列向量，将这个列向量作为$\boldsymbol{a}^{[l-1]}$，送入输出层处理。</li><li>若是二分类，则输出层是一个DNN神经元；若是多分类，则输出层是一个以$softmax$为激活函数的DNN网络层。此处DNN的含义指，通过矩阵乘法产生$\boldsymbol{z}^{[l]}$，而不是卷积操作。</li></ol><blockquote><ul><li>显然输出的层数等于本层卷积核的数量，而输出的每一层的尺寸则由前文提及的公式计算得到。</li><li>与DNN不同这里的权重矩阵$\boldsymbol{W}$的维度，都是人为指定的。这也正式CNN存在的理由，减少参数。</li></ul></blockquote><h2 id="池化的数学原理"><a href="#池化的数学原理" class="headerlink" title="池化的数学原理"></a>池化的数学原理</h2><p>与卷积操作的过程相同，但是每个核与输入矩阵进行的不是卷积操作，而是选取区域最大值的操作（最大池化）或者计算区域平均值的操作（平均池化）。一般用于降维。最大池化比较常用，并且一般伴随着0 padding。</p><h2 id="CNN目标检测的基本原理"><a href="#CNN目标检测的基本原理" class="headerlink" title="CNN目标检测的基本原理"></a>CNN目标检测的基本原理</h2><h3 id="单图片，单目标"><a href="#单图片，单目标" class="headerlink" title="单图片，单目标"></a>单图片，单目标</h3><p>先说最简单的情形，即判断一张图片中有没有猫的问题。只需要将图片送入CNN，然后将模型输出向量定义为如下形式即可（假设待检测目标有3类）</p><blockquote><p>$\hat{\boldsymbol{y}} = (p,x,y,w,h,c_1,c_2,c_3)^T$</p></blockquote><p>其中</p><ul><li>$p$为这个result向量，或者说这个检测框存在目标的概率</li><li>$x,y,w,h$为这个检测框的在输入图片中的位置</li><li>$c_1, c_2, c_3$为该检测框属于各个类别的概率</li></ul><h3 id="单图片，多目标"><a href="#单图片，多目标" class="headerlink" title="单图片，多目标"></a>单图片，多目标</h3><p>如果一张图片存在多个目标——这也是实际情况中更可能发生的情况，事情就变得复杂起来。最直观并且容易理解的思路是<strong>滑动窗口搜索法</strong>：<br></p><ol><li>定义一个尺寸的框，将输入图片的左上角与这个框对齐</li><li>将被框框住的部分作为“单图片，单目标”的情形处理，问题解决</li><li>然后移动框的位置，不断向右，向下遍历整张输入图片</li><li>完成遍历之后，换一个尺寸，重复1-3</li><li>所有尺寸都尝试过之后，输入图片中的所有目标自然被检测出来</li></ol><p>很明显，上述做法计算量非常大，症结主要有</p><ol><li>每一个框都要不断移动，遍历整张图</li><li>两个要穷举所有尺寸的框</li></ol><p>对于第1个问题，可以通过<strong>滑动窗口的卷积实现</strong>来规避。而对于第二个问题，目前有两种主流思路：</p><ol><li>感兴趣区域预提取的思路<ol><li>通过图像分割，提取感兴趣区域，然后将所有感兴趣区域依次送入CNN进行预测——RCNN</li><li>用滑动窗口的卷积实现，代替RCNN中依次进行的CNN处理——Fast RCNN</li><li>用一个CNN代替图像分割进行感兴趣区域提取——Faster RCNN</li></ol></li><li>YOLO思路<br>将输入图片分割为19x19个区域，然后结合<strong>滑动窗口卷积实现</strong>的思路将整张图片送入CNN网络，得到19x19个$(p,x,y,w,h,c_1,c_2,c_3)^T$，每一个$(p,x,y,w,h,c_1,c_2,c_3)^T$表示对应区域中的目标检测结果。特别的，存在区域认领目标的概念，意思是说，当一个目标的检测框的中心在某个区域之内时，这个区域对应的result向量才会生成有效信息。基于这个概念，产生了以下两个问题：<ol><li>一个区域内就是有多个目标的检测框的中心点怎么办？</li><li>相邻的几个区域都声称同一个目标的中心点在它那里，然后都生成了几乎相同的有效的result向量怎么办？<br>对于问题1，采用anchor box应对，问题2则采用NMS应对。</li></ol></li></ol><h2 id="滑动窗口的卷积实现原理"><a href="#滑动窗口的卷积实现原理" class="headerlink" title="滑动窗口的卷积实现原理"></a>滑动窗口的卷积实现原理</h2><p>将CNN网络模型中最后的几个全连接层看作卷积层。因为从数学角度看，卷积层和全连接层是一样的，因为这400个节点中每个节点的$\boldsymbol{w}$向量都可以看作一个5×5×16的过滤器，所以不论是把最后这几层看作全连接层还是卷积层，他们的输出都是上一层输出经过某个任意线性函数处理后的结果。所以本质上就是一个如何看的问题，计算的本质过程是不变的。这么做的好处是，可以把输入的19x19个区域与输出的19x19个result做有意义的关联了。</p><div align=center><img title="滑动窗口的卷积实现" src="/img/article/hdck.png" width="60%" height="60%" align=center></div><h2 id="anchor-box的工作原理"><a href="#anchor-box的工作原理" class="headerlink" title="anchor box的工作原理"></a>anchor box的工作原理</h2><p>将某个区域认领某个目标的对应关系再细分一级，变成某个区域的某个anchor box认领某个目标。这样，当一个区域内有多个中心点时，再额外的计算一下每个目标与每个预设的anchor box之间的交并比之后，即可将目标细分到anchor box。这时，每个区域便可以认领多个不同种类的目标。一般通过聚类训练集中的目标进行anchor box的尺寸设定。</p><h2 id="NMS的工作原理"><a href="#NMS的工作原理" class="headerlink" title="NMS的工作原理"></a>NMS的工作原理</h2><p>对于所有19x19个result向量，一类一类的来看，比如先看归属于vehicle类的box：</p><ol><li>选取这类box中scores最大的哪一个，记为box_best，并保留它</li><li>计算box_best与其余的box的IOU</li><li>如果其IOU&gt;0.5了，那么就舍弃这个box（由于可能这两个box表示同一目标，所以保留分数高的哪一个）</li><li>从最后剩余的boxes中，再找出最大scores的哪一个，如此循环往复</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DNN的数学原理</title>
    <link href="/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
    <url>/2021/07/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>数学是现实世界的精确抽象，再花里胡哨的东西，内在终究还是一堆加减乘除。因此，从数学的角度写一下到底啥是深度学习，学过线代就能懂。</p><h2 id="神经网络模型的结构"><a href="#神经网络模型的结构" class="headerlink" title="神经网络模型的结构"></a>神经网络模型的结构</h2><p><strong>神经网络</strong>的基础单元是<strong>神经元</strong>，多个<strong>神经元</strong>纵向堆叠形成神经<strong>网络层</strong>，神经<strong>网络层</strong>横向堆叠形成<strong>神经网络</strong>。</p><div align=center><img title="" src="/img/article/神经网络的数学原理/神经网络.png" width="50%" height="50%" align=center></div><br><div align=center><img title="" src="/img/article/神经网络的数学原理/神经元.png" width="60%" height="60%" align=center></div><h2 id="神经元的数学原理"><a href="#神经元的数学原理" class="headerlink" title="神经元的数学原理"></a>神经元的数学原理</h2><p>对于一个神经元，进行的数学计算为：</p><blockquote><p>接受一个向量$\boldsymbol{a}^{[j-1]}$，通过与$\boldsymbol{w}^{[j]}_i$进行向量内积运算产生一个中间值$z_i^{[j]}$（标量），然后用激活函数$g_i^{[j]}()$将$z$转换为$a_i^{[j]}$。</p></blockquote><p>其中:</p><blockquote><p>上标用来定位该神经元位于哪一层，一般输入层后的第一层为1；<br>下标用来定位该神经元位于第几个，一般最上方的序号为0；</p></blockquote><h2 id="矩阵维度确认的数学原理"><a href="#矩阵维度确认的数学原理" class="headerlink" title="矩阵维度确认的数学原理"></a>矩阵维度确认的数学原理</h2><p>首先区分开这4个概念：<strong>模型的参数</strong>，<strong>层的参数</strong>，<strong>神经元的参数</strong>，<strong>数据及数据的中间值</strong>。然后，仔细理解上面两段话，神经网络中最为tricky的维度问题便迎刃而解：</p><ol><li>对于$\boldsymbol{x}$和$\boldsymbol{y}$，其维度看样本就知道，已经定义好了;</li><li>对于某一个神经元的权重参数$\boldsymbol{w}^{[j]}_i$，由于要跟输入的向量$\boldsymbol{a}^{[j-1]}$进行内积，所以两者的维度必然是相同的，而后者作为一个列向量，其行数等于上一层的神经元数量（因为每个神经元输出一个标量）。然后由于本层的每一个神经元都有一个权重参数$\boldsymbol{w}^{[j]}$，那么由${\boldsymbol{w}^{[j]}}^T$纵向堆叠形成的${\boldsymbol{W}^{[j]}}^T$的行数就是$\boldsymbol{w}^{[j]}$的个数，亦即本层的神经元数量，其列数前面已经说了，就是$\boldsymbol{w}^{[j]}$的行数，亦即上一层的神经元数量。<br></li><li>对于某一个神经元的偏移量参数${b}^{[j]}$，自然是一个标量。那么本层的偏移量参数$\boldsymbol{b}^{[j]}$的行数就是${b}^{[j]}$的数量，亦即本层的神经元数量。</li><li>对于每一层的中间值$\boldsymbol{z}^{[j]}$，输出值$\boldsymbol{a}^{[j]}$，其维度确定方式与$\boldsymbol{b}^{[j]}$一样。</li><li>另外对于激活函数，一般同一层都一样，所以$\boldsymbol{g}^{[j]}()$退化为${g}^{[j]}()$。</li></ol><h2 id="矢量化的数学原理"><a href="#矢量化的数学原理" class="headerlink" title="矢量化的数学原理"></a>矢量化的数学原理</h2><p>矢量化的本质是将样本在时间轴上被神经网络模型处理的序列转化为空间上的序列：<br></p><blockquote><p>$X= (\boldsymbol{x}^1,\boldsymbol{x}^2, …,  \boldsymbol{x}^)$</p></blockquote><p>说人话就是，原来每次送入模型一个列向量，计算得到一个列向量。现在每次送入m个列向量，计算的到m个列向量。当然了，各层的中间值$\boldsymbol{z}^{[j]}$和输出值$\boldsymbol{a}^{[j]}$也都将因此横向扩充一个维度。</p><h2 id="Batch的数学原理"><a href="#Batch的数学原理" class="headerlink" title="Batch的数学原理"></a>Batch的数学原理</h2><p>一个batch指每次送入模型的一批样本。比如，现有2000个样本，将其划分为4个batch，那么每个batch包含500个样本，即</p><blockquote><p>batch_size=500;<br>batch_num = 4;</p></blockquote><p>一个epoch指整个训练集被利用了一次。即，2000个样本中的每一个都被代入模型进行了一次前向计算和反向传播。</p><p>一个iteration指权重参数更新一次。一个epoch中可能有多个iteration，也可能只有一个iteration，这取决于batch_num的值。</p><p>留一个疑问，在一个epoch中，一个batch会循环多次使用吗？还是只用1次就结束了？例如，在一个epoch中用batch_1迭代10次，然后batch_2迭代10次，….，最后batch_4迭代10次，一个epoch完成。是这样吗？<em>——<a href="https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks">目前来看不是这样，而是只用1次。 2021年7月15日</a></em></p><h2 id="正向计算的数学原理"><a href="#正向计算的数学原理" class="headerlink" title="正向计算的数学原理"></a>正向计算的数学原理</h2><p>正向计算，用于得到所需的预测结果：</p><ol><li>输入一个列向量：<br><blockquote><p> $\boldsymbol{x} = (x_1, x_2, … , x_n)^T$</p></blockquote></li><li><p>进行一系列矩阵计算：<br></p><blockquote><p> $\boldsymbol{a}^{[0]} = \boldsymbol{x}$<br> $\boldsymbol{z}^{[1]} = {\boldsymbol{W}^{[1]}}^T\boldsymbol{a}^{[0]}+\boldsymbol{b}^{[1]}$<br> $\boldsymbol{a}^{[2]} = g^{[1]}(\boldsymbol{z}^{[1]})$<br> …<br> $\boldsymbol{z}^{[j]} = {\boldsymbol{W}^{[j]}}^T\boldsymbol{a}^{[j-1]}+\boldsymbol{b}^{[j]}$<br> $\boldsymbol{a}^{[j]} = g^{[j]}(\boldsymbol{z}^{[j]})$<br> …<br> $\boldsymbol{z}^{[l]} = {\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}$<br> $\boldsymbol{a}^{[l]} = g^{[l]}(\boldsymbol{z}^{[l]})$<br> $\boldsymbol{\hat{y}} = \boldsymbol{a}^{[l]}$</p></blockquote><p>特别地，对于多分类模型的输出层（最后一层），一般有：</p><blockquote><p>$g^{[l]}(\boldsymbol{x}) = softmax(\boldsymbol{x}) = \frac{exp(\boldsymbol{x})}{\boldsymbol{1}^Texp(\boldsymbol{x})}$</p></blockquote><p>其中 $\boldsymbol{1}$ 为全1列向量，维度可从context推得。</p></li><li>得到一个列向量：<br><blockquote><p> $\boldsymbol{\hat{y}} = (\hat{y}_1,\hat{y}_2, … , \hat{y}_n)^T$</p></blockquote></li></ol><h2 id="反向传播的数学原理"><a href="#反向传播的数学原理" class="headerlink" title="反向传播的数学原理"></a>反向传播的数学原理</h2><p>反向传播，用于得到各网络层参数的更新量。主要有以下3个步骤：</p><ol><li>单个样本$\boldsymbol{x}$ 正向计算，得到各层的$\boldsymbol{z}$和$\boldsymbol{a}$备用。</li><li>计算输出层的参数更新量：<ol><li>求得损失$L$对$\boldsymbol{z}^{[l]}$的偏导$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}$，记作$d\boldsymbol{z}^{[l]}$。上标$l$表示最后一层，即输出层。若输出层激活函数为$softmax$且损失函数为交叉熵，则有$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]}) - \boldsymbol{y}$，对应的求导过程如下：<blockquote><p>$L = -\boldsymbol{y}^Tlog\hat{\boldsymbol{y}}$<br>$\downarrow$<br>$dL = -d\boldsymbol{y}^Tlog\hat{\boldsymbol{y}}-\boldsymbol{y}^Td(log\hat{\boldsymbol{y}})$<br>$= -\boldsymbol{y}^T d(log\hat{\boldsymbol{y}})$<br>$= -\boldsymbol{y}^Td(log(softmax(\boldsymbol{z}^{[l]})))$<br>$= -\boldsymbol{y}^Td(log(\frac{exp(\boldsymbol{\boldsymbol{z}^{[l]}})}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}))$<br>$= -\boldsymbol{y}^Td(log(exp(\boldsymbol{\boldsymbol{z}^{[l]}}))+\boldsymbol{1}log(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})))$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + d(log(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})))$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + \frac{d(\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}}))}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + \frac{\boldsymbol{1}^T(exp(\boldsymbol{\boldsymbol{z}^{[l]}}) \odot d\boldsymbol{z}^{[l]} )}{\boldsymbol{1}^Texp(\boldsymbol{\boldsymbol{z}^{[l]}})}$<br>$= -\boldsymbol{y}^Td\boldsymbol{z}^{[l]} + {\frac{exp(\boldsymbol{\boldsymbol{z}^{[l]}})}{\boldsymbol{1}^Texp(\boldsymbol{z}^{[l]})}}^Td\boldsymbol{z}^{[l]}$<br>$= (softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T)^Td\boldsymbol{z}^{[l]}$<br>$\downarrow$<br>$tr(dL) = dL = tr((softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T)^Td\boldsymbol{z}^{[l]}) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}} = softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T$<br>$\downarrow$<br>$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]})-\boldsymbol{y}^T$</p></blockquote></li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{z}^{[l]}$得到$d{\boldsymbol{W}^{[l]}}^T$，$d\boldsymbol{b}^{[l]}$，$d\boldsymbol{a}^{[l-1]}$<ul><li>$d{\boldsymbol{W}^{[l]}}^T$的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td{\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]})$<br>$= tr(\boldsymbol{a}^{[l-1]}{\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td{\boldsymbol{W}^{[l]}}^T)$<br>$= tr(({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}{\boldsymbol{a}^{[l-1]}}^T)^Td{\boldsymbol{W}^{[l]}}^T)$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{W}^{[l]}}^T} = {\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}{\boldsymbol{a}^{[l-1]}}^T$<br>$\downarrow$<br>$d{\boldsymbol{W}^{[l]}}^T = d\boldsymbol{z}^{[l]}{\boldsymbol{a}^{[l-1]}}^T$</p></blockquote></li><li>$d\boldsymbol{b}^{[l]}$的求导过程<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{b}^{[l]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{b}^{[l]}}} = {\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}$<br>$\downarrow$<br>$d\boldsymbol{b}^{[l]} = d\boldsymbol{z}^{[l]}$</p></blockquote></li><li>$d\boldsymbol{a}^{[l-1]}$的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td\boldsymbol{z}^{[l]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^Td({\boldsymbol{W}^{[l]}}^T\boldsymbol{a}^{[l-1]}+\boldsymbol{b}^{[l]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}^T{\boldsymbol{W}^{[l]}}^Td\boldsymbol{a}^{[l-1]})$<br>$= tr((\boldsymbol{W}^{[l]}\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}})^Td\boldsymbol{a}^{[l-1]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} = {\boldsymbol{W}^{[l]}\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l]}}}}$<br>$\downarrow$<br>$d\boldsymbol{a}^{[l-1]} = \boldsymbol{W}^{[l]}d\boldsymbol{z}^{[l]}$</p></blockquote></li></ul></li><li>利用微分的分解+迹技巧实现链式法则，由$d\boldsymbol{a}^{[l-1]}$得到$d\boldsymbol{z}^{[l-1]}$，对应的求导过程如下：<blockquote><p>$tr(dL) = tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^Td\boldsymbol{a}^{[l-1]})$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^Td(g(\boldsymbol{z}^{[l-1]}))$<br>$= tr({\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}}}^T(g’(\boldsymbol{z}^{[l-1]}) \odot d\boldsymbol{z}^{[l-1]}))$<br>$= tr((\frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} \odot g’(\boldsymbol{z}^{[l-1]}))^T d\boldsymbol{z}^{[l-1]})$<br>$\downarrow$<br>$\frac{\partial{L}}{\partial{\boldsymbol{z}^{[l-1]}}} = \frac{\partial{L}}{\partial{\boldsymbol{a}^{[l-1]}}} \odot g’(\boldsymbol{z}^{[l-1]})$<br>$\downarrow$<br>$d\boldsymbol{z}^{[l-1]} = d\boldsymbol{a}^{[l-1]}\odot g’(\boldsymbol{z}^{[l-1]})$</p></blockquote></li></ol></li><li>仿照输出层的计算方式，反向传播，依次得到各层的参数更新量。<blockquote><p>$d\boldsymbol{z}^{[l]} = softmax(\boldsymbol{z}^{[l]}) - \boldsymbol{y}$<br>$d{\boldsymbol{W}^{[l]}}^T = d\boldsymbol{z}^{[l]}{\boldsymbol{a}^{[l-1]}}^T$ AND $d\boldsymbol{b}^{[l]} = d\boldsymbol{z}^{[l]}$<br>…<br>$d\boldsymbol{z}^{[j]} = (\boldsymbol{W}^{[j+1]}d\boldsymbol{z}^{[j+1]}) \odot g’(\boldsymbol{z}^{[j]})$<br>$d{\boldsymbol{W}^{[j]}}^T = d\boldsymbol{z}^{[j]}{\boldsymbol{a}^{[j-1]}}^T$ AND $d\boldsymbol{b}^{[j]} = d\boldsymbol{z}^{[j]}$<br>$d\boldsymbol{z}^{[j-1]} = (\boldsymbol{W}^{[j]}d\boldsymbol{z}^{[j]}) \odot g’(\boldsymbol{z}^{[j-1]})$<br>…<br>$d{\boldsymbol{W}^{[1]}}^T = d\boldsymbol{z}^{[1]}{\boldsymbol{a}^{[0]}}^T$ AND $d\boldsymbol{b}^{[1]} = d\boldsymbol{z}^{[1]}$<br>$d\boldsymbol{z}^{[0]} = (\boldsymbol{W}^{[1]}d\boldsymbol{z}^{[1]}) \odot g’(\boldsymbol{z}^{[0]})$</p></blockquote></li></ol><h2 id="梯度下降的数学原理"><a href="#梯度下降的数学原理" class="headerlink" title="梯度下降的数学原理"></a>梯度下降的数学原理</h2><p>梯度下降的思路：</p><ol><li>将整个模型视为一个以模型参数为自变量，以L为因变量，以样本数据为参数的函数</li><li>然后将模型参数寻优的问题转化为求这个函数最值和驻点的问题</li><li>然后基于这样一个原理<strong>自变量的数值沿梯度方向靠近时因变量的数值将随之向最值靠近</strong>，便可得到梯度下降公式：<blockquote><p>$\boldsymbol{W} -= \alpha d\boldsymbol{W}$<br>$\boldsymbol{b} -= \alpha d\boldsymbol{b}$</p></blockquote></li></ol><p>然后根据不同的实现，又有以下的梯度下降法变种：</p><ol><li>基于batch_size的不同<ul><li>batch_size = sample_size，称Batch Gradient Descent；</li><li>batch_size = 1，称Stocastic Gradient Descent；</li><li>batch_size介于两者之间，称Mini-Batch Gradient Descent;</li></ul></li><li>每次不是简单的减去更新量，而是减去更新量的移动平均值，即可得到GD with Momentum；其中移动平均的含义是取前n次更新量的平均值作为本次的更新量，$n=\frac{1}{1-\beta}$，$\beta$一般取0.9；<blockquote><p>$v^{d\boldsymbol{W}} = \beta v^{d\boldsymbol{W}} + (1-\beta)d\boldsymbol{W}$<br><br>$v^{d\boldsymbol{b}} = \beta v^{d\boldsymbol{b}} + (1-\beta)d\boldsymbol{b}$<br><br>$\boldsymbol{W} -= \alpha v^{d\boldsymbol{W}}$<br><br>$\boldsymbol{b} -= \alpha v^{d\boldsymbol{W}}$<br></p></blockquote></li><li>在上面的基础上，如果将用更新量的平方进行移动平均，然后再对移动平均值开方得到本次的更新量，即可得到RMSprop；<blockquote><p>$S^{d\boldsymbol{W}} = \beta S^{d\boldsymbol{W}} + (1-\beta)({d\boldsymbol{W}})^2$<br><br>$S^{d\boldsymbol{b}} = \beta S^{d\boldsymbol{b}} + (1-\beta)({d\boldsymbol{b}})^2$<br><br>$\boldsymbol{W} -= \alpha \frac{d\boldsymbol{W}}{\sqrt{S^{d\boldsymbol{W}}}}$<br><br>$\boldsymbol{b} -= \alpha \frac{d\boldsymbol{b}}{\sqrt{S^{d\boldsymbol{b}}}}$<br></p></blockquote></li><li>将2和3合并起来使用，即可得到Adam；<blockquote><p>$v^{d\boldsymbol{W}} = \beta_1 v^{d\boldsymbol{W}} + (1-\beta_1)d\boldsymbol{W}$<br><br>$v^{d\boldsymbol{b}} = \beta_1 v^{d\boldsymbol{b}} + (1-\beta_1)d\boldsymbol{b}$<br><br>$S^{d\boldsymbol{W}} = \beta_2 S^{d\boldsymbol{W}} + (1-\beta_2)({d\boldsymbol{W}})^2$<br><br>$S^{d\boldsymbol{b}} = \beta_2 S^{d\boldsymbol{b}} + (1-\beta_2)({d\boldsymbol{b}})^2$<br><br>$\boldsymbol{W} -= \alpha \frac{v^{d\boldsymbol{W}}}{\sqrt{S^{d\boldsymbol{W}}}+\epsilon}$<br><br>$\boldsymbol{b} -= \alpha \frac{v^{d\boldsymbol{b}}}{\sqrt{S^{d\boldsymbol{b}}}+\epsilon}$<br></p></blockquote></li></ol><h2 id="归一化的数学原理"><a href="#归一化的数学原理" class="headerlink" title="归一化的数学原理"></a>归一化的数学原理</h2><p>在向量化和Mini-Batch的前提下，每一层的中间值$Z^{[j]}$先进行跨样本的normalization之后再进行激活，就是所谓的归一化。</p><blockquote><p>矩阵$Z^{[j]}$按行求算数平均，得到均值列向量$\bar{z}$；<br>矩阵$Z^{[j]}$与均值列向量$\bar{z}$进行标准差运算，得到标准差矩阵$\boldsymbol{\Sigma}$；<br>矩阵$Z^{[j]}$的每一列都减去均值列向量$\bar{z}$后，再逐元素除以标准差矩阵$\boldsymbol{\Sigma}$即可得到归一化后的新中间值矩阵。</p></blockquote><p>另外归一化还存在一个小问题，就是他的前提是向量化和Mini-Batch，就是说跨样本求均值和方差的基础是有多个样本。但是在完成模型训练之后进行预测时，肯定都是每次喂到模型中一个样本，那么此时如何求均值和方差呢？毕竟最好怎么训练出来的怎么用嘛。</p><p>一般的做法是，在这个训练过程中用移动平均数实时追踪均值和方差，或者用整个训练集的均值和方差也行，问题不大，而且主流的DL框架一般都封装好了。</p><h2 id="正则化的数学原理"><a href="#正则化的数学原理" class="headerlink" title="正则化的数学原理"></a>正则化的数学原理</h2><p>正则化是一种缓解高方差，过拟合问题的措施。具体做法是在原有的损失函数之后额外增加一个关于权重矩阵的损失项，这样一来权重越大损失就越大，随着训练的进行权重会越来越趋近于0，变相地降低了模型的参数量，缓解了过拟合。</p><p>常用的两种正则化方式如下，</p><script type="math/tex; mode=display">L = L_{original} + L_1 = L_{original} + \lambda \sum_{i = 0}^{l} \lvert\lvert \boldsymbol{W}^{[i]} \rvert\rvert _1</script><script type="math/tex; mode=display">L = L_{original} + L_2 = L_{original} + \frac{\lambda}{2} \sum_{i = 0}^{l}  \lvert\lvert \boldsymbol{W}^{[i]} \rvert\rvert ^2_2</script><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>$\lvert\lvert \boldsymbol{W}^{[i]} \rvert\rvert _1$的含义是矩阵$\boldsymbol{W}^{[i]}$每个元素的绝对值的总和，被称为L1正则化，最后容易得到稀疏的权重矩阵，有利于后续的剪枝和模型压缩。对应地，由于$d\boldsymbol{W}^{[i]}$是$L$关于$\boldsymbol{W}^{[i]}$的偏导，此时有</p><script type="math/tex; mode=display">d\boldsymbol{W}^{[i]} = d\boldsymbol{W}^{[i]}_{original} + \lambda</script><script type="math/tex; mode=display">\boldsymbol{W}^{[i]} -= \alpha (d\boldsymbol{W}^{[i]}_{original} + \lambda)</script><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>$\lvert\lvert \boldsymbol{W}^{[i]} \rvert\rvert ^2_2$的含义是矩阵$\boldsymbol{W}^{[i]}$每个元素的平方的总和，被称为L2正则化，最后得到的权重矩阵较为平滑，被广泛用于防止过拟合。此时有</p><script type="math/tex; mode=display">d\boldsymbol{W}^{[i]} = d\boldsymbol{W}^{[i]}_{original} + \lambda \boldsymbol{W}^{[i]}</script><script type="math/tex; mode=display">\boldsymbol{W}^{[i]} -= \alpha (d\boldsymbol{W}^{[i]}_{original} + \lambda \boldsymbol{W}^{[i]})</script><p>从L2正则权重更新的公式可以看出，相对于在不加正则的基础上，先将原矩阵乘以$1-\alpha\lambda$因子，然后在进行参数更新，因此L2正则又被称为权重衰减。</p><h3 id="正则权重更新公式推导"><a href="#正则权重更新公式推导" class="headerlink" title="正则权重更新公式推导"></a>正则权重更新公式推导</h3><p>本来想用矩阵求导术推导出加入正则后的$d\boldsymbol{W}^{[i]}$，但是好像不太适用。所以这里就离散的理解一下好了，比如对于L2正则，求其对于某一层的权重矩阵的偏导$d\boldsymbol{W}^{[i]}$时是不依赖于链式法则的。首先，因为求的是偏导，其它层的W项自动忽略。然后将矩阵打开来看，分别对每一个标量w求偏导，此时其他w自动忽略。</p><script type="math/tex; mode=display">d\boldsymbol{W}^{[i]}_{l2} = \frac{\partial L_2}{\partial \boldsymbol{W}^{[i]}} =\left[\begin{array}{cccc}  \frac{\partial L_2}{w^{[i]}_{11}} & \frac{\partial L_2}{w^{[i]}_{12}}       & \cdots & \frac{\partial L_2}{w^{[i]}_{1n}}       \\ \frac{\partial L_2}{w^{[i]}_{21}}       & \frac{\partial L_2}{w^{[i]}_{22}}       &{\cdot^{\cdot^{\cdot}}} & \frac{\partial L_2}{w^{[i]}_{2n}}       \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial L_2}{w^{[i]}_{m1}}       & \frac{\partial L_2}{w^{[i]}_{m2}}       & \cdots & \frac{\partial L_2}{w^{[i]}_{mn}}       \\\end{array}\right]</script><script type="math/tex; mode=display">L_2 = \frac{lambda}{2} (w_{11}^2 + w_{12}^2 + w_{13}^2 + ... + w_{mn}^2)</script><script type="math/tex; mode=display">\frac{\partial L_2}{w^{[i]}_{11}} = \frac{\lambda}{2} \cdot 2w^{[i]}_{11}</script><script type="math/tex; mode=display">d\boldsymbol{W}^{[i]}_{l2} = \lambda \boldsymbol{W}^{[i]}</script>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>DNN</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CSAPP并发编程总结</title>
    <link href="/2021/07/09/CSAPP%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <url>/2021/07/09/CSAPP%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="并发编程基本概念"><a href="#并发编程基本概念" class="headerlink" title="并发编程基本概念"></a>并发编程基本概念</h2><ul><li><strong>并发</strong>：多个逻辑控制流的生命周期有重叠，即称为<strong>并发现象</strong>(concurrency)</li><li><strong>并行</strong>：发生在多核/多计算机上的并发现象（在一个时刻上存在多个逻辑控制流），称为<strong>并行现象</strong>（parallel），是并发现象的真子集；</li></ul><h2 id="并发程序的三种构造方式："><a href="#并发程序的三种构造方式：" class="headerlink" title="并发程序的三种构造方式："></a>并发程序的三种构造方式：</h2><ul><li><strong>进程</strong>：每个逻辑控制流实现为一个进程<ul><li>特点：独立的虚拟地址空间</li><li>优点：独立则不易混淆</li><li>缺点：<ul><li>独立则难以共享数据</li><li>进程context切换和IPC开销高，所以往往比较慢（ 进程间通信机制）</li></ul></li></ul></li><li><strong>I/O多路复用</strong>：状态机化，逻辑控制流的切换实现为状态机的状态切换。具体原理看<a href="https://www.zhihu.com/question/32163005/answer/55772739">这个</a><ul><li>优点：共享数据容易，并且没有进程context切换的开销</li><li>缺点：编码复杂，不能充分利用多核处理器</li></ul></li><li><strong>线程</strong>：重点，下面展开讲。</li></ul><h2 id="线程基本概念："><a href="#线程基本概念：" class="headerlink" title="线程基本概念："></a>线程基本概念：</h2><ul><li><strong>主线程</strong>：进程中第一个运行的线程</li><li><strong>对等线程</strong>：进程中后来运行的线程</li><li><strong>与进程的区别</strong>：<ul><li>上下文内容少，切换更快，开销更少，具体包括：线程ID、栈和栈指针、PC、条件码、register value</li><li>一个进程的所有线程（对等线程池）彼此之间没有层次结构，都是对等的；</li><li>对等线程之间共享进程的虚拟地址空间，可以等待另外一个对等线程终止或主动杀死它</li></ul></li><li><strong>共享变量</strong>：一个变量的一个实例被不止一个线程引用，那么这个变量称为共享变量</li><li><strong>线程安全的函数</strong>：被多个并发线程反复调用时能够一直产生正确结果的函数称为线程安全函数</li><li><strong>可重入函数</strong>：线程安全函数的一个真子集，指不会引入任何共享数据的函数<ul><li><strong>显式可重入</strong>：传参均为值传递（且非指针值传递），而且所有数据引用的都是本地自动栈变量</li><li><strong>隐式可重入</strong>：在显式的基础上取消“值传递（且非指针值传递）”的限制，允许指针值传递和引用传递，但是传递的变量都是非共享变量时，该函数是隐式可重入的</li></ul></li><li><strong>竞争</strong>：程序的正确性依赖于某条/某些特定的轨迹线，或者说不是全部的轨迹线都能让程序正确执行，哪怕是那些绕过了互斥锁禁止区的全部轨迹线也不行。（具体例子见CSAPP P719）</li></ul><h2 id="Posix线程模型："><a href="#Posix线程模型：" class="headerlink" title="Posix线程模型："></a>Posix线程模型：</h2><p>管理Linux线程的C语言接口包<pthread.h>，包含大约60个函数。</p><ul><li><strong>线程例程概念</strong>：接受和返回一个void指针的函数类型，其内容为线程真正要做的事。若输入输出的参数较多，应封装为一个结构体。</li><li><strong>常用的pthread函数</strong>：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/*创建线程*/</span><br>int pthread_creat(&amp;tid, NULL, thread, NULL)<br><span class="hljs-regexp">//</span>(返回线程ID，设置线程属性(高阶内容)，线程例程函数名，线程例程函数的传参)，成功返回<span class="hljs-number">0</span>，否则非<span class="hljs-number">0</span><br><br><span class="hljs-regexp">/*终止线程*/</span><br><span class="hljs-regexp">//</span>方式<span class="hljs-number">1</span>：某个对等线程的例程函数执行完毕，该线程会隐式终止<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">2</span>：某个对等线程调用pthread_exit函数，线程会显式终止，而且如果是主线程调用，它会等待所有其他对等线程终止后再终止（进程也被终止了）<br>void pthread_exit(void *thread_return) <span class="hljs-regexp">//</span>函数不返回（因为逻辑控制流都结束了啊），会将一些信息写到thread_return中<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">3</span>：某个对等线程调用系统<span class="hljs-keyword">exit</span>函数，终止其所属进程及该进程所有的线程<br><span class="hljs-regexp">//</span>方式<span class="hljs-number">4</span>：某个对等线程调用pthread_cancel函数，终止另一个对等线程<br>int pthread_cancel(pthread_t tid)  <span class="hljs-regexp">//</span>终止线程ID为tid的对等线程，成功返回<span class="hljs-number">0</span>，否则非<span class="hljs-number">0</span><br><br><span class="hljs-regexp">/*回收已终止线程的资源*/</span><br>int pthread_join(pthread_t tid, void**thread_return) <span class="hljs-regexp">//</span>调用该函数的对等线程阻塞，等待线程ID为tid的对等线程结束，然后回收其资源后返回<br><br><span class="hljs-regexp">/*分离线程*/</span><br><span class="hljs-regexp">//</span>一个线程的状态要么是detached要么是joinable，处于后者时意味着可以被其他线程杀死和回收资源，前者不可（自行终止，系统回收资源）<br>int pthread_detach(pthread_t tid)  <span class="hljs-regexp">//</span>调用该函数的对等线程将线程ID为tid的线程分离<br><br><span class="hljs-regexp">/*获取自身ID*/</span><br>pthread_t pthread_self();<br></code></pre></td></tr></table></figure></li></ul><h2 id="线程的内存模型（两个关键问题）"><a href="#线程的内存模型（两个关键问题）" class="headerlink" title="线程的内存模型（两个关键问题）"></a>线程的内存模型（两个关键问题）</h2><ul><li><strong>线程的内存模型是怎样的？</strong>——不是整齐清楚的。。。</li><li><strong>变量的实例如何映射到线程的内存模型中？</strong><ul><li>全局变量+局部的静态变量：一个进程中只有一个实例，任何线程均可引用；</li><li>局部的自动变量：多个实例，由各个线程栈自行管理。</li></ul></li></ul><h2 id="线程共享变量的冲突问题"><a href="#线程共享变量的冲突问题" class="headerlink" title="线程共享变量的冲突问题"></a>线程共享变量的冲突问题</h2><p>关键字：进度图-&gt;信号量-&gt;PV操作-&gt;互斥锁-&gt;互斥锁加/解锁-&gt;死锁</p><ul><li><strong>进度图</strong>：注意把P/V操作放到线段上，状态放到端点上，这样端点的状态即可解释为执行P/V操作前或者P/V操作后</li><li><strong>信号量</strong>s：一个非负整数全局变量</li><li><strong>PV操作</strong>（原子操作）：<ul><li>P(s)操作：检查s是否为0；<ul><li>是，则调用该函数的线程在此处阻塞；</li><li>否，则将s减1后继续向下执行；</li></ul></li><li>V(s)操作：先将s加1，然后检查有么有因为P(s)阻塞的线程，如有则将完成其P操作，然后置为就绪状态（等待调度），若没有那就没有。。若有不止一个，就随机选择一个，反正只能一个（因为要完成P操作啊）</li></ul></li><li><strong>互斥锁</strong>：二元的信号量</li><li><strong>互斥锁加/解锁</strong>：针对二元信号量的P/V操作</li><li><strong>死锁</strong>：<br>禁止区外存在这样一些状态点，既不能向右，也不能向上，因为向上会进入线程A的禁止区，向右会进入线程B的禁止区。<br><br>通过以下原则来防止死锁：<br>给定所有互斥操作的一个全序( 全序概念)，如果每个线程都是以该全序获得互斥锁并以相反的顺序（不是说全序的逆序，而是线程A和线程B释放的顺序相反）释放，那么这个程序就不会出现死锁。（但是该原则现在看下来只适用于两个线程，更多的线程就要用到更复杂的银行家算法了）<br><br>例如：<br><ul><li>线程1： P(s) -&gt; P(t) -&gt; V(t) -&gt; V(s);</li><li>线程2： P(s) -&gt; P(t) -&gt; V(s) -&gt; V(t);</li></ul></li></ul><h2 id="并行程序的性能量化（暂时略过）"><a href="#并行程序的性能量化（暂时略过）" class="headerlink" title="并行程序的性能量化（暂时略过）"></a>并行程序的性能量化（暂时略过）</h2><h2 id="信号量用于共享资源调度（暂时略过）"><a href="#信号量用于共享资源调度（暂时略过）" class="headerlink" title="信号量用于共享资源调度（暂时略过）"></a>信号量用于共享资源调度（暂时略过）</h2>]]></content>
    
    
    <categories>
      
      <category>工作技能</category>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>并发编程</tag>
      
      <tag>线程</tag>
      
      <tag>信号量</tag>
      
      <tag>锁</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何用vim写markdown</title>
    <link href="/2021/07/08/%E5%A6%82%E4%BD%95%E7%94%A8vim%E5%86%99markdown/"/>
    <url>/2021/07/08/%E5%A6%82%E4%BD%95%E7%94%A8vim%E5%86%99markdown/</url>
    
    <content type="html"><![CDATA[<h1 id="安装neovim"><a href="#安装neovim" class="headerlink" title="安装neovim"></a>安装neovim</h1><blockquote><p>sudo apt-get install neovim<br>neovim<br>:checkhealth</p></blockquote><h1 id="安装vim插件管理器vim-plug"><a href="#安装vim插件管理器vim-plug" class="headerlink" title="安装vim插件管理器vim-plug"></a>安装vim插件管理器vim-plug</h1><blockquote><p>sh -c ‘curl -fLo “${XDG_DATA_HOME:-$HOME/.local/share}”/nvim/site/autoload/plug.vim —create-dirs<br><a href="https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim">https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim</a>‘</p></blockquote><p>链接不上的话，需要：<br></p><blockquote><p>sudo nvim /etc/hosts<br></p></blockquote><p>添加一行：<br></p><blockquote><p>199.232.96.133 raw.githubusercontent.com<br></p></blockquote><p>其中的ip地址来自于<em><a href="https://githubusercontent.com.ipaddress.com/raw.githubusercontent.com">https://githubusercontent.com.ipaddress.com/raw.githubusercontent.com</a></em></p><h1 id="创建neovim配置文件init-vim"><a href="#创建neovim配置文件init-vim" class="headerlink" title="创建neovim配置文件init.vim"></a>创建neovim配置文件init.vim</h1><blockquote><p>mkdir .config/nvim<br>touch .config/nvim/init.vim</p></blockquote><h1 id="修改init-vim以添加插件"><a href="#修改init-vim以添加插件" class="headerlink" title="修改init.vim以添加插件"></a>修改init.vim以添加插件</h1><p>目录，markdown，preview一共三个暂时（修改的内容直接看文件）,<br>中间: </p><ul><li>遇到了自动折叠问题：修改配置文件解决  </li><li>又遇到了无法预览的问题，看github<a href="https://github.com/iamcco/markdown-preview.nvim/issues/120">作者回复</a>解决</li></ul>]]></content>
    
    
    <categories>
      
      <category>生活记录</category>
      
      <category>电脑设置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>vim</tag>
      
      <tag>markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
